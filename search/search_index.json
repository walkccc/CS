{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Computer Science Notes Getting Started In this website, I'll work on my notes about several topics: Operating System Maching Learning Please don't hesitate to give me your feedback if any adjustment is needed. You can simply press the \"Pencil icon\" in the upper right corner to edit the contents. How I generate this website I use the static site generator MkDocs and the beautiful theme Material for MkDocs to build this website! Since KaTeX is now supporting more functions and is much faster than MathJax , I've updated all math equations from MathJax to KaTeX. I also add overflow-x: auto to prevent the overflow issue on small screen devices, so you can scroll horizontally in the math display equations. More Informations For more informations please visit my GitHub site . By Jay Chen on May 14, 2018.","title":"Preface"},{"location":"#computer-science-notes","text":"","title":"Computer Science Notes"},{"location":"#getting-started","text":"In this website, I'll work on my notes about several topics: Operating System Maching Learning Please don't hesitate to give me your feedback if any adjustment is needed. You can simply press the \"Pencil icon\" in the upper right corner to edit the contents.","title":"Getting Started"},{"location":"#how-i-generate-this-website","text":"I use the static site generator MkDocs and the beautiful theme Material for MkDocs to build this website! Since KaTeX is now supporting more functions and is much faster than MathJax , I've updated all math equations from MathJax to KaTeX. I also add overflow-x: auto to prevent the overflow issue on small screen devices, so you can scroll horizontally in the math display equations.","title":"How I generate this website"},{"location":"#more-informations","text":"For more informations please visit my GitHub site . By Jay Chen on May 14, 2018.","title":"More Informations"},{"location":"ML/","text":"Machine Learning | notes In this page, I'll work on my notes when I watch the Machine Learning course video provided by Professor Hung-yi Lee .","title":"Preface"},{"location":"ML/#machine-learning-notes","text":"In this page, I'll work on my notes when I watch the Machine Learning course video provided by Professor Hung-yi Lee .","title":"Machine Learning | notes"},{"location":"ML/1/","text":"Lecture 1: Regression - Case Study Regression: Output a scalar Regression \u7684\u4f8b\u5b50\u8209\u4f8b\u6709\uff1a \u9053\u74ca\u65af\u6307\u6578\u9810\u6e2c \u81ea\u52d5\u99d5\u99db\u4e2d\u7684\u65b9\u5411\u76e4\u89d2\u5ea6\u9810\u6e2c \u63a8\u85a6\u7cfb\u7d71\u4e2d\u8cfc\u8cb7\u53ef\u80fd\u6027\u7684\u9810\u6e2c Example Application Estimating the Combat Power (CP) of a pokemon after evolution \u4e00\u96bb\u5bf6\u53ef\u5922\u53ef\u7531 5 \u500b\u53c3\u6578\u8868\u793a\uff1a $$x = (x_{cp}, x_s, x_{hp}, x_w, x_h).$$ \u6211\u5011\u5e0c\u671b\u80fd\u8a08\u7b97\u51fa\uff0c\u7d66\u5b9a\u5bf6\u53ef\u5922\u7684\u4e00\u4e9b features $x$\uff0c\u80fd\u904e\u7b97\u51fa\u4ed6\u9032\u5316\u5f8c\u7684 CP \u503c $y$ Machine Learning Steps Machine Learning \u4e3b\u8981\u6709\u4e09\u500b\u6b65\u9a5f Step 1: Model \u6211\u5011\u5047\u8a2d\u9810\u6e2c $y$ \u503c\u7684 function \u70ba $y = b + w \\cdot x_{cp}$\uff0c\u8a31\u591a\u4e0d\u540c\u7684 $w$ \u548c $b$ \u53ef\u69cb\u6210\u4e0d\u540c\u7684 $f_1, f_2, \\dots$\uff0c\u4f8b\u5982\uff1a \\begin{align} f_1: y & = 10.0 + 9.0 \\cdot x_{cp} \\\\ f_2: y & = 9.8 + 9.2 \\cdot x_{cp} \\\\ f_3: y & = -0.8 - 1.2 \\cdot x_{cp} \\\\ & \\vdots \\end{align} \u5982\u6b64\u4e00\u4f86\uff0c\u6211\u5011\u5c31\u6709\u4e86 linear model: $$y = b + \\sum w_ib_i$$ \u5176\u4e2d\uff0c $x_i: x_{cp}, x_s, x_{hp}, x_w, x_h$\uff1a\u5bf6\u53ef\u5922\u7684 features $w_i$\uff1a\u4e0d\u540c features\uff0c\u7d66\u5b83\u4e0d\u540c\u7684 weight $b$\uff1abias Step 2: Goodness of Function \u6709\u4e86\u4e00\u5806 functions \u5f8c\uff0c\u6211\u5011\u5fc5\u9700\u8981\u77e5\u9053\uff0c\u9019\u4e9b functions \u6709\u591a\u597d\uff1f\u56e0\u6b64\u6211\u5011\u9700\u8981\u4e00\u500b\u8a55\u6bd4\u7684\u6a19\u6e96\uff0c\u6211\u5011\u53ef\u4ee5\u7528 Loss function $L$\uff0c\u5b83\u7684 input \u70ba $f$\uff0coutput \u70ba how bad it is\uff1f loss function \u53ef\u5b9a\u7fa9\u5982\u4e0b\uff1a \\begin{align} L(f) & = \\sum_{n = 1}^{10} (\\hat y^n - f(x_{cp}^n))^2 \\\\ L(w, b) & = \\sum_{n = 1}^{10} (\\hat y^n - (b + w \\cdot x_{cp}^n))^2 \\end{align} Step 3: Best Function \u6700\u5f8c\uff0c\u6211\u5011\u8981\u9078\u51fa best function\uff0c\u5373\uff1a\u9078\u64c7 loss \u6700\u5c0f\u7684 function\uff08\u53c3\u6578\uff09\uff1a \\begin{align} f ^ { * } & = \\arg \\min _ { f } L ( f ) \\\\ w ^ { * } , b ^ { * } & = \\arg \\min _ { w , b } L ( w , b ) \\\\ & = \\arg \\min _ { w , b } \\sum _ { n = 1 } ^ { 10 } \\left( \\hat { y } ^ { n } - \\left( b + w \\cdot x _ { c p } ^ { n } \\right) \\right) ^ { 2 } \\end{align} \u6c42\u89e3\u6700\u4f73\u53c3\u6578\u7684\u65b9\u6cd5\u53ef\u4ee5\u662f Gradient Descent\u3002 Gradient Descent Gradient Descent \u7684\u6b65\u9a5f\u5982\u4e0b\uff1a \uff08\u96a8\u6a5f\uff09\u9078\u64c7\u53c3\u6578\u521d\u59cb\u503c\uff08e.g. $w^0$\uff09\uff0c\u4e0a\u6a19\u8868\u793a\u6642\u9593\u9ede \u8a08\u7b97 loss function $L(w)$ \u5728 $w = w^0$ \u4e0a\u6642\u7684 gradient\uff0c\u5373\uff1a$\\left. \\frac { d L } { d w } \\right| _ { w = w ^ { 0 } }$ \u5411\u8ca0\u68af\u5ea6\u65b9\u5411\u8fed\u4ee3\u66f4\u65b0 $\\frac{dL}{dw} \\Big|_{w = w^0} < 0 \\to$ Increase $w$ $\\frac{dL}{dw} \\Big|_{w = w^0} > 0 \\to$ Decrease $w$ \\begin{align} w ^ { 1 } & \\leftarrow w ^ { 0 } - \\eta \\left. \\frac { d L } { d w } \\right| _ { w = w ^ { 0 } } \\\\ w ^ { 2 } & \\leftarrow w ^ { 1 } - \\eta \\left. \\frac { d L } { d w } \\right| _ { w = w ^ { 1 } } \\end{align} \u5176\u4e2d\uff0c$\\eta$ \u70ba learning rate\uff0c\u7528\u4f86\u63a7\u5236\u4e00\u6b21\u8d70\u591a\u9060\u3001\u5b78\u7fd2\u901f\u5ea6 \u91cd\u8907\u9019 3 \u500b\u6b65\u9a5f\uff0c\u76f4\u5230 $w$ \u5728 local minima \u6536\u6582\u70ba\u6b62\u3002 \u9806\u5e36\u4e00\u63d0\uff0c\u53ea\u6709\u4e00\u500b\u53c3\u6578 $w$ \u6642\uff0c$\\nabla L$ \u5c31\u662f $\\frac{\\partial L}{\\partial w}$ \u70ba $L$ \u7684 gradient\uff0c\u6240\u4ee5\u624d\u53eb Gradient Descent\u3002 \u82e5\u6709\u5169\u500b\u53c3\u6578 $w, b$ \u6642\uff0c$\\nabla L = \\left[ \\begin{array} { l } { \\frac { \\partial L } { \\partial w } } \\\\ { \\frac { \\partial L } { \\partial b } } \\end{array} \\right]$\uff0c\u672c\u8cea\u4e0a\u6982\u5ff5\u9084\u662f\u76f8\u540c\u7684 Gradient Descent \u53ef\u80fd\u4f7f\u53c3\u6578\u505c\u5728\u640d\u5931\u51fd\u6578\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u3001\u5fae\u5206\u70ba $0$ \u7684\u9ede\uff0c\u6216\u8005\u5fae\u5206\u70ba\u6975\u5c0f\u503c\u7684\u9ede\u3002Linear regression \u4e2d\u4e0d\u5fc5\u64d4\u5fc3\u5c40\u90e8\u6700\u5c0f\u503c\u7684\u554f\u984c\uff0c\u56e0\u70ba loss function \u662f convex \u7684\u3002 \u5728\u5f97\u5230 best function \u4e4b\u5f8c\uff0c\u6211\u5011\u771f\u6b63\u5728\u610f\u7684\u662f\u5b83 \u5728 testing data \u4e0a\u7684\u8868\u73fe \u3002\u9078\u64c7\u4e0d\u540c\u7684 model\uff0c\u6703\u5f97\u5230\u4e0d\u540c\u7684 best function\uff0c\u5b83\u5011\u5728testing data \u4e0a\u6709\u4e0d\u540c\u8868\u73fe\u3002 \u8907\u96dc\u6a21\u578b\u7684 model space \u6db5\u84cb\u4e86\u7c21\u55ae\u6a21\u578b\u7684 model space\uff0c\u56e0\u6b64\u5728 training data \u4e0a\u7684\u932f\u8aa4\u7387\u66f4\u5c0f\uff0c\u4f46\u4e26\u4e0d\u610f\u5473\u8457\u5728 testing data \u4e0a\u932f\u8aa4\u7387\u4e5f\u6703\u5c0f\uff0c\u56e0\u70ba Model \u592a\u8907\u96dc\u6709\u5f88\u9ad8\u6a5f\u7387\u6703\u51fa\u73fe overfitting\u3002 What are the hidden factors? \u5982\u679c\u6211\u5011\u6536\u96c6\u66f4\u591a\u5bf6\u53ef\u5922\u9032\u5316\u524d\u5f8c\u7684 CP \u503c\u6703\u767c\u73fe\uff0c\u9032\u5316\u5f8c\u7684 CP \u503c\u4e0d\u53ea\u4f9d\u8cf4\u65bc\u9032\u5316\u524d\u7684 CP \u503c\uff0c\u53ef\u80fd\u9084\u6709\u5176\u5b83\u7684\u96b1\u85cf\u56e0\u7d20\uff08\u4f8b\u5982\uff1a\u5bf6\u53ef\u5922\u6240\u5c6c\u7684\u7269\u7a2e\uff09\u3002\u540c\u6642\u8003\u616e\u9032\u5316\u524d CP \u503c $x_{cp}$ \u548c\u7269\u7a2e $x_s$\uff0c\u6211\u5011\u53ef\u4ee5\u56de\u5230 Step 1: Model \u91cd\u65b0\u9078\u64c7\u65b0\u7684 model\uff1a \\begin{align} \\text{if } & x_s = \\text{pidgey}: & y = b_1 + w_1 \\cdot x_{cp} \\\\ \\text{if } & x_s = \\text{weedle}: & y = b_2 + w_2 \\cdot x_{cp} \\\\ \\text{if } & x_s = \\text{caterpie}: & y = b_3 + w_3 \\cdot x_{cp} \\\\ \\text{if } & x_s = \\text{eevee}: & y = b_4 + w_4 \\cdot x_{cp} \\end{align} \u9019\u4ecd\u662f\u4e00\u500b\u7dda\u6027\u6a21\u578b\uff0c\u56e0\u70ba\u5b83\u53ef\u4ee5\u5beb\u4f5c\uff1a \\begin{align} y = & b_1 \\cdot \\boldsymbol{\\delta(x_s = \\text{pidgey}) } & + & w_1 \\cdot \\boldsymbol{\\delta(x_s = \\text{pidgey}) \\cdot x_{cp}} + \\\\ & b_2 \\cdot \\boldsymbol{\\delta(x_s = \\text{weedle}) } & + & w_2 \\cdot \\boldsymbol{\\delta(x_s = \\text{weedle}) \\cdot x_{cp}} + \\\\ & b_3 \\cdot \\boldsymbol{\\delta(x_s = \\text{caterpie})} & + & w_3 \\cdot \\boldsymbol{\\delta(x_s = \\text{caterpie}) \\cdot x_{cp}} + \\\\ & b_4 \\cdot \\boldsymbol{\\delta(x_s = \\text{eevee}) } & + & w_4 \\cdot \\boldsymbol{\\delta(x_s = \\text{eevee}) \\cdot x_{cp}} \\end{align} \u4e0a\u5f0f\u4e2d\u7684\u7c97\u9ad4\u9805\u90fd\u662f linear model $y = b + \\Sigma w_i \\cdot x_i$ \u4e2d\u7684 feature $x_i$\u3002 \u9019\u500b\u6a21\u578b\u5728 testing data \u4e0a\u6709\u66f4\u597d\u7684\u8868\u73fe\uff0c\u5982\u679c\u540c\u6642\u8003\u616e\u5bf6\u53ef\u5922\u7684\u5176\u5b83\u5c6c\u6027\uff0c\u9078\u4e00\u500b\u5f88\u8907\u96dc\u7684\u6a21\u578b\uff0c\u7d50\u679c\u53ef\u80fd\u6703\u5c0e\u81f4 overfitting\u3002 \u5c0d\u7dda\u6027\u6a21\u578b\u4f86\u8aaa\uff0c\u6211\u5011\u5e0c\u671b\u9078\u51fa\u7684 best function \u80fd\u5e73\u6ed1\u4e00\u9ede\uff0c\u4e5f\u5c31\u662f\u6b0a\u91cd\u4fc2\u6578\u5c0f\u4e00\u4e9b\uff0c\u56e0\u70ba\u9019\u6a23\u7684\u8a71\uff0c\u5728 testing data \u53d7 noise \u5f71\u97ff\u6642\uff0c\u9810\u6e2c\u503c\u6240\u53d7\u7684\u5f71\u97ff\u6703\u66f4\u5c0f\u3002 \u6240\u4ee5\u5728 Step 2: Goodness of Function \u8a2d\u8a08 loss function \u6642\uff0c\u6211\u5011\u53ef\u4ee5\u52a0\u4e00\u500b\u6b63\u5247\u9805 $\\lambda\\Sigma w_i^2$\uff1a $$L = \\sum _ { n } \\left( \\hat { y } ^ { n } - \\left( b + \\sum w _ { i } x _ { i }^n \\right) \\right) ^ { 2 } + \\lambda \\sum \\left( w _ { i } \\right) ^ { 2 }$$ \u8d8a\u5927\u7684 $\\lambda$\uff0ctraining error \u53ef\u80fd\u6703\u8d8a\u5927\uff0c\u4f46\u6c92\u95dc\u4fc2\uff0c\u6211\u5011\u5e0c\u671b\u51fd\u6578\u5e73\u6ed1\uff0c\u4f46\u4e5f\u4e0d\u80fd\u592a\u5e73\u6ed1\uff0c\u65bc\u662f\u6211\u5011\u8abf\u6574 $\\lambda$\uff0c\u9078\u64c7\u4f7f testing error \u6700\u5c0f\u7684 $\\lambda$\u3002","title":"Lec 1 - Regression - Case Study"},{"location":"ML/1/#lecture-1-regression-case-study","text":"","title":"Lecture 1: Regression - Case Study"},{"location":"ML/1/#regression-output-a-scalar","text":"Regression \u7684\u4f8b\u5b50\u8209\u4f8b\u6709\uff1a \u9053\u74ca\u65af\u6307\u6578\u9810\u6e2c \u81ea\u52d5\u99d5\u99db\u4e2d\u7684\u65b9\u5411\u76e4\u89d2\u5ea6\u9810\u6e2c \u63a8\u85a6\u7cfb\u7d71\u4e2d\u8cfc\u8cb7\u53ef\u80fd\u6027\u7684\u9810\u6e2c","title":"Regression: Output a scalar"},{"location":"ML/1/#example-application","text":"Estimating the Combat Power (CP) of a pokemon after evolution \u4e00\u96bb\u5bf6\u53ef\u5922\u53ef\u7531 5 \u500b\u53c3\u6578\u8868\u793a\uff1a $$x = (x_{cp}, x_s, x_{hp}, x_w, x_h).$$ \u6211\u5011\u5e0c\u671b\u80fd\u8a08\u7b97\u51fa\uff0c\u7d66\u5b9a\u5bf6\u53ef\u5922\u7684\u4e00\u4e9b features $x$\uff0c\u80fd\u904e\u7b97\u51fa\u4ed6\u9032\u5316\u5f8c\u7684 CP \u503c $y$","title":"Example Application"},{"location":"ML/1/#machine-learning-steps","text":"Machine Learning \u4e3b\u8981\u6709\u4e09\u500b\u6b65\u9a5f","title":"Machine Learning Steps"},{"location":"ML/1/#step-1-model","text":"\u6211\u5011\u5047\u8a2d\u9810\u6e2c $y$ \u503c\u7684 function \u70ba $y = b + w \\cdot x_{cp}$\uff0c\u8a31\u591a\u4e0d\u540c\u7684 $w$ \u548c $b$ \u53ef\u69cb\u6210\u4e0d\u540c\u7684 $f_1, f_2, \\dots$\uff0c\u4f8b\u5982\uff1a \\begin{align} f_1: y & = 10.0 + 9.0 \\cdot x_{cp} \\\\ f_2: y & = 9.8 + 9.2 \\cdot x_{cp} \\\\ f_3: y & = -0.8 - 1.2 \\cdot x_{cp} \\\\ & \\vdots \\end{align} \u5982\u6b64\u4e00\u4f86\uff0c\u6211\u5011\u5c31\u6709\u4e86 linear model: $$y = b + \\sum w_ib_i$$ \u5176\u4e2d\uff0c $x_i: x_{cp}, x_s, x_{hp}, x_w, x_h$\uff1a\u5bf6\u53ef\u5922\u7684 features $w_i$\uff1a\u4e0d\u540c features\uff0c\u7d66\u5b83\u4e0d\u540c\u7684 weight $b$\uff1abias","title":"Step 1: Model"},{"location":"ML/1/#step-2-goodness-of-function","text":"\u6709\u4e86\u4e00\u5806 functions \u5f8c\uff0c\u6211\u5011\u5fc5\u9700\u8981\u77e5\u9053\uff0c\u9019\u4e9b functions \u6709\u591a\u597d\uff1f\u56e0\u6b64\u6211\u5011\u9700\u8981\u4e00\u500b\u8a55\u6bd4\u7684\u6a19\u6e96\uff0c\u6211\u5011\u53ef\u4ee5\u7528 Loss function $L$\uff0c\u5b83\u7684 input \u70ba $f$\uff0coutput \u70ba how bad it is\uff1f loss function \u53ef\u5b9a\u7fa9\u5982\u4e0b\uff1a \\begin{align} L(f) & = \\sum_{n = 1}^{10} (\\hat y^n - f(x_{cp}^n))^2 \\\\ L(w, b) & = \\sum_{n = 1}^{10} (\\hat y^n - (b + w \\cdot x_{cp}^n))^2 \\end{align}","title":"Step 2: Goodness of Function"},{"location":"ML/1/#step-3-best-function","text":"\u6700\u5f8c\uff0c\u6211\u5011\u8981\u9078\u51fa best function\uff0c\u5373\uff1a\u9078\u64c7 loss \u6700\u5c0f\u7684 function\uff08\u53c3\u6578\uff09\uff1a \\begin{align} f ^ { * } & = \\arg \\min _ { f } L ( f ) \\\\ w ^ { * } , b ^ { * } & = \\arg \\min _ { w , b } L ( w , b ) \\\\ & = \\arg \\min _ { w , b } \\sum _ { n = 1 } ^ { 10 } \\left( \\hat { y } ^ { n } - \\left( b + w \\cdot x _ { c p } ^ { n } \\right) \\right) ^ { 2 } \\end{align} \u6c42\u89e3\u6700\u4f73\u53c3\u6578\u7684\u65b9\u6cd5\u53ef\u4ee5\u662f Gradient Descent\u3002","title":"Step 3: Best Function"},{"location":"ML/1/#gradient-descent","text":"Gradient Descent \u7684\u6b65\u9a5f\u5982\u4e0b\uff1a \uff08\u96a8\u6a5f\uff09\u9078\u64c7\u53c3\u6578\u521d\u59cb\u503c\uff08e.g. $w^0$\uff09\uff0c\u4e0a\u6a19\u8868\u793a\u6642\u9593\u9ede \u8a08\u7b97 loss function $L(w)$ \u5728 $w = w^0$ \u4e0a\u6642\u7684 gradient\uff0c\u5373\uff1a$\\left. \\frac { d L } { d w } \\right| _ { w = w ^ { 0 } }$ \u5411\u8ca0\u68af\u5ea6\u65b9\u5411\u8fed\u4ee3\u66f4\u65b0 $\\frac{dL}{dw} \\Big|_{w = w^0} < 0 \\to$ Increase $w$ $\\frac{dL}{dw} \\Big|_{w = w^0} > 0 \\to$ Decrease $w$ \\begin{align} w ^ { 1 } & \\leftarrow w ^ { 0 } - \\eta \\left. \\frac { d L } { d w } \\right| _ { w = w ^ { 0 } } \\\\ w ^ { 2 } & \\leftarrow w ^ { 1 } - \\eta \\left. \\frac { d L } { d w } \\right| _ { w = w ^ { 1 } } \\end{align} \u5176\u4e2d\uff0c$\\eta$ \u70ba learning rate\uff0c\u7528\u4f86\u63a7\u5236\u4e00\u6b21\u8d70\u591a\u9060\u3001\u5b78\u7fd2\u901f\u5ea6 \u91cd\u8907\u9019 3 \u500b\u6b65\u9a5f\uff0c\u76f4\u5230 $w$ \u5728 local minima \u6536\u6582\u70ba\u6b62\u3002 \u9806\u5e36\u4e00\u63d0\uff0c\u53ea\u6709\u4e00\u500b\u53c3\u6578 $w$ \u6642\uff0c$\\nabla L$ \u5c31\u662f $\\frac{\\partial L}{\\partial w}$ \u70ba $L$ \u7684 gradient\uff0c\u6240\u4ee5\u624d\u53eb Gradient Descent\u3002 \u82e5\u6709\u5169\u500b\u53c3\u6578 $w, b$ \u6642\uff0c$\\nabla L = \\left[ \\begin{array} { l } { \\frac { \\partial L } { \\partial w } } \\\\ { \\frac { \\partial L } { \\partial b } } \\end{array} \\right]$\uff0c\u672c\u8cea\u4e0a\u6982\u5ff5\u9084\u662f\u76f8\u540c\u7684 Gradient Descent \u53ef\u80fd\u4f7f\u53c3\u6578\u505c\u5728\u640d\u5931\u51fd\u6578\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u3001\u5fae\u5206\u70ba $0$ \u7684\u9ede\uff0c\u6216\u8005\u5fae\u5206\u70ba\u6975\u5c0f\u503c\u7684\u9ede\u3002Linear regression \u4e2d\u4e0d\u5fc5\u64d4\u5fc3\u5c40\u90e8\u6700\u5c0f\u503c\u7684\u554f\u984c\uff0c\u56e0\u70ba loss function \u662f convex \u7684\u3002 \u5728\u5f97\u5230 best function \u4e4b\u5f8c\uff0c\u6211\u5011\u771f\u6b63\u5728\u610f\u7684\u662f\u5b83 \u5728 testing data \u4e0a\u7684\u8868\u73fe \u3002\u9078\u64c7\u4e0d\u540c\u7684 model\uff0c\u6703\u5f97\u5230\u4e0d\u540c\u7684 best function\uff0c\u5b83\u5011\u5728testing data \u4e0a\u6709\u4e0d\u540c\u8868\u73fe\u3002 \u8907\u96dc\u6a21\u578b\u7684 model space \u6db5\u84cb\u4e86\u7c21\u55ae\u6a21\u578b\u7684 model space\uff0c\u56e0\u6b64\u5728 training data \u4e0a\u7684\u932f\u8aa4\u7387\u66f4\u5c0f\uff0c\u4f46\u4e26\u4e0d\u610f\u5473\u8457\u5728 testing data \u4e0a\u932f\u8aa4\u7387\u4e5f\u6703\u5c0f\uff0c\u56e0\u70ba Model \u592a\u8907\u96dc\u6709\u5f88\u9ad8\u6a5f\u7387\u6703\u51fa\u73fe overfitting\u3002","title":"Gradient Descent"},{"location":"ML/1/#what-are-the-hidden-factors","text":"\u5982\u679c\u6211\u5011\u6536\u96c6\u66f4\u591a\u5bf6\u53ef\u5922\u9032\u5316\u524d\u5f8c\u7684 CP \u503c\u6703\u767c\u73fe\uff0c\u9032\u5316\u5f8c\u7684 CP \u503c\u4e0d\u53ea\u4f9d\u8cf4\u65bc\u9032\u5316\u524d\u7684 CP \u503c\uff0c\u53ef\u80fd\u9084\u6709\u5176\u5b83\u7684\u96b1\u85cf\u56e0\u7d20\uff08\u4f8b\u5982\uff1a\u5bf6\u53ef\u5922\u6240\u5c6c\u7684\u7269\u7a2e\uff09\u3002\u540c\u6642\u8003\u616e\u9032\u5316\u524d CP \u503c $x_{cp}$ \u548c\u7269\u7a2e $x_s$\uff0c\u6211\u5011\u53ef\u4ee5\u56de\u5230 Step 1: Model \u91cd\u65b0\u9078\u64c7\u65b0\u7684 model\uff1a \\begin{align} \\text{if } & x_s = \\text{pidgey}: & y = b_1 + w_1 \\cdot x_{cp} \\\\ \\text{if } & x_s = \\text{weedle}: & y = b_2 + w_2 \\cdot x_{cp} \\\\ \\text{if } & x_s = \\text{caterpie}: & y = b_3 + w_3 \\cdot x_{cp} \\\\ \\text{if } & x_s = \\text{eevee}: & y = b_4 + w_4 \\cdot x_{cp} \\end{align} \u9019\u4ecd\u662f\u4e00\u500b\u7dda\u6027\u6a21\u578b\uff0c\u56e0\u70ba\u5b83\u53ef\u4ee5\u5beb\u4f5c\uff1a \\begin{align} y = & b_1 \\cdot \\boldsymbol{\\delta(x_s = \\text{pidgey}) } & + & w_1 \\cdot \\boldsymbol{\\delta(x_s = \\text{pidgey}) \\cdot x_{cp}} + \\\\ & b_2 \\cdot \\boldsymbol{\\delta(x_s = \\text{weedle}) } & + & w_2 \\cdot \\boldsymbol{\\delta(x_s = \\text{weedle}) \\cdot x_{cp}} + \\\\ & b_3 \\cdot \\boldsymbol{\\delta(x_s = \\text{caterpie})} & + & w_3 \\cdot \\boldsymbol{\\delta(x_s = \\text{caterpie}) \\cdot x_{cp}} + \\\\ & b_4 \\cdot \\boldsymbol{\\delta(x_s = \\text{eevee}) } & + & w_4 \\cdot \\boldsymbol{\\delta(x_s = \\text{eevee}) \\cdot x_{cp}} \\end{align} \u4e0a\u5f0f\u4e2d\u7684\u7c97\u9ad4\u9805\u90fd\u662f linear model $y = b + \\Sigma w_i \\cdot x_i$ \u4e2d\u7684 feature $x_i$\u3002 \u9019\u500b\u6a21\u578b\u5728 testing data \u4e0a\u6709\u66f4\u597d\u7684\u8868\u73fe\uff0c\u5982\u679c\u540c\u6642\u8003\u616e\u5bf6\u53ef\u5922\u7684\u5176\u5b83\u5c6c\u6027\uff0c\u9078\u4e00\u500b\u5f88\u8907\u96dc\u7684\u6a21\u578b\uff0c\u7d50\u679c\u53ef\u80fd\u6703\u5c0e\u81f4 overfitting\u3002 \u5c0d\u7dda\u6027\u6a21\u578b\u4f86\u8aaa\uff0c\u6211\u5011\u5e0c\u671b\u9078\u51fa\u7684 best function \u80fd\u5e73\u6ed1\u4e00\u9ede\uff0c\u4e5f\u5c31\u662f\u6b0a\u91cd\u4fc2\u6578\u5c0f\u4e00\u4e9b\uff0c\u56e0\u70ba\u9019\u6a23\u7684\u8a71\uff0c\u5728 testing data \u53d7 noise \u5f71\u97ff\u6642\uff0c\u9810\u6e2c\u503c\u6240\u53d7\u7684\u5f71\u97ff\u6703\u66f4\u5c0f\u3002 \u6240\u4ee5\u5728 Step 2: Goodness of Function \u8a2d\u8a08 loss function \u6642\uff0c\u6211\u5011\u53ef\u4ee5\u52a0\u4e00\u500b\u6b63\u5247\u9805 $\\lambda\\Sigma w_i^2$\uff1a $$L = \\sum _ { n } \\left( \\hat { y } ^ { n } - \\left( b + \\sum w _ { i } x _ { i }^n \\right) \\right) ^ { 2 } + \\lambda \\sum \\left( w _ { i } \\right) ^ { 2 }$$ \u8d8a\u5927\u7684 $\\lambda$\uff0ctraining error \u53ef\u80fd\u6703\u8d8a\u5927\uff0c\u4f46\u6c92\u95dc\u4fc2\uff0c\u6211\u5011\u5e0c\u671b\u51fd\u6578\u5e73\u6ed1\uff0c\u4f46\u4e5f\u4e0d\u80fd\u592a\u5e73\u6ed1\uff0c\u65bc\u662f\u6211\u5011\u8abf\u6574 $\\lambda$\uff0c\u9078\u64c7\u4f7f testing error \u6700\u5c0f\u7684 $\\lambda$\u3002","title":"What are the hidden factors?"},{"location":"ML/2/","text":"Lecture 2: Where does the error come from? Bias and Variance of Estimator Error \u6709\u5169\u7a2e\u4f86\u6e90\uff1a bias variance \u5206\u6790 error \u7684\u4f86\u6e90\uff0c\u53ef\u4ee5\u6311\u9078\u9069\u7576\u7684\u65b9\u6cd5\u6539\u5584 model\u3002 \u4ee5\u9032\u5316\u524d\u7684\u5bf6\u53ef\u5922\u70ba\u8f38\u5165\uff0c\u4ee5\u9032\u5316\u5f8c\u7684\u771f\u5be6 CP \u503c\u70ba\u8f38\u51fa\uff0c\u771f\u5be6\u7684\u51fd\u6578\u8a18\u70ba $\\hat f$\uff08\u4e0a\u5e1d\u8996\u89d2\u624d\u77e5\u9053\uff09 \u5f9e\u8a13\u7df4\u6578\u64da\uff0c\u6211\u5011\u627e\u5230 $f^*$\uff08$\\hat f$ \u7684\u4f30\u8a08\uff09 \u7528\u8868\u683c\u4f86\u770b \u7c21\u55ae\u6a21\u578b \u8907\u96dc\u6a21\u578b variance \u5c0f \u5927 bias \u5927 \u5c0f \u7c21\u55ae\u6a21\u578b\u66f4\u5c11\u53d7 training data \u5f71\u97ff\u3002 \u8907\u96dc\u6a21\u578b\u6703\u76e1\u529b\u53bb\u64ec\u5408 training data \u7684\u8b8a\u5316\u3002 Bias \u5373 $d(\\bar f, \\hat f)$ \u7c21\u55ae\u6a21\u578b\u7684 model space \u8f03\u5c0f\uff0c\u53ef\u80fd\u6839\u672c\u6c92\u6709\u5305\u542b\u5230 target\u3002 \u5728 underfitting \u7684\u60c5\u6cc1\u4e0b\uff0cerror \u5927\u90e8\u5206\u4f86\u81ea bias\u3002 \u5728 overfitting \u7684\u60c5\u6cc1\u4e0b\uff0cerror \u5927\u90e8\u5206\u4f86\u81ea variance\u3002 \u5982\u679c model \u9023 training data \u90fd fit \u4e0d\u597d\uff0c\u90a3\u5c31\u662f underfitting \u5982\u679c model \u53ef\u4ee5 fit training data\uff0c\u4f46 testing error \u5927\uff0c\u90a3\u5c31\u662f overfitting \u5728 bias \u5927\u7684\u60c5\u6cc1\u4e0b\uff0c\u9700\u8981\u91cd\u65b0\u8a2d\u8a08 model\uff0c\u4f8b\u5982\u589e\u52a0\u66f4\u591a features\uff0c\u6216\u8457\u8b93 model \u66f4\u8907\u96dc \u5728 variance \u5927\u7684\u60c5\u6cc1\u4e0b\uff0c\u9700\u8981\u66f4\u591a\u8cc7\u6599\uff0c\u6216\u8005 regularization\u3002More data \u5f88\u6709\u6548\uff0c\u4f46\u537b\u4e0d\u4e00\u5b9a\u53ef\u884c\uff0cregularization \u5e0c\u671b\u66f2\u7dda\u5e73\u6ed1\uff0c\u4f46\u53ef\u80fd\u6703\u640d\u5bb3 bias\uff0c\u9020\u6210 model space \u7121\u6cd5\u5305\u542b target $\\hat f$\u3002 Model Selection \u5728\u9078\u64c7\u6a21\u578b\u6642\uff0c\u8981\u8003\u616e 2 \u7a2e error \u7684\u6298\u8877\uff0c\u4f7f total error \u6700\u5c0f\u3002 \u4e0d\u61c9\u9019\u6a23\u505a\uff1a \u56e0\u70ba\u9019\u6a23\u505a\uff0c\u5728 public testing error \u7684\u8868\u73fe\u4e0d\u80fd\u5b8c\u5168\u7684\u53cd\u61c9\u5728 private testing error\u3002 Cross Validation \u5c07 training set \u5206\u6210 training set \u548c validation set\uff0c\u5728 training set \u4e0a\u8a13\u7df4 model 1-3\uff0c\u9078\u64c7\u5728 validation set \u4e0a error \u6700\u5c0f\u7684 model\u3002 \u5982\u679c\u5acc training set \u4e2d data \u592a\u5c11\u7684\u8a71\uff0c\u53ef\u4ee5\u5728\u78ba\u5b9a model \u5f8c\u5728\u5168\u90e8 training set \u4e0a\u518d\u8a13\u7df4\u4e00\u904d\u8a72 model\u3002 \u9019\u6a23\u505a\uff0c\u5728 public testing set \u4e0a\u7684 error \u624d\u6703\u4ee3\u8868\u5728 private testing set \u4e0a\u7684 error\u3002 \u4e0d\u80fd\u7528 public testing set \u53bb\u8abf\u6574 model\uff01 $N$-fold Cross Validation \u5c07 training set \u5206\u6210 $N$ \u6298\uff0c\u6bcf\u6b21\u53ea\u6709\u4e00\u6298\u4f5c\u70ba validation set\uff0c\u5176\u5b83\u6298\u4f5c\u70ba training set\uff0c\u5728\u5404 model \u4e2d\u9078\u64c7 $N$ \u6b21\u8a13\u7df4\u5f97\u5230\u7684 $N$ \u500b validation error \u5747\u503c\u6700\u5c0f\u7684 model\u3002","title":"Lec 2 - Where does the error com from?"},{"location":"ML/2/#lecture-2-where-does-the-error-come-from","text":"","title":"Lecture 2: Where does the error come from?"},{"location":"ML/2/#bias-and-variance-of-estimator","text":"Error \u6709\u5169\u7a2e\u4f86\u6e90\uff1a bias variance \u5206\u6790 error \u7684\u4f86\u6e90\uff0c\u53ef\u4ee5\u6311\u9078\u9069\u7576\u7684\u65b9\u6cd5\u6539\u5584 model\u3002 \u4ee5\u9032\u5316\u524d\u7684\u5bf6\u53ef\u5922\u70ba\u8f38\u5165\uff0c\u4ee5\u9032\u5316\u5f8c\u7684\u771f\u5be6 CP \u503c\u70ba\u8f38\u51fa\uff0c\u771f\u5be6\u7684\u51fd\u6578\u8a18\u70ba $\\hat f$\uff08\u4e0a\u5e1d\u8996\u89d2\u624d\u77e5\u9053\uff09 \u5f9e\u8a13\u7df4\u6578\u64da\uff0c\u6211\u5011\u627e\u5230 $f^*$\uff08$\\hat f$ \u7684\u4f30\u8a08\uff09 \u7528\u8868\u683c\u4f86\u770b \u7c21\u55ae\u6a21\u578b \u8907\u96dc\u6a21\u578b variance \u5c0f \u5927 bias \u5927 \u5c0f \u7c21\u55ae\u6a21\u578b\u66f4\u5c11\u53d7 training data \u5f71\u97ff\u3002 \u8907\u96dc\u6a21\u578b\u6703\u76e1\u529b\u53bb\u64ec\u5408 training data \u7684\u8b8a\u5316\u3002 Bias \u5373 $d(\\bar f, \\hat f)$ \u7c21\u55ae\u6a21\u578b\u7684 model space \u8f03\u5c0f\uff0c\u53ef\u80fd\u6839\u672c\u6c92\u6709\u5305\u542b\u5230 target\u3002 \u5728 underfitting \u7684\u60c5\u6cc1\u4e0b\uff0cerror \u5927\u90e8\u5206\u4f86\u81ea bias\u3002 \u5728 overfitting \u7684\u60c5\u6cc1\u4e0b\uff0cerror \u5927\u90e8\u5206\u4f86\u81ea variance\u3002 \u5982\u679c model \u9023 training data \u90fd fit \u4e0d\u597d\uff0c\u90a3\u5c31\u662f underfitting \u5982\u679c model \u53ef\u4ee5 fit training data\uff0c\u4f46 testing error \u5927\uff0c\u90a3\u5c31\u662f overfitting \u5728 bias \u5927\u7684\u60c5\u6cc1\u4e0b\uff0c\u9700\u8981\u91cd\u65b0\u8a2d\u8a08 model\uff0c\u4f8b\u5982\u589e\u52a0\u66f4\u591a features\uff0c\u6216\u8457\u8b93 model \u66f4\u8907\u96dc \u5728 variance \u5927\u7684\u60c5\u6cc1\u4e0b\uff0c\u9700\u8981\u66f4\u591a\u8cc7\u6599\uff0c\u6216\u8005 regularization\u3002More data \u5f88\u6709\u6548\uff0c\u4f46\u537b\u4e0d\u4e00\u5b9a\u53ef\u884c\uff0cregularization \u5e0c\u671b\u66f2\u7dda\u5e73\u6ed1\uff0c\u4f46\u53ef\u80fd\u6703\u640d\u5bb3 bias\uff0c\u9020\u6210 model space \u7121\u6cd5\u5305\u542b target $\\hat f$\u3002","title":"Bias and Variance of Estimator"},{"location":"ML/2/#model-selection","text":"\u5728\u9078\u64c7\u6a21\u578b\u6642\uff0c\u8981\u8003\u616e 2 \u7a2e error \u7684\u6298\u8877\uff0c\u4f7f total error \u6700\u5c0f\u3002 \u4e0d\u61c9\u9019\u6a23\u505a\uff1a \u56e0\u70ba\u9019\u6a23\u505a\uff0c\u5728 public testing error \u7684\u8868\u73fe\u4e0d\u80fd\u5b8c\u5168\u7684\u53cd\u61c9\u5728 private testing error\u3002","title":"Model Selection"},{"location":"ML/2/#cross-validation","text":"\u5c07 training set \u5206\u6210 training set \u548c validation set\uff0c\u5728 training set \u4e0a\u8a13\u7df4 model 1-3\uff0c\u9078\u64c7\u5728 validation set \u4e0a error \u6700\u5c0f\u7684 model\u3002 \u5982\u679c\u5acc training set \u4e2d data \u592a\u5c11\u7684\u8a71\uff0c\u53ef\u4ee5\u5728\u78ba\u5b9a model \u5f8c\u5728\u5168\u90e8 training set \u4e0a\u518d\u8a13\u7df4\u4e00\u904d\u8a72 model\u3002 \u9019\u6a23\u505a\uff0c\u5728 public testing set \u4e0a\u7684 error \u624d\u6703\u4ee3\u8868\u5728 private testing set \u4e0a\u7684 error\u3002 \u4e0d\u80fd\u7528 public testing set \u53bb\u8abf\u6574 model\uff01","title":"Cross Validation"},{"location":"ML/2/#n-fold-cross-validation","text":"\u5c07 training set \u5206\u6210 $N$ \u6298\uff0c\u6bcf\u6b21\u53ea\u6709\u4e00\u6298\u4f5c\u70ba validation set\uff0c\u5176\u5b83\u6298\u4f5c\u70ba training set\uff0c\u5728\u5404 model \u4e2d\u9078\u64c7 $N$ \u6b21\u8a13\u7df4\u5f97\u5230\u7684 $N$ \u500b validation error \u5747\u503c\u6700\u5c0f\u7684 model\u3002","title":"$N$-fold Cross Validation"},{"location":"ML/3/","text":"Lecture 3: Gradient Descent Review: Gradient Descent \u5728 Gradient Descent \u4e2d\uff0c\u6211\u5011\u60f3\u89e3\u6c7a\u7684\u662f\u4e0b\u5217\u554f\u984c\uff1a $$ \\theta ^ { * } = \\arg \\min _ \\theta L ( \\theta ) $$ \u5176\u4e2d\uff0c $L$: loss function $\\theta$: parameters \u5047\u8a2d $\\theta$ \u6709\u5169\u500b\u53c3\u6578 ${\\theta_1, \\theta_2}$\uff0c\u6211\u5011\u96a8\u6a5f\u9078\u53d6 $\\theta ^ { 0 } = \\left[ \\begin{array} { l } { \\theta _ { 1 } ^ { 0 } } \\\\ { \\theta _ { 2 } ^ { 0 } } \\end{array} \\right]$\uff0c\u6240\u4ee5 $\\nabla L ( \\theta ) = \\left[ \\begin{array} { l } { \\partial L \\left( \\theta _ { 1 } \\right) / \\partial \\theta _ { 1 } } \\\\ { \\partial L \\left( \\theta _ { 2 } \\right) / \\partial \\theta _ { 2 } } \\end{array} \\right]$ \\begin{align} \\left[ \\begin{array} { l } { \\theta _ { 1 } ^ { 1 } } \\\\ { \\theta _ { 2 } ^ { 1 } } \\end{array} \\right] = \\left[ \\begin{array} { c } { \\theta _ { 1 } ^ { 0 } } \\\\ { \\theta _ { 2 } ^ { 0 } } \\end{array} \\right] - \\eta \\left[ \\begin{array} { l } { \\partial L \\left( \\theta _ { 1 } ^ { 0 } \\right) / \\partial \\theta _ { 1 } } \\\\ { \\partial L \\left( \\theta _ { 2 } ^ { 0 } \\right) / \\partial \\theta _ { 2 } } \\end{array} \\right] \\to \\theta ^ { 1 } = \\theta ^ { 0 } - \\eta \\nabla L \\left( \\theta ^ { 0 } \\right) \\\\ \\left[ \\begin{array} { l } { \\theta _ { 1 } ^ { 2 } } \\\\ { \\theta _ { 2 } ^ { 2 } } \\end{array} \\right] = \\left[ \\begin{array} { c } { \\theta _ { 1 } ^ { 1 } } \\\\ { \\theta _ { 2 } ^ { 1 } } \\end{array} \\right] - \\eta \\left[ \\begin{array} { l } { \\partial L \\left( \\theta _ { 1 } ^ { 1 } \\right) / \\partial \\theta _ { 1 } } \\\\ { \\partial L \\left( \\theta _ { 2 } ^ { 1 } \\right) / \\partial \\theta _ { 2 } } \\end{array} \\right] \\to \\theta ^ { 2 } = \\theta ^ { 1 } - \\eta \\nabla L \\left( \\theta ^ { 1 } \\right) \\end{align} Tip 1: Tuning your learning rates \u6211\u5011\u5fc5\u9700\u8981\u5c0f\u5fc3\u7684\u8a2d\u6211\u5011\u7684 learning rate $\\eta$ $$ \\theta ^i = \\theta ^{i - 1} - \\eta \\nabla L \\left( \\theta ^{i - 1} \\right) $$ Adaptive Learning Rates \u56e0\u70ba\u4e00\u958b\u59cb learning rate \u66f4\u65b0\u7684\u5f88\u6162\uff0c\u6240\u4ee5\u6211\u5011\u53ef\u4ee5\u4e00\u958b\u59cb\u5148\u7528\u8f03\u5927\u7684 learning rate \u4f86\u52a0\u901f\uff0c\u4e4b\u5f8c\u518d\u9010\u6f38\u4e0b\u964d learning rate \u4f86\u907f\u514d\u7121\u6cd5\u9054\u5230 loss \u7684 minimum\uff0c\u4f8b\u5982\uff1a$\\eta^t = \\eta / \\sqrt{t + 1}$\u3002 Adagrad Adagrad \u662f\u4e00\u500b\u5e38\u898b\u7684\u9069\u6027\u65b9\u6cd5\uff1aDivide the learning rate of each parameter by the root mean square of its previous derivatives Vanilla Gradient Descent $$w^{t + 1} \\leftarrow w^t - \\eta^t g^t$$ \u5176\u4e2d\uff0c $\\eta ^ { t } = \\frac \\eta { \\sqrt { t + 1 } }$ $g ^ { t } = \\frac { \\partial L \\left( \\theta ^ { t } \\right) } { \\partial w }$ Adagrad \\begin{align} w^{t + 1} & \\leftarrow w^t - \\frac{\\eta^t}{\\sigma^t} g^t \\\\ \\equiv w^{t + 1} & \\leftarrow w^t - \\frac{\\frac \\eta { \\sqrt { t + 1 } }}{\\sqrt { \\frac { 1 } { t + 1 } \\sum _ { i = 0 } ^ { t } \\left( g ^i \\right) ^ { 2 } }} g^t \\\\ \\equiv w^{t + 1} & \\leftarrow w^t - \\frac{\\eta}{\\sqrt{\\sum_{i = 0}^t (g^i)^2}} g^t \\\\ \\end{align} \u5176\u4e2d\uff0c $\\sigma^t$\uff1a root mean square of the previous derivatives of parameter $w$ $g^t$ \u4ee3\u8868 first derivative\uff0c\u5206\u6bcd\u5247\u662f\u5229\u7528\u6b77\u53f2\u7684 first derivative \u53bb\u4f30\u8a08 second derivative\u3002 Tip 2: Stochastic Gradient Descent Gradient Descent Loss is the summation over all training examples: $$L = \\sum_n \\Big(\\hat y^n - \\Big(b + \\sum w_i x_i^n \\Big) \\Big)^2$$ \u66f4\u65b0\u53c3\u6578\u65b9\u5f0f\uff1a $$\\theta^i = \\theta^{i - 1} - \\eta \\nabla L (\\theta^{i - 1})$$ Stochastic Gradient Descent Pick an example $x^n$ Loss for only one example: $$L = \\Big(\\hat y^n - \\Big(b + \\sum w_i x_i^n \\Big) \\Big)^2$$ \u66f4\u65b0\u53c3\u6578\u65b9\u5f0f\uff1a $$\\theta ^i = \\theta^{i - 1} - \\eta \\nabla L^n (\\theta^{i - 1})$$ Stochastic Gradient Descent \u80fd\u5920\u52a0\u901f\u8a13\u7df4\uff0c\u4e00\u822c Gradient Descent \u662f\u5728\u770b\u5230\u6240\u6709\u6a23\u672c\uff08\u8a13\u7df4\u6578\u64da\uff09\u5f8c\uff0c\u624d\u8a08\u7b97\u51fa $L$\uff0c\u518d\u66f4\u65b0\u53c3\u6578\u3002\u800c Stochastic Gradient Descent \u662f\u6bcf\u770b\u5230\u4e00\u6b21\u6a23\u672c $n$ \u5c31\u8a08\u7b97\u4e00\u6b21 $L^n$\uff0c\u4e26\u66f4\u65b0\u53c3\u6578\u3002 Tip 3: Feature Scaling \u4f8b\u5982\u6211\u5011\u7684 model \u70ba\uff1a$y = b + w_1x_1 + w_2x_2$ \u5c07 features \u5f9e\u5de6\u5716 scaling \u6210\u53f3\u5716\uff0c\u80fd\u78ba\u4fdd\u6bcf\u4e00\u500b features \u300c\u8ca2\u737b\u300d\u7684\u6578\u5b57\uff0c\u4e0d\u6703\u56e0\u70ba\u4e0d\u540c\u53c3\u6578\u9593\u7684\u5927\u5c0f\u7bc4\u570d\u800c\u5dee\u592a\u591a\u3002 \u82e5\u4e0d\u540c\u53c3\u6578\u7684\u5927\u5c0f\u7bc4\u570d\u5dee\u592a\u591a\uff0c\u90a3 loss function \u7b49\u9ad8\u7dda\u65b9\u5411\uff08\u8ca0\u68af\u5ea6\u3001\u53c3\u6578\u66f4\u65b0\u65b9\u5411\uff09\u4e0d\u6703\u6307\u5411 loss minimum\u3002Feature scaling \u8b93\u4e0d\u540c\u53c3\u6578\u7684\u53d6\u76f8\u540c\u5340\u9593\u7bc4\u570d\u7684\u503c\uff0c\u8b8a\u6210\u6b63\u5713\uff0c\u66f4\u5bb9\u6613\u65b0\u53c3\u6578\u3002 \u505a\u6cd5\u5982\u4e0b\uff0c\u5c0d $x^1, x^2, x^3, \\dots, x^r, \\dots, x^R$ \u5171 $R$ \u7b46\u8cc7\u6599\uff0c\u6bcf\u4e00\u500b\u7dad\u5ea6 $i$ \u505a\u6a19\u6e96\u5316\uff1a Theory \u63a5\u4e0b\u4f86\u8981\u8aaa\u8aaa\uff0c\u70ba\u4ec0\u9ebc Gradient Descent \u662f\u53ef\u884c\u7684\u5462\uff1f \u6211\u5011\u6b32\u89e3\u7684\u554f\u984c\u70ba\uff1a $$ \\theta ^ { * } = \\arg \\min _ { \\theta } L ( \\theta ) \\quad \\text { by gradient descent } $$ \u8003\u616e\u4ee5\u4e0b\u9673\u8ff0\u662f\u5426\u6b63\u78ba\uff1a \u6bcf\u6b21\u66f4\u65b0\u53c3\u6578\uff0c\u6211\u5011\u5c31\u5f97\u5230 $\\theta$ \u4f7f\u5f97 $L(\\theta)$ \u66f4\u5c0f\uff1a $$L \\left( \\theta ^ { 0 } \\right) > L \\left( \\theta ^ { 1 } \\right) > L \\left( \\theta ^ { 2 } \\right) > \\cdots$$ Formal Derivation \u5047\u8a2d $\\theta$ \u6709\u5169\u500b\u8b8a\u6578 ${\\theta_1, \\theta_2}$\uff0c\u6211\u5011\u5982\u4f55\u8f15\u6613\u7684\u627e\u51fa\u7d66\u5b9a\u4e00\u500b\u9ede\uff0c\u96e2\u4ed6\u6700\u8fd1\u6700\u5c0f\u7684\u503c\uff1f Taylor Series Single-variable Let $h(x)$ be any function infinitely differentiable around $x = x_0$. $$ \\begin{align} { h } ( { x } ) & = \\sum _ { k = 0 } ^ { \\infty } \\frac { { h } ^ { ( k ) } \\left( x _ { 0 } \\right) } { k ! } \\left( x - x _ { 0 } \\right) ^ { k } \\\\ & = h \\left( x _ { 0 } \\right) + h ^ { \\prime } \\left( x _ { 0 } \\right) \\left( x - x _ { 0 } \\right) + \\frac { h ^ { \\prime \\prime } \\left( x _ { 0 } \\right) } { 2 ! } \\left( x - x _ { 0 } \\right) ^ { 2 } + \\cdots \\end{align} $$ \u7576 $x$ \u63a5\u8fd1 $x_0$ \u6642\uff1a$h(x) \\approx h(x_0) + h'(x_0)(x - x_0)$ Multi-variable Let $h(x)$ be any function infinitely differentiable around $x = x_0$ and $y = y_0$. $$ h ( x , y ) = h \\left( x _ { 0 } , y _ { 0 } \\right) + \\frac { \\partial h \\left( x _ { 0 } , y _ { 0 } \\right) } { \\partial x } \\left( x - x _ { 0 } \\right) + \\frac { \\partial h \\left( x _ { 0 } , y _ { 0 } \\right) } { \\partial y } \\left( y - y _ { 0 } \\right) $$ \u7576 $x$ \u63a5\u8fd1 $x_0$ \u548c $y_0$ \u6642\uff1a$h ( x , y ) \\approx h \\left( x _ { 0 } , y _ { 0 } \\right) + \\frac { \\partial h \\left( x _ { 0 } , y _ { 0 } \\right) } { \\partial x } \\left( x - x _ { 0 } \\right) + \\frac { \\partial h \\left( x _ { 0 } , y _ { 0 } \\right) } { \\partial y } \\left( y - y _ { 0 } \\right)$ Back to Formal Derivation Based on Taylor Series: If the red circle is small enough (i.e., $\\theta_1$ and $\\theta_2$ is close to $a$ and $b$), in the red circle $$ \\begin{align} { L } ( \\theta ) & \\approx { L } ( a , b ) + \\frac { \\partial { L } ( a , b ) } { \\partial \\theta _ { 1 } } \\left( \\theta _ { 1 } - a \\right) + \\frac { \\partial { L } ( a , b ) } { \\partial \\theta _ { 2 } } \\left( \\theta _ { 2 } - b \\right) \\\\ & \\approx s + u \\left( \\theta _ { 1 } - a \\right) + v \\left( \\theta _ { 2 } - b \\right) \\end{align} $$ \u5176\u4e2d\uff0c $s = L(a, b)$ $u = \\frac { \\partial { L } ( a , b ) } { \\partial \\theta _ { 1 } } , v = \\frac { \\partial { L } ( a , b ) } { \\partial \\theta _ { 2 } }$ Goal: Find $\\theta_1$ and $\\theta_2$ in the red circle minimizing $L(\\theta)$ \u5728 red circle \u5167\u4ee3\u8868\u7684\u662f\u4e00\u500b\u5713\u7684\u65b9\u7a0b\u5f0f\uff1a $$ \\begin{align} \\left( \\theta _ { 1 } - a \\right) ^ { 2 } + \\left( \\theta _ { 2 } - b \\right) ^ { 2 } & \\le d ^ { 2 } \\\\ (\\Delta \\theta_1)^2 + (\\Delta \\theta_2)^2 & \\le d^2 \\end{align} $$ To minimize $L(\\theta)$: $$ \\begin{align} \\left[ \\begin{array} { l } { \\Delta \\theta _ { 1 } } \\\\ { \\Delta \\theta _ { 2 } } \\end{array} \\right] & = - \\eta \\left[ \\begin{array} { l } { u } \\\\ { v } \\end{array} \\right] \\\\ \\Rightarrow \\left[ \\begin{array} { l } { \\theta _ { 1 } } \\\\ { \\theta _ { 2 } } \\end{array} \\right] & = \\left[ \\begin{array} { l } { a } \\\\ { b } \\end{array} \\right] - \\eta \\left[ \\begin{array} { l } { u } \\\\ { v } \\end{array} \\right] \\\\ & = \\left[ \\begin{array} { l } { a } \\\\ { b } \\end{array} \\right] - \\eta \\left[ \\begin{array} { c } { \\frac { \\partial { L } ( a , b ) } { \\partial \\theta _ { 1 } } } \\\\ { \\frac { \\partial { L } ( a , b ) } { \\partial \\theta _ { 2 } } } \\end{array} \\right] \\end{align} $$","title":"Lec 3 - Gradient Descent"},{"location":"ML/3/#lecture-3-gradient-descent","text":"","title":"Lecture 3: Gradient Descent"},{"location":"ML/3/#review-gradient-descent","text":"\u5728 Gradient Descent \u4e2d\uff0c\u6211\u5011\u60f3\u89e3\u6c7a\u7684\u662f\u4e0b\u5217\u554f\u984c\uff1a $$ \\theta ^ { * } = \\arg \\min _ \\theta L ( \\theta ) $$ \u5176\u4e2d\uff0c $L$: loss function $\\theta$: parameters \u5047\u8a2d $\\theta$ \u6709\u5169\u500b\u53c3\u6578 ${\\theta_1, \\theta_2}$\uff0c\u6211\u5011\u96a8\u6a5f\u9078\u53d6 $\\theta ^ { 0 } = \\left[ \\begin{array} { l } { \\theta _ { 1 } ^ { 0 } } \\\\ { \\theta _ { 2 } ^ { 0 } } \\end{array} \\right]$\uff0c\u6240\u4ee5 $\\nabla L ( \\theta ) = \\left[ \\begin{array} { l } { \\partial L \\left( \\theta _ { 1 } \\right) / \\partial \\theta _ { 1 } } \\\\ { \\partial L \\left( \\theta _ { 2 } \\right) / \\partial \\theta _ { 2 } } \\end{array} \\right]$ \\begin{align} \\left[ \\begin{array} { l } { \\theta _ { 1 } ^ { 1 } } \\\\ { \\theta _ { 2 } ^ { 1 } } \\end{array} \\right] = \\left[ \\begin{array} { c } { \\theta _ { 1 } ^ { 0 } } \\\\ { \\theta _ { 2 } ^ { 0 } } \\end{array} \\right] - \\eta \\left[ \\begin{array} { l } { \\partial L \\left( \\theta _ { 1 } ^ { 0 } \\right) / \\partial \\theta _ { 1 } } \\\\ { \\partial L \\left( \\theta _ { 2 } ^ { 0 } \\right) / \\partial \\theta _ { 2 } } \\end{array} \\right] \\to \\theta ^ { 1 } = \\theta ^ { 0 } - \\eta \\nabla L \\left( \\theta ^ { 0 } \\right) \\\\ \\left[ \\begin{array} { l } { \\theta _ { 1 } ^ { 2 } } \\\\ { \\theta _ { 2 } ^ { 2 } } \\end{array} \\right] = \\left[ \\begin{array} { c } { \\theta _ { 1 } ^ { 1 } } \\\\ { \\theta _ { 2 } ^ { 1 } } \\end{array} \\right] - \\eta \\left[ \\begin{array} { l } { \\partial L \\left( \\theta _ { 1 } ^ { 1 } \\right) / \\partial \\theta _ { 1 } } \\\\ { \\partial L \\left( \\theta _ { 2 } ^ { 1 } \\right) / \\partial \\theta _ { 2 } } \\end{array} \\right] \\to \\theta ^ { 2 } = \\theta ^ { 1 } - \\eta \\nabla L \\left( \\theta ^ { 1 } \\right) \\end{align}","title":"Review: Gradient Descent"},{"location":"ML/3/#tip-1-tuning-your-learning-rates","text":"\u6211\u5011\u5fc5\u9700\u8981\u5c0f\u5fc3\u7684\u8a2d\u6211\u5011\u7684 learning rate $\\eta$ $$ \\theta ^i = \\theta ^{i - 1} - \\eta \\nabla L \\left( \\theta ^{i - 1} \\right) $$","title":"Tip 1: Tuning your learning rates"},{"location":"ML/3/#adaptive-learning-rates","text":"\u56e0\u70ba\u4e00\u958b\u59cb learning rate \u66f4\u65b0\u7684\u5f88\u6162\uff0c\u6240\u4ee5\u6211\u5011\u53ef\u4ee5\u4e00\u958b\u59cb\u5148\u7528\u8f03\u5927\u7684 learning rate \u4f86\u52a0\u901f\uff0c\u4e4b\u5f8c\u518d\u9010\u6f38\u4e0b\u964d learning rate \u4f86\u907f\u514d\u7121\u6cd5\u9054\u5230 loss \u7684 minimum\uff0c\u4f8b\u5982\uff1a$\\eta^t = \\eta / \\sqrt{t + 1}$\u3002","title":"Adaptive Learning Rates"},{"location":"ML/3/#adagrad","text":"Adagrad \u662f\u4e00\u500b\u5e38\u898b\u7684\u9069\u6027\u65b9\u6cd5\uff1aDivide the learning rate of each parameter by the root mean square of its previous derivatives Vanilla Gradient Descent $$w^{t + 1} \\leftarrow w^t - \\eta^t g^t$$ \u5176\u4e2d\uff0c $\\eta ^ { t } = \\frac \\eta { \\sqrt { t + 1 } }$ $g ^ { t } = \\frac { \\partial L \\left( \\theta ^ { t } \\right) } { \\partial w }$ Adagrad \\begin{align} w^{t + 1} & \\leftarrow w^t - \\frac{\\eta^t}{\\sigma^t} g^t \\\\ \\equiv w^{t + 1} & \\leftarrow w^t - \\frac{\\frac \\eta { \\sqrt { t + 1 } }}{\\sqrt { \\frac { 1 } { t + 1 } \\sum _ { i = 0 } ^ { t } \\left( g ^i \\right) ^ { 2 } }} g^t \\\\ \\equiv w^{t + 1} & \\leftarrow w^t - \\frac{\\eta}{\\sqrt{\\sum_{i = 0}^t (g^i)^2}} g^t \\\\ \\end{align} \u5176\u4e2d\uff0c $\\sigma^t$\uff1a root mean square of the previous derivatives of parameter $w$ $g^t$ \u4ee3\u8868 first derivative\uff0c\u5206\u6bcd\u5247\u662f\u5229\u7528\u6b77\u53f2\u7684 first derivative \u53bb\u4f30\u8a08 second derivative\u3002","title":"Adagrad"},{"location":"ML/3/#tip-2-stochastic-gradient-descent","text":"Gradient Descent Loss is the summation over all training examples: $$L = \\sum_n \\Big(\\hat y^n - \\Big(b + \\sum w_i x_i^n \\Big) \\Big)^2$$ \u66f4\u65b0\u53c3\u6578\u65b9\u5f0f\uff1a $$\\theta^i = \\theta^{i - 1} - \\eta \\nabla L (\\theta^{i - 1})$$ Stochastic Gradient Descent Pick an example $x^n$ Loss for only one example: $$L = \\Big(\\hat y^n - \\Big(b + \\sum w_i x_i^n \\Big) \\Big)^2$$ \u66f4\u65b0\u53c3\u6578\u65b9\u5f0f\uff1a $$\\theta ^i = \\theta^{i - 1} - \\eta \\nabla L^n (\\theta^{i - 1})$$ Stochastic Gradient Descent \u80fd\u5920\u52a0\u901f\u8a13\u7df4\uff0c\u4e00\u822c Gradient Descent \u662f\u5728\u770b\u5230\u6240\u6709\u6a23\u672c\uff08\u8a13\u7df4\u6578\u64da\uff09\u5f8c\uff0c\u624d\u8a08\u7b97\u51fa $L$\uff0c\u518d\u66f4\u65b0\u53c3\u6578\u3002\u800c Stochastic Gradient Descent \u662f\u6bcf\u770b\u5230\u4e00\u6b21\u6a23\u672c $n$ \u5c31\u8a08\u7b97\u4e00\u6b21 $L^n$\uff0c\u4e26\u66f4\u65b0\u53c3\u6578\u3002","title":"Tip 2: Stochastic Gradient Descent"},{"location":"ML/3/#tip-3-feature-scaling","text":"\u4f8b\u5982\u6211\u5011\u7684 model \u70ba\uff1a$y = b + w_1x_1 + w_2x_2$ \u5c07 features \u5f9e\u5de6\u5716 scaling \u6210\u53f3\u5716\uff0c\u80fd\u78ba\u4fdd\u6bcf\u4e00\u500b features \u300c\u8ca2\u737b\u300d\u7684\u6578\u5b57\uff0c\u4e0d\u6703\u56e0\u70ba\u4e0d\u540c\u53c3\u6578\u9593\u7684\u5927\u5c0f\u7bc4\u570d\u800c\u5dee\u592a\u591a\u3002 \u82e5\u4e0d\u540c\u53c3\u6578\u7684\u5927\u5c0f\u7bc4\u570d\u5dee\u592a\u591a\uff0c\u90a3 loss function \u7b49\u9ad8\u7dda\u65b9\u5411\uff08\u8ca0\u68af\u5ea6\u3001\u53c3\u6578\u66f4\u65b0\u65b9\u5411\uff09\u4e0d\u6703\u6307\u5411 loss minimum\u3002Feature scaling \u8b93\u4e0d\u540c\u53c3\u6578\u7684\u53d6\u76f8\u540c\u5340\u9593\u7bc4\u570d\u7684\u503c\uff0c\u8b8a\u6210\u6b63\u5713\uff0c\u66f4\u5bb9\u6613\u65b0\u53c3\u6578\u3002 \u505a\u6cd5\u5982\u4e0b\uff0c\u5c0d $x^1, x^2, x^3, \\dots, x^r, \\dots, x^R$ \u5171 $R$ \u7b46\u8cc7\u6599\uff0c\u6bcf\u4e00\u500b\u7dad\u5ea6 $i$ \u505a\u6a19\u6e96\u5316\uff1a","title":"Tip 3: Feature Scaling"},{"location":"ML/3/#theory","text":"\u63a5\u4e0b\u4f86\u8981\u8aaa\u8aaa\uff0c\u70ba\u4ec0\u9ebc Gradient Descent \u662f\u53ef\u884c\u7684\u5462\uff1f \u6211\u5011\u6b32\u89e3\u7684\u554f\u984c\u70ba\uff1a $$ \\theta ^ { * } = \\arg \\min _ { \\theta } L ( \\theta ) \\quad \\text { by gradient descent } $$ \u8003\u616e\u4ee5\u4e0b\u9673\u8ff0\u662f\u5426\u6b63\u78ba\uff1a \u6bcf\u6b21\u66f4\u65b0\u53c3\u6578\uff0c\u6211\u5011\u5c31\u5f97\u5230 $\\theta$ \u4f7f\u5f97 $L(\\theta)$ \u66f4\u5c0f\uff1a $$L \\left( \\theta ^ { 0 } \\right) > L \\left( \\theta ^ { 1 } \\right) > L \\left( \\theta ^ { 2 } \\right) > \\cdots$$","title":"Theory"},{"location":"ML/3/#formal-derivation","text":"\u5047\u8a2d $\\theta$ \u6709\u5169\u500b\u8b8a\u6578 ${\\theta_1, \\theta_2}$\uff0c\u6211\u5011\u5982\u4f55\u8f15\u6613\u7684\u627e\u51fa\u7d66\u5b9a\u4e00\u500b\u9ede\uff0c\u96e2\u4ed6\u6700\u8fd1\u6700\u5c0f\u7684\u503c\uff1f","title":"Formal Derivation"},{"location":"ML/3/#taylor-series","text":"","title":"Taylor Series"},{"location":"ML/3/#single-variable","text":"Let $h(x)$ be any function infinitely differentiable around $x = x_0$. $$ \\begin{align} { h } ( { x } ) & = \\sum _ { k = 0 } ^ { \\infty } \\frac { { h } ^ { ( k ) } \\left( x _ { 0 } \\right) } { k ! } \\left( x - x _ { 0 } \\right) ^ { k } \\\\ & = h \\left( x _ { 0 } \\right) + h ^ { \\prime } \\left( x _ { 0 } \\right) \\left( x - x _ { 0 } \\right) + \\frac { h ^ { \\prime \\prime } \\left( x _ { 0 } \\right) } { 2 ! } \\left( x - x _ { 0 } \\right) ^ { 2 } + \\cdots \\end{align} $$ \u7576 $x$ \u63a5\u8fd1 $x_0$ \u6642\uff1a$h(x) \\approx h(x_0) + h'(x_0)(x - x_0)$","title":"Single-variable"},{"location":"ML/3/#multi-variable","text":"Let $h(x)$ be any function infinitely differentiable around $x = x_0$ and $y = y_0$. $$ h ( x , y ) = h \\left( x _ { 0 } , y _ { 0 } \\right) + \\frac { \\partial h \\left( x _ { 0 } , y _ { 0 } \\right) } { \\partial x } \\left( x - x _ { 0 } \\right) + \\frac { \\partial h \\left( x _ { 0 } , y _ { 0 } \\right) } { \\partial y } \\left( y - y _ { 0 } \\right) $$ \u7576 $x$ \u63a5\u8fd1 $x_0$ \u548c $y_0$ \u6642\uff1a$h ( x , y ) \\approx h \\left( x _ { 0 } , y _ { 0 } \\right) + \\frac { \\partial h \\left( x _ { 0 } , y _ { 0 } \\right) } { \\partial x } \\left( x - x _ { 0 } \\right) + \\frac { \\partial h \\left( x _ { 0 } , y _ { 0 } \\right) } { \\partial y } \\left( y - y _ { 0 } \\right)$","title":"Multi-variable"},{"location":"ML/3/#back-to-formal-derivation","text":"Based on Taylor Series: If the red circle is small enough (i.e., $\\theta_1$ and $\\theta_2$ is close to $a$ and $b$), in the red circle $$ \\begin{align} { L } ( \\theta ) & \\approx { L } ( a , b ) + \\frac { \\partial { L } ( a , b ) } { \\partial \\theta _ { 1 } } \\left( \\theta _ { 1 } - a \\right) + \\frac { \\partial { L } ( a , b ) } { \\partial \\theta _ { 2 } } \\left( \\theta _ { 2 } - b \\right) \\\\ & \\approx s + u \\left( \\theta _ { 1 } - a \\right) + v \\left( \\theta _ { 2 } - b \\right) \\end{align} $$ \u5176\u4e2d\uff0c $s = L(a, b)$ $u = \\frac { \\partial { L } ( a , b ) } { \\partial \\theta _ { 1 } } , v = \\frac { \\partial { L } ( a , b ) } { \\partial \\theta _ { 2 } }$ Goal: Find $\\theta_1$ and $\\theta_2$ in the red circle minimizing $L(\\theta)$ \u5728 red circle \u5167\u4ee3\u8868\u7684\u662f\u4e00\u500b\u5713\u7684\u65b9\u7a0b\u5f0f\uff1a $$ \\begin{align} \\left( \\theta _ { 1 } - a \\right) ^ { 2 } + \\left( \\theta _ { 2 } - b \\right) ^ { 2 } & \\le d ^ { 2 } \\\\ (\\Delta \\theta_1)^2 + (\\Delta \\theta_2)^2 & \\le d^2 \\end{align} $$ To minimize $L(\\theta)$: $$ \\begin{align} \\left[ \\begin{array} { l } { \\Delta \\theta _ { 1 } } \\\\ { \\Delta \\theta _ { 2 } } \\end{array} \\right] & = - \\eta \\left[ \\begin{array} { l } { u } \\\\ { v } \\end{array} \\right] \\\\ \\Rightarrow \\left[ \\begin{array} { l } { \\theta _ { 1 } } \\\\ { \\theta _ { 2 } } \\end{array} \\right] & = \\left[ \\begin{array} { l } { a } \\\\ { b } \\end{array} \\right] - \\eta \\left[ \\begin{array} { l } { u } \\\\ { v } \\end{array} \\right] \\\\ & = \\left[ \\begin{array} { l } { a } \\\\ { b } \\end{array} \\right] - \\eta \\left[ \\begin{array} { c } { \\frac { \\partial { L } ( a , b ) } { \\partial \\theta _ { 1 } } } \\\\ { \\frac { \\partial { L } ( a , b ) } { \\partial \\theta _ { 2 } } } \\end{array} \\right] \\end{align} $$","title":"Back to Formal Derivation"},{"location":"ML/4/","text":"Lecture 4: Classification: Probabilistic Generative Model \u5206\u985e\u554f\u984c\u5e38\u88ab\u7528\u5728\u5404\u7a2e\u9818\u57df\uff0c\u4f8b\u5982\uff1a \u91d1\u878d\u4ea4\u6613\u6210\u529f\u8207\u5426 \u91ab\u5b78\u8a3a\u65b7\u5206\u985e \u624b\u5beb\u8fa8\u8b58 \u4eba\u81c9\u8fa8\u8a66 \u8ab2\u5802\u4e0a\u7684\u4f8b\u5b50\uff1a\u6839\u64da\u5bf6\u53ef\u5922\u5404\u5c6c\u6027\uff08features\uff09\u53bb\u9810\u6e2c\u5176\u985e\u5225\uff08class\uff09\u3002 Training data\uff1a\u5bf6\u53ef\u5922\u7684\u5404\u5c6c\u6027\uff08features\uff09\u53ca\u5176\u985e\u5225\uff08class\uff09\u3002 How to do Classification? Classification as Regression? \u50cf\u4e0a\u5716\u53f3\u5074\uff0c\u5373\u4f7f\u5206\u985e\u6b63\u78ba\u4e86\uff0c\u9084\u662f\u6703\u56e0\u70ba\u9060\u5927\u65bc $1$\uff0c\u9020\u6210 loss \u904e\u5927\uff0c\u56e0\u6b64\u4e0d\u9069\u5408\u4f7f\u7528 regression\u3002 \u6b64\u5916\uff0c\u7576\u985e\u5225\u8d85\u904e $2$ \u7a2e\u6642\uff0c\u4e5f\u6703\u7522\u751f\u8a31\u591a\u554f\u984c\uff0c\u56e0\u70ba\u9019\u6a23\u8868\u793a\uff0c\u5404\u985e\u5225\u4e4b\u9593\u662f\u6709\u5927\u5c0f\u9060\u8fd1\u95dc\u4fc2\u7684\uff0c\u4f46\u5728\u5206\u985e\u554f\u984c\u9019\u6a23\u662f\u4e0d\u5408\u7406\u7684\u3002 Ideal Alternatives Function (Model): $$ f(x) = \\begin{cases} g(x) > 0 & \\text{output: class 1}, \\\\ \\text{else} & \\text{output: class 2} \\end{cases} $$ Loss function: $$L(f) = \\sum_n \\delta(f(x^n) \\ne \\hat y^n)$$ Find the best function Bayes' Theorem \u5047\u8a2d\u6709\u5169\u500b\u985e\u5225\uff0c\u6211\u5011\u60f3\u8981\u4f30\u7b97\u4f86\u81ea training data \u7684\u6a5f\u7387\u3002 Given an $x$, which class does it belong to: $$ P \\left( C _ { 1 } | x \\right) = \\frac { P ( x | C _ { 1 } ) P \\left( C _ { 1 } \\right) } { P ( x | C _ { 1 } ) P \\left( C _ { 1 } \\right) + P ( x | C _ { 2 } ) P \\left( C _ { 2 } \\right) } $$ Generative Model: $$ P ( x ) = P ( x | C _ { 1 } ) P \\left( C _ { 1 } \\right) + P ( x | C _ { 2 } ) P \\left( C _ { 2 } \\right) $$ Prior \u8209\u4f8b\u4f86\u8aaa\uff1a\u6211\u5011\u6311\u9078\u51fa 79 \u96bb\u6c34\u7cfb\uff08$C_1$\uff09\u548c 61 \u96bb\u4e00\u822c\u7cfb\uff08$C_2$\uff09\u7684\u5bf6\u53ef\u5922\uff0c\u90a3\u9ebc \\begin{align} P(C_1) & = 79 / (79 + 61) = 0.56 \\\\ P(C_2) & = 61 / (79 + 61) = 0.44 \\end{align} Probability from Class \u73fe\u5728\u6211\u5011\u60f3\u8981\u77e5\u9053\u7684\u4e8b\uff0c\u5f9e\u6c34\u7cfb\uff08$C_1$\uff09\u6311\u51fa\u4e00\u96bb\u300c\u6d77\u9f9c\u300d\u7684\u6a5f\u7387\u662f\u591a\u5c11\uff1f Probability from Class - Feature \u56e0\u70ba\u5bf6\u53ef\u5922\u7684 features \u592a\u591a\uff0c\u70ba\u4e86\u65b9\u4fbf\uff0c\u6211\u5011\u53ea\u8003\u616e\u5169\u9805\uff0cDefense \u548c SP Defense\uff0c\u4e26\u5047\u8a2d\u9019 79 \u96bb\u6c34\u7cfb\u5bf6\u53ef\u5922\u90fd\u662f\u7531 Gaussian distribution sample \u51fa\u4f86\u7684\u3002 Maximum Likelihood \u56e0\u70ba\u6bcf\u500b\u9ede $x$ \u662f\u7368\u7acb\u7684\uff0c\u6240\u4ee5\u6211\u5011\u53ef\u4ee5\u900f\u904e\u4ee5\u4e0b\u9023\u4e58\u4f86\u5f97\u51fa $L(\\mu, \\Sigma)$ Likelihood of Gaussian with mean $\\mu$ and covariance matrix $\\Sigma$ = the probability of the Gaussian samples $x^1, x^2, x^3, \\dots, x^{79}$: $$L(\\mu, \\Sigma) = f_{\\mu, \\Sigma}(x^1) f_{\\mu, \\Sigma}(x^2) f_{\\mu, \\Sigma}(x^3) \\dots f_{\\mu, \\Sigma}(x^{79})$$ \u6240\u4ee5\u6211\u5011\u7684\u76ee\u7684\u662f\uff0c\u627e\u51fa\u4e00\u500b $(\\mu^*, \\Sigma^*)$ \u80fd\u8b93\u4e0a\u5f0f\u7684 likelihood \u6700\u5927\uff1a $$\\mu^*, \\Sigma^* = \\arg \\max_{\\mu, \\Sigma} L(\\mu, \\Sigma)$$ \u900f\u904e\u5fae\u5206\u53ef\u5f97\u4ee5\u4e0b\u7d50\u679c\uff1a \\begin{align} \\mu^* & = \\frac 1 {79} \\sum_{n = 1}^{79} x^n \\\\ \\Sigma^* & = \\frac 1 {79} \\sum_{n = 1}^{79} (x^n - \\mu^*)(x^n - \\mu^*)^T \\end{align} \u6240\u4ee5\u6211\u5011\u53ef\u8a08\u7b97\u51fa\uff1a \u73fe\u5728\u53ef\u4ee5\u958b\u59cb\u5206\u985e\u4e86\uff0c $$P(C_1 \\mid x) = \\frac{P(x \\mid C_1) P(C_1)}{P(x \\mid C_1) P(C_1) + P(x \\mid C_2) P(C_2)}$$ \u5176\u4e2d\uff0c $P(C_1) = 79 / (79 + 61) = 0.56$ $P(C_2) = 61 / (79 + 61) = 0.44$ $$P(x \\mid C_1) = f_{\\mu^1, \\Sigma^1}(x) = \\frac 1 {(2\\pi)^{D / 2}} \\frac 1 {|\\Sigma^1|^{1 / 2}} exp \\Big(- \\frac 1 2 (x - \\mu^1)^T (\\Sigma^1)^{-1} (x - \\mu^1) \\Big)$$ $$P(x \\mid C_2) = f_{\\mu^2, \\Sigma^2}(x) = \\frac 1 {(2\\pi)^{D / 2}} \\frac 1 {|\\Sigma^2|^{1 / 2}} exp \\Big(- \\frac 1 2 (x - \\mu^2)^T (\\Sigma^2)^{-1} (x - \\mu^2) \\Big)$$ \u82e5 $P(C_1 \\mid x) > 0.5$\uff0c$x$ \u5c31\u5c6c\u65bc class 1\uff08\u6c34\u7cfb\uff09 \u4f46\u7d50\u679c\u537b\u4e0d\u600e\u9ebc\u6a23\uff0c\u56e0\u70ba\u53c3\u6578\u592a\u591a\u4e86\uff0c\u53ef\u80fd\u6709 overfit \u7591\u616e\u3002 Modifying Model \u539f\u672c\u5c0d\u5169\u500b\u5206\u4f48\uff08\u6c34\u7cfb\u3001\u4e00\u822c\u7cfb\uff09\u90fd\u6709\u5c0d\u61c9\u7684 covariance matrix \uff08$\\Sigma_1, \\Sigma_2$\uff09\uff0c\u4f46\u9019\u6a23\u53c3\u6578\u91cf\u592a\u5927\uff0c\u53ef\u80fd\u6703 overfit\uff0c\u6240\u4ee5\u6211\u5011\u964d\u4f4e\u53c3\u6578\u91cf\u53bb\u907f\u514d overfit\uff0c\u53ea\u8003\u616e\u4e00\u500b convariance matrix\uff08$\\Sigma$\uff09\uff0c\u4fee\u6539 loss function \u5982\u4e0b\uff1a $$L(\\mu^1, \\mu^2, \\Sigma) = f_{\\mu^1, \\Sigma}(x^1) f_{\\mu^2, \\Sigma}(x^2) \\cdots f_{\\mu^1, \\Sigma}(x^{79}) \\times f_{\\mu^1, \\Sigma}(x^{80}) f_{\\mu^1, \\Sigma}(x^{81}) \\cdots f_{\\mu^1, \\Sigma}(x^{140})$$ $\\mu^1$ \u548c $\\mu^2$ \u7dad\u6301\u4e0d\u8b8a\uff0c$\\Sigma = \\frac{79}{140}\\Sigma^1 + \\frac{61}{140}\\Sigma^2$ Three Steps Function Set (Model): \u8f38\u5165\u4e00\u500b $x$\uff1a$P(C_1 \\mid x) = \\frac{P(x \\mid C_1) P(C_1)}{P(x \\mid C_1) P(C_1) + P(x \\mid C_2) P(C_2)}$ \\begin{cases} P(C_1 \\mid x) > 0.5 & \\text{output: class 1}, \\\\ \\text{else} & \\text{output: class 2} \\end{cases} Goodness of a function The mean $\\mu$ and covariance $\\Sigma$ that maximizing the likelihood (the probability of generating data) Find the test function: easy Posterior Probability \\begin{align} P(C_1 \\mid x) & = \\frac{P(x \\mid C_1) P(C_1)}{P(x \\mid C_1) P(C_1) + P(x \\mid C_2) P(C_2)} \\\\ & = \\frac 1 {1 + \\frac{P(x \\mid C_2) P(C_2)}{P(x \\mid C_1) P(C_1)}} \\\\ & = \\frac 1 {1 + e^{-z}} = \\sigma(z) \\end{align} \u5176\u4e2d\uff0c$z = \\ln \\frac{P(x \\mid C_1) P(C_1)}{P(x \\mid C_2) P(C_2)}$ (sigmoid) \u7d93\u904e\u5f88\u9577\u4e00\u6bb5\u63a8\u5c0e\uff08\u9019\u88e1\u5c31\u4e0d\u518d\u8d05\u8ff0\uff0c\u53ef\u76f4\u63a5\u770b\u8001\u5e2b \u6295\u5f71\u7247 \uff09\uff0c\u53ef\u5f97\u51fa\uff1a \\begin{align} P(C_1 \\mid x) & = \\sigma(z) \\\\ & = \\sigma(w \\cdot x + b) \\end{align} \u5176\u4e2d\uff0c $w^T = (\\mu^1 - \\mu^2)^T \\Sigma^{-1}$ $b = -\\frac 1 2 (\\mu^1)^T \\Sigma^{-1} \\mu^1 + \\frac 1 2 (\\mu^2)^T \\Sigma^{-1} \\mu^2 + \\ln \\frac{N_1}{N_2}$","title":"Lec 4 - Classification"},{"location":"ML/4/#lecture-4-classification-probabilistic-generative-model","text":"\u5206\u985e\u554f\u984c\u5e38\u88ab\u7528\u5728\u5404\u7a2e\u9818\u57df\uff0c\u4f8b\u5982\uff1a \u91d1\u878d\u4ea4\u6613\u6210\u529f\u8207\u5426 \u91ab\u5b78\u8a3a\u65b7\u5206\u985e \u624b\u5beb\u8fa8\u8b58 \u4eba\u81c9\u8fa8\u8a66 \u8ab2\u5802\u4e0a\u7684\u4f8b\u5b50\uff1a\u6839\u64da\u5bf6\u53ef\u5922\u5404\u5c6c\u6027\uff08features\uff09\u53bb\u9810\u6e2c\u5176\u985e\u5225\uff08class\uff09\u3002 Training data\uff1a\u5bf6\u53ef\u5922\u7684\u5404\u5c6c\u6027\uff08features\uff09\u53ca\u5176\u985e\u5225\uff08class\uff09\u3002","title":"Lecture 4: Classification: Probabilistic Generative Model"},{"location":"ML/4/#how-to-do-classification","text":"","title":"How to do Classification?"},{"location":"ML/4/#classification-as-regression","text":"\u50cf\u4e0a\u5716\u53f3\u5074\uff0c\u5373\u4f7f\u5206\u985e\u6b63\u78ba\u4e86\uff0c\u9084\u662f\u6703\u56e0\u70ba\u9060\u5927\u65bc $1$\uff0c\u9020\u6210 loss \u904e\u5927\uff0c\u56e0\u6b64\u4e0d\u9069\u5408\u4f7f\u7528 regression\u3002 \u6b64\u5916\uff0c\u7576\u985e\u5225\u8d85\u904e $2$ \u7a2e\u6642\uff0c\u4e5f\u6703\u7522\u751f\u8a31\u591a\u554f\u984c\uff0c\u56e0\u70ba\u9019\u6a23\u8868\u793a\uff0c\u5404\u985e\u5225\u4e4b\u9593\u662f\u6709\u5927\u5c0f\u9060\u8fd1\u95dc\u4fc2\u7684\uff0c\u4f46\u5728\u5206\u985e\u554f\u984c\u9019\u6a23\u662f\u4e0d\u5408\u7406\u7684\u3002","title":"Classification as Regression?"},{"location":"ML/4/#ideal-alternatives","text":"Function (Model): $$ f(x) = \\begin{cases} g(x) > 0 & \\text{output: class 1}, \\\\ \\text{else} & \\text{output: class 2} \\end{cases} $$ Loss function: $$L(f) = \\sum_n \\delta(f(x^n) \\ne \\hat y^n)$$ Find the best function","title":"Ideal Alternatives"},{"location":"ML/4/#bayes-theorem","text":"\u5047\u8a2d\u6709\u5169\u500b\u985e\u5225\uff0c\u6211\u5011\u60f3\u8981\u4f30\u7b97\u4f86\u81ea training data \u7684\u6a5f\u7387\u3002 Given an $x$, which class does it belong to: $$ P \\left( C _ { 1 } | x \\right) = \\frac { P ( x | C _ { 1 } ) P \\left( C _ { 1 } \\right) } { P ( x | C _ { 1 } ) P \\left( C _ { 1 } \\right) + P ( x | C _ { 2 } ) P \\left( C _ { 2 } \\right) } $$ Generative Model: $$ P ( x ) = P ( x | C _ { 1 } ) P \\left( C _ { 1 } \\right) + P ( x | C _ { 2 } ) P \\left( C _ { 2 } \\right) $$","title":"Bayes' Theorem"},{"location":"ML/4/#prior","text":"\u8209\u4f8b\u4f86\u8aaa\uff1a\u6211\u5011\u6311\u9078\u51fa 79 \u96bb\u6c34\u7cfb\uff08$C_1$\uff09\u548c 61 \u96bb\u4e00\u822c\u7cfb\uff08$C_2$\uff09\u7684\u5bf6\u53ef\u5922\uff0c\u90a3\u9ebc \\begin{align} P(C_1) & = 79 / (79 + 61) = 0.56 \\\\ P(C_2) & = 61 / (79 + 61) = 0.44 \\end{align}","title":"Prior"},{"location":"ML/4/#probability-from-class","text":"\u73fe\u5728\u6211\u5011\u60f3\u8981\u77e5\u9053\u7684\u4e8b\uff0c\u5f9e\u6c34\u7cfb\uff08$C_1$\uff09\u6311\u51fa\u4e00\u96bb\u300c\u6d77\u9f9c\u300d\u7684\u6a5f\u7387\u662f\u591a\u5c11\uff1f","title":"Probability from Class"},{"location":"ML/4/#probability-from-class-feature","text":"\u56e0\u70ba\u5bf6\u53ef\u5922\u7684 features \u592a\u591a\uff0c\u70ba\u4e86\u65b9\u4fbf\uff0c\u6211\u5011\u53ea\u8003\u616e\u5169\u9805\uff0cDefense \u548c SP Defense\uff0c\u4e26\u5047\u8a2d\u9019 79 \u96bb\u6c34\u7cfb\u5bf6\u53ef\u5922\u90fd\u662f\u7531 Gaussian distribution sample \u51fa\u4f86\u7684\u3002","title":"Probability from Class - Feature"},{"location":"ML/4/#maximum-likelihood","text":"\u56e0\u70ba\u6bcf\u500b\u9ede $x$ \u662f\u7368\u7acb\u7684\uff0c\u6240\u4ee5\u6211\u5011\u53ef\u4ee5\u900f\u904e\u4ee5\u4e0b\u9023\u4e58\u4f86\u5f97\u51fa $L(\\mu, \\Sigma)$ Likelihood of Gaussian with mean $\\mu$ and covariance matrix $\\Sigma$ = the probability of the Gaussian samples $x^1, x^2, x^3, \\dots, x^{79}$: $$L(\\mu, \\Sigma) = f_{\\mu, \\Sigma}(x^1) f_{\\mu, \\Sigma}(x^2) f_{\\mu, \\Sigma}(x^3) \\dots f_{\\mu, \\Sigma}(x^{79})$$ \u6240\u4ee5\u6211\u5011\u7684\u76ee\u7684\u662f\uff0c\u627e\u51fa\u4e00\u500b $(\\mu^*, \\Sigma^*)$ \u80fd\u8b93\u4e0a\u5f0f\u7684 likelihood \u6700\u5927\uff1a $$\\mu^*, \\Sigma^* = \\arg \\max_{\\mu, \\Sigma} L(\\mu, \\Sigma)$$ \u900f\u904e\u5fae\u5206\u53ef\u5f97\u4ee5\u4e0b\u7d50\u679c\uff1a \\begin{align} \\mu^* & = \\frac 1 {79} \\sum_{n = 1}^{79} x^n \\\\ \\Sigma^* & = \\frac 1 {79} \\sum_{n = 1}^{79} (x^n - \\mu^*)(x^n - \\mu^*)^T \\end{align} \u6240\u4ee5\u6211\u5011\u53ef\u8a08\u7b97\u51fa\uff1a \u73fe\u5728\u53ef\u4ee5\u958b\u59cb\u5206\u985e\u4e86\uff0c $$P(C_1 \\mid x) = \\frac{P(x \\mid C_1) P(C_1)}{P(x \\mid C_1) P(C_1) + P(x \\mid C_2) P(C_2)}$$ \u5176\u4e2d\uff0c $P(C_1) = 79 / (79 + 61) = 0.56$ $P(C_2) = 61 / (79 + 61) = 0.44$ $$P(x \\mid C_1) = f_{\\mu^1, \\Sigma^1}(x) = \\frac 1 {(2\\pi)^{D / 2}} \\frac 1 {|\\Sigma^1|^{1 / 2}} exp \\Big(- \\frac 1 2 (x - \\mu^1)^T (\\Sigma^1)^{-1} (x - \\mu^1) \\Big)$$ $$P(x \\mid C_2) = f_{\\mu^2, \\Sigma^2}(x) = \\frac 1 {(2\\pi)^{D / 2}} \\frac 1 {|\\Sigma^2|^{1 / 2}} exp \\Big(- \\frac 1 2 (x - \\mu^2)^T (\\Sigma^2)^{-1} (x - \\mu^2) \\Big)$$ \u82e5 $P(C_1 \\mid x) > 0.5$\uff0c$x$ \u5c31\u5c6c\u65bc class 1\uff08\u6c34\u7cfb\uff09 \u4f46\u7d50\u679c\u537b\u4e0d\u600e\u9ebc\u6a23\uff0c\u56e0\u70ba\u53c3\u6578\u592a\u591a\u4e86\uff0c\u53ef\u80fd\u6709 overfit \u7591\u616e\u3002","title":"Maximum Likelihood"},{"location":"ML/4/#modifying-model","text":"\u539f\u672c\u5c0d\u5169\u500b\u5206\u4f48\uff08\u6c34\u7cfb\u3001\u4e00\u822c\u7cfb\uff09\u90fd\u6709\u5c0d\u61c9\u7684 covariance matrix \uff08$\\Sigma_1, \\Sigma_2$\uff09\uff0c\u4f46\u9019\u6a23\u53c3\u6578\u91cf\u592a\u5927\uff0c\u53ef\u80fd\u6703 overfit\uff0c\u6240\u4ee5\u6211\u5011\u964d\u4f4e\u53c3\u6578\u91cf\u53bb\u907f\u514d overfit\uff0c\u53ea\u8003\u616e\u4e00\u500b convariance matrix\uff08$\\Sigma$\uff09\uff0c\u4fee\u6539 loss function \u5982\u4e0b\uff1a $$L(\\mu^1, \\mu^2, \\Sigma) = f_{\\mu^1, \\Sigma}(x^1) f_{\\mu^2, \\Sigma}(x^2) \\cdots f_{\\mu^1, \\Sigma}(x^{79}) \\times f_{\\mu^1, \\Sigma}(x^{80}) f_{\\mu^1, \\Sigma}(x^{81}) \\cdots f_{\\mu^1, \\Sigma}(x^{140})$$ $\\mu^1$ \u548c $\\mu^2$ \u7dad\u6301\u4e0d\u8b8a\uff0c$\\Sigma = \\frac{79}{140}\\Sigma^1 + \\frac{61}{140}\\Sigma^2$","title":"Modifying Model"},{"location":"ML/4/#three-steps","text":"Function Set (Model): \u8f38\u5165\u4e00\u500b $x$\uff1a$P(C_1 \\mid x) = \\frac{P(x \\mid C_1) P(C_1)}{P(x \\mid C_1) P(C_1) + P(x \\mid C_2) P(C_2)}$ \\begin{cases} P(C_1 \\mid x) > 0.5 & \\text{output: class 1}, \\\\ \\text{else} & \\text{output: class 2} \\end{cases} Goodness of a function The mean $\\mu$ and covariance $\\Sigma$ that maximizing the likelihood (the probability of generating data) Find the test function: easy","title":"Three Steps"},{"location":"ML/4/#posterior-probability","text":"\\begin{align} P(C_1 \\mid x) & = \\frac{P(x \\mid C_1) P(C_1)}{P(x \\mid C_1) P(C_1) + P(x \\mid C_2) P(C_2)} \\\\ & = \\frac 1 {1 + \\frac{P(x \\mid C_2) P(C_2)}{P(x \\mid C_1) P(C_1)}} \\\\ & = \\frac 1 {1 + e^{-z}} = \\sigma(z) \\end{align} \u5176\u4e2d\uff0c$z = \\ln \\frac{P(x \\mid C_1) P(C_1)}{P(x \\mid C_2) P(C_2)}$ (sigmoid) \u7d93\u904e\u5f88\u9577\u4e00\u6bb5\u63a8\u5c0e\uff08\u9019\u88e1\u5c31\u4e0d\u518d\u8d05\u8ff0\uff0c\u53ef\u76f4\u63a5\u770b\u8001\u5e2b \u6295\u5f71\u7247 \uff09\uff0c\u53ef\u5f97\u51fa\uff1a \\begin{align} P(C_1 \\mid x) & = \\sigma(z) \\\\ & = \\sigma(w \\cdot x + b) \\end{align} \u5176\u4e2d\uff0c $w^T = (\\mu^1 - \\mu^2)^T \\Sigma^{-1}$ $b = -\\frac 1 2 (\\mu^1)^T \\Sigma^{-1} \\mu^1 + \\frac 1 2 (\\mu^2)^T \\Sigma^{-1} \\mu^2 + \\ln \\frac{N_1}{N_2}$","title":"Posterior Probability"},{"location":"ML/5/","text":"Lecture 5: Logistic Regression Three Steps Step 1: Function Set \u6211\u5011\u60f3\u627e\u7684\u662f\uff1a$P_{w, b} (C_1 \\mid x)$ $$ f_{w, b} = \\begin{cases} P_{w, b} (C_1 \\mid x) \\ge 0.5 & \\text{output: } C_1, \\\\ \\text{else} & \\text{output: } C_2 \\end{cases} $$ \\begin{align} P_{w, b} (C_1 \\mid x) & = \\sigma(z) \\\\ & = \\sigma(w \\cdot x + b) \\end{align} \u6211\u5011\u6703\u6709\u4ee5\u4e0b\u7684 Function set\uff08\u5305\u542b\u5404\u7a2e\u4e0d\u540c\u7684 $w$ \u548c $b$\uff09: $$f_{w, b}(x) = P_{w, b} (C_1 \\mid x)$$ Step 2: Goodness of a Function \u82e5 Training Data \u70ba\uff1a $$ \\begin{array} { l l l l } { x ^ { 1 } } & { x ^ { 2 } } & { x ^ { 3 } } & \\cdots & { x ^ { N } } \\\\ { C _ { 1 } } & { C _ { 1 } } & { C _ { 2 } } & \\cdots & { C _ { 1 } } \\end{array} $$ \u63a5\u4e0b\u4f86\u4e00\u6a23\u8981\u6c7a\u5b9a\u4e00\u500b function \u7684\u597d\u58de\uff0c\u5047\u8a2d data \u662f\u5f9e $f_{w, b}(x) = P_{w, b} (C_1 \\mid x)$ \u7522\u751f\u3002 Given a set of $w$ and $b$, what is its probability of generating the data? $$ L ( w , b ) = f _ { w , b } \\left( x ^ { 1 } \\right) f _ { w , b } \\left( x ^ { 2 } \\right) \\left( 1 - f _ { w , b } \\left( x ^ { 3 } \\right) \\right) \\cdots f _ { w , b } \\left( x ^ { N } \\right) $$ The most likely $w^*$ and $b^*$ is the one with the largest $L(w, b)$: $$ w ^ { * } , b ^ { * } = \\arg \\max _ { w , b } L ( w , b ) $$ \u6c42\u4e0a\u5f0f\u7b49\u540c\u65bc\u6c42\uff1a $$ \\begin{align} w ^ { * } , b ^ { * } & = \\arg \\min _ { w , b } - \\ln L ( w , b ) \\\\ & = \\arg \\min _ { w , b } - \\ln f _ { w , b } \\left( x ^ { 1 } \\right) - \\ln f _ { w , b } \\left( x ^ { 2 } \\right) - \\ln \\left( 1 - f _ { w , b } \\left( x ^ { 3 } \\right) \\right) \\cdots - \\ln f _ { w , b } \\left( x ^ { N } \\right) \\\\ & = \\arg \\min _ { w , b } \\sum _ { n } - \\left[ \\hat { y } ^ { n } \\ln f _ { w , b } \\left( x ^ { n } \\right) + \\left( 1 - \\hat { y } ^ { n } \\right) \\ln \\left( 1 - f _ { w , b } \\left( x ^ { n } \\right) \\right) \\right] \\end{align} $$ \u5176\u4e2d\uff0c $\\hat y^n$\uff1a$1$ for $C_1$, $0$ for $C_2$ $\\Sigma_n$ \u9805\u7b49\u540c\u65bc\u6c42\u4ee5\u4e0b\u5169\u500b\u5206\u4f48 $p$\u3001$q$ \u7684 cross entropy Distribution $p$: $$ \\begin{array} { l } { p ( x = 1 ) = \\hat { y } ^ { n } } \\\\ { p ( x = 0 ) = 1 - \\hat { y } ^ { n } } \\end{array} $$ Distribution $q$: $$ \\begin{array} { l } {{ q } ( x = 1 ) = f \\left( x ^ { n } \\right) } \\\\ {{ q } ( x = 0 ) = 1 - f \\left( x ^ { n } \\right) } \\end{array} $$ $$ H ( p , q ) = - \\sum _ { x } p ( x ) \\ln ( q ( x ) ) $$ \u554f\u984c\u662f\uff1a\u70ba\u4ec0\u9ebc\u5728 Logistic Regression \u4e0d\u7528 rms \u7576 loss function \u4e86\uff1f \u7b54\uff1a\u505a\u5fae\u5206\u5f8c\uff0c\u67d0\u4e9b\u9805\u6b21\u6703\u70ba $0$\uff0c\u5c0e\u81f4\u53c3\u6578\u66f4\u65b0\u904e\u6162\u3002 Step 3: Find the best function \u6c7a\u5b9a\u5b8c loss function \u5f8c\uff0c\u6211\u5011\u8981\u5f9e\u4e00\u500b set \u4e2d\uff0c\u627e\u51fa best function\uff0c\u5148\u5c0d $w_i$ \u9032\u884c\u504f\u5fae\u5206\uff1a $$ \\frac { - \\ln L ( w , b ) } { \\partial w _ { i } } = \\sum _ { n } - \\left[ \\hat { y } ^ { n } \\frac { \\ln f _ { w , b } \\left( x ^ { n } \\right) } { \\partial w _ { i } } + \\left( 1 - \\hat { y } ^ { n } \\right) \\frac { \\ln \\left( 1 - f _ { w , b } \\left( x ^ { n } \\right) \\right) } { \\partial w _ { i } } \\right] \\tag{*} $$ \u5176\u4e2d\uff0c $f _ { w , b } ( x ) = \\sigma ( z ) = 1 / (1 + e^{-z})$ $z = w \\cdot x + b = \\sum _ { i } w _ { i } x _ { i } + b$ $$ \\begin{align} \\frac { \\partial \\ln f _ { w , b } ( x ) } { \\partial w _ { i } } & = \\frac { \\partial \\ln f _ { w , b } ( x ) } { \\partial z } \\frac { \\partial z } { \\partial w _ { i } } \\\\ & = \\frac { \\partial \\ln \\sigma ( z ) } { \\partial z } \\cdot x_i \\\\ & = \\frac { 1 } { \\sigma ( z ) } \\frac { \\partial \\sigma ( z ) } { \\partial z } \\cdot x_i \\\\ & = \\frac{1}{\\sigma(z)} \\sigma(z) (1 - \\sigma(z)) \\cdot x_i \\\\ & = (1 - \\sigma(z)) \\cdot x_i \\\\ & = \\left( 1 - f _ { w , b } \\left( x \\right) \\right) \\cdot x_i \\end{align} $$ $$ \\begin{align} \\frac { \\partial \\ln \\left( 1 - f _ { w , b } ( x ) \\right) } { \\partial w _ { i } } & = \\frac { \\partial \\ln \\left( 1 - f _ { w , b } ( x ) \\right) } { \\partial z } \\frac { \\partial z } { \\partial w _ { i } } \\\\ & = \\frac { \\partial \\ln ( 1 - \\sigma ( z ) ) } { \\partial z } \\cdot x_i \\\\ & = - \\frac { 1 } { 1 - \\sigma ( z ) } \\frac { \\partial \\sigma ( z ) } { \\partial z } \\cdot x_i \\\\ & = - \\frac{1}{1 - \\sigma(z)} \\sigma(z) (1 - \\sigma(z)) \\cdot x_i \\\\ & = -\\sigma(z) \\cdot x_i \\\\ & = -f _ { w , b } \\left( x \\right) \\cdot x_i \\end{align} $$ \u900f\u904e\u4e0a\u9762\u8a08\u7b97\u51fa\u4f86\u7684\u7d50\u679c\uff0c\u6211\u5011\u53ef\u4ee5\u5c0d $*$ \u5f0f\u9032\u884c\u4ee3\u63db\uff1a $$ \\begin{align} \\frac { - \\ln L ( w , b ) } { \\partial w _ { i } } & = \\sum _ { n } - \\left[ \\hat { y } ^ { n } \\frac { \\ln f _ { w , b } \\left( x ^ { n } \\right) } { \\partial w _ { i } } + \\left( 1 - \\hat { y } ^ { n } \\right) \\frac { \\ln \\left( 1 - f _ { w , b } \\left( x ^ { n } \\right) \\right) } { \\partial w _ { i } } \\right] \\\\ & = \\sum _ { n } - \\left[ \\hat { y } ^ { n } \\left( 1 - f _ { w , b } \\left( x ^ { n } \\right) \\right) x _ { i } ^ { n } - \\left( 1 - \\hat { y } ^ { n } \\right) f _ { w , b } \\left( x ^ { n } \\right) x _ { i } ^ { n } \\right] \\\\ & = \\sum _ { n } - \\left[ \\hat { y } ^ { n } - \\hat { y } ^ { n } f _ { w , b } \\left( x ^ { n } \\right) - f _ { w , b } \\left( x ^ { n } \\right) + \\hat { y } ^ { n } f _ { W , b } \\left( x ^ { n } \\right) \\right] x _ { i } ^ { n } \\\\ & = \\sum _ { n } - \\left( \\hat { y } ^ { n } - f _ { w , b } \\left( x ^ { n } \\right) \\right) x _ { i } ^ { n } \\end{align} $$ \u53c3\u6578\u66f4\u65b0\u65b9\u5f0f\u5982\u4e0b\uff1a $$ w _ { i } \\leftarrow w _ { i } - \\eta \\sum _ { n } - \\left( \\hat { y } ^ { n } - f _ { w , b } \\left( x ^ { n } \\right) \\right) x _ { i } ^ { n } $$ Logistic v.s. Linear \u4e0b\u9762\u5c0d Logistic Regression \u548c Linear Regression \u505a\u6bd4\u8f03\uff1a Logistic Regression Linear Regression $f_{w, b}(x)$ $\\sigma (\\sum_i w_ix_i + b)$ $\\sum_i w_ix_i + b$ Output between $0$ and $1$ any value Training data $(x^n, \\hat y^n)$ $(x^n, \\hat y^n)$ $\\hat y^n$ $1$ for class 1, $0$ for class 2 a real number $L(f)$ $\\sum_n C(f(x^n), \\hat y^n) = \\sum_n - \\left[ \\hat { y } ^ { n } \\ln f \\left( x ^ { n } \\right) + \\left( 1 - \\hat { y } ^ { n } \\right) \\ln \\left( 1 - f \\left( x ^ { n } \\right) \\right) \\right]$ $\\frac 1 2 \\sum_n (f(x^n) - \\hat y^n)^2$ update method $w_i \\leftarrow w_i - \\eta \\sum_n - (\\hat y^n - f_{w, b}(x^n)) x_i^n$ Why not Logistic Regression with Square Error? \u82e5\u6211\u5011\u7684 loss function \u6539\u5beb\u6210 square error \u7248\u672c\uff1a $$ L ( f ) = \\frac { 1 } { 2 } \\sum _ { n } \\left( f _ { w , b } \\left( x ^ { n } \\right) - \\hat { y } ^ { n } \\right) ^ { 2 } $$ \u5728 Step 3: Find the best function \u5c0d $w_i$ \u505a\u5fae\u5206\u6642\uff1a $$ \\begin{align} \\frac { \\partial \\left( f _ { w , b } ( x ) - \\hat { y } \\right) ^ { 2 } } { \\partial w _ { i } } & = 2 \\left( f _ { w , b } ( x ) - \\hat { y } \\right) \\frac { \\partial f _ { w , b } ( x ) } { \\partial z } \\frac { \\partial z } { \\partial w _ { i } } \\\\ & = 2 \\left( f _ { w , b } ( x ) - \\hat { y } \\right) f _ { w , b } ( x ) \\left( 1 - f _ { w , b } ( x ) \\right) x _ { i } \\end{align} $$ \u4e0d\u8ad6 $\\hat y^n = 1$ \u6216 $\\hat y^n = 0$\uff0c\u90fd\u53ef\u80fd\u5c0e\u81f4 $\\partial L / \\partial w _ { i } = 0$\uff0c\u7121\u6cd5\u6709\u6548\u66f4\u65b0\u53c3\u6578\u3002 Generative v.s. Discriminative Benefit of generative model With the assumption of probability distribution, less training data is needed With the assumption of probability distribution, more robust to the noise Priors and class-dependent probabilities can be estimated from different sources. Multi-class Classification \u6211\u5011\u7528 3 \u500b classes \u4f86\u505a\u4f8b\u5b50\uff1a Limitation of Logistic Regression \u7d66\u5b9a\u4e0a\u8ff0\u7684 4 \u500b features\uff0c\u6211\u5011\u7121\u6cd5\u6709\u6548\u7684\u5206\u985e\uff0c\u56e0\u70ba\u6c92\u6709\u4efb\u4f55\u7dda\u6027\u7684\u5207\u6cd5\u662f\u53ef\u4ee5\u5b8c\u7f8e\u5c07\u7d05\u9ede\u548c\u85cd\u9ede\u5206\u958b\uff0c\u56e0\u6b64\u6211\u5011\u6709\u4ee5\u4e0b\u5169\u7a2e\u65b9\u6cd5\uff1a Feature Transformation: Not always easy to find a good transformation Cascading logistic regression models","title":"Lec 5 - Logistic Regression"},{"location":"ML/5/#lecture-5-logistic-regression","text":"","title":"Lecture 5: Logistic Regression"},{"location":"ML/5/#three-steps","text":"","title":"Three Steps"},{"location":"ML/5/#step-1-function-set","text":"\u6211\u5011\u60f3\u627e\u7684\u662f\uff1a$P_{w, b} (C_1 \\mid x)$ $$ f_{w, b} = \\begin{cases} P_{w, b} (C_1 \\mid x) \\ge 0.5 & \\text{output: } C_1, \\\\ \\text{else} & \\text{output: } C_2 \\end{cases} $$ \\begin{align} P_{w, b} (C_1 \\mid x) & = \\sigma(z) \\\\ & = \\sigma(w \\cdot x + b) \\end{align} \u6211\u5011\u6703\u6709\u4ee5\u4e0b\u7684 Function set\uff08\u5305\u542b\u5404\u7a2e\u4e0d\u540c\u7684 $w$ \u548c $b$\uff09: $$f_{w, b}(x) = P_{w, b} (C_1 \\mid x)$$","title":"Step 1: Function Set"},{"location":"ML/5/#step-2-goodness-of-a-function","text":"\u82e5 Training Data \u70ba\uff1a $$ \\begin{array} { l l l l } { x ^ { 1 } } & { x ^ { 2 } } & { x ^ { 3 } } & \\cdots & { x ^ { N } } \\\\ { C _ { 1 } } & { C _ { 1 } } & { C _ { 2 } } & \\cdots & { C _ { 1 } } \\end{array} $$ \u63a5\u4e0b\u4f86\u4e00\u6a23\u8981\u6c7a\u5b9a\u4e00\u500b function \u7684\u597d\u58de\uff0c\u5047\u8a2d data \u662f\u5f9e $f_{w, b}(x) = P_{w, b} (C_1 \\mid x)$ \u7522\u751f\u3002 Given a set of $w$ and $b$, what is its probability of generating the data? $$ L ( w , b ) = f _ { w , b } \\left( x ^ { 1 } \\right) f _ { w , b } \\left( x ^ { 2 } \\right) \\left( 1 - f _ { w , b } \\left( x ^ { 3 } \\right) \\right) \\cdots f _ { w , b } \\left( x ^ { N } \\right) $$ The most likely $w^*$ and $b^*$ is the one with the largest $L(w, b)$: $$ w ^ { * } , b ^ { * } = \\arg \\max _ { w , b } L ( w , b ) $$ \u6c42\u4e0a\u5f0f\u7b49\u540c\u65bc\u6c42\uff1a $$ \\begin{align} w ^ { * } , b ^ { * } & = \\arg \\min _ { w , b } - \\ln L ( w , b ) \\\\ & = \\arg \\min _ { w , b } - \\ln f _ { w , b } \\left( x ^ { 1 } \\right) - \\ln f _ { w , b } \\left( x ^ { 2 } \\right) - \\ln \\left( 1 - f _ { w , b } \\left( x ^ { 3 } \\right) \\right) \\cdots - \\ln f _ { w , b } \\left( x ^ { N } \\right) \\\\ & = \\arg \\min _ { w , b } \\sum _ { n } - \\left[ \\hat { y } ^ { n } \\ln f _ { w , b } \\left( x ^ { n } \\right) + \\left( 1 - \\hat { y } ^ { n } \\right) \\ln \\left( 1 - f _ { w , b } \\left( x ^ { n } \\right) \\right) \\right] \\end{align} $$ \u5176\u4e2d\uff0c $\\hat y^n$\uff1a$1$ for $C_1$, $0$ for $C_2$ $\\Sigma_n$ \u9805\u7b49\u540c\u65bc\u6c42\u4ee5\u4e0b\u5169\u500b\u5206\u4f48 $p$\u3001$q$ \u7684 cross entropy Distribution $p$: $$ \\begin{array} { l } { p ( x = 1 ) = \\hat { y } ^ { n } } \\\\ { p ( x = 0 ) = 1 - \\hat { y } ^ { n } } \\end{array} $$ Distribution $q$: $$ \\begin{array} { l } {{ q } ( x = 1 ) = f \\left( x ^ { n } \\right) } \\\\ {{ q } ( x = 0 ) = 1 - f \\left( x ^ { n } \\right) } \\end{array} $$ $$ H ( p , q ) = - \\sum _ { x } p ( x ) \\ln ( q ( x ) ) $$ \u554f\u984c\u662f\uff1a\u70ba\u4ec0\u9ebc\u5728 Logistic Regression \u4e0d\u7528 rms \u7576 loss function \u4e86\uff1f \u7b54\uff1a\u505a\u5fae\u5206\u5f8c\uff0c\u67d0\u4e9b\u9805\u6b21\u6703\u70ba $0$\uff0c\u5c0e\u81f4\u53c3\u6578\u66f4\u65b0\u904e\u6162\u3002","title":"Step 2: Goodness of a Function"},{"location":"ML/5/#step-3-find-the-best-function","text":"\u6c7a\u5b9a\u5b8c loss function \u5f8c\uff0c\u6211\u5011\u8981\u5f9e\u4e00\u500b set \u4e2d\uff0c\u627e\u51fa best function\uff0c\u5148\u5c0d $w_i$ \u9032\u884c\u504f\u5fae\u5206\uff1a $$ \\frac { - \\ln L ( w , b ) } { \\partial w _ { i } } = \\sum _ { n } - \\left[ \\hat { y } ^ { n } \\frac { \\ln f _ { w , b } \\left( x ^ { n } \\right) } { \\partial w _ { i } } + \\left( 1 - \\hat { y } ^ { n } \\right) \\frac { \\ln \\left( 1 - f _ { w , b } \\left( x ^ { n } \\right) \\right) } { \\partial w _ { i } } \\right] \\tag{*} $$ \u5176\u4e2d\uff0c $f _ { w , b } ( x ) = \\sigma ( z ) = 1 / (1 + e^{-z})$ $z = w \\cdot x + b = \\sum _ { i } w _ { i } x _ { i } + b$ $$ \\begin{align} \\frac { \\partial \\ln f _ { w , b } ( x ) } { \\partial w _ { i } } & = \\frac { \\partial \\ln f _ { w , b } ( x ) } { \\partial z } \\frac { \\partial z } { \\partial w _ { i } } \\\\ & = \\frac { \\partial \\ln \\sigma ( z ) } { \\partial z } \\cdot x_i \\\\ & = \\frac { 1 } { \\sigma ( z ) } \\frac { \\partial \\sigma ( z ) } { \\partial z } \\cdot x_i \\\\ & = \\frac{1}{\\sigma(z)} \\sigma(z) (1 - \\sigma(z)) \\cdot x_i \\\\ & = (1 - \\sigma(z)) \\cdot x_i \\\\ & = \\left( 1 - f _ { w , b } \\left( x \\right) \\right) \\cdot x_i \\end{align} $$ $$ \\begin{align} \\frac { \\partial \\ln \\left( 1 - f _ { w , b } ( x ) \\right) } { \\partial w _ { i } } & = \\frac { \\partial \\ln \\left( 1 - f _ { w , b } ( x ) \\right) } { \\partial z } \\frac { \\partial z } { \\partial w _ { i } } \\\\ & = \\frac { \\partial \\ln ( 1 - \\sigma ( z ) ) } { \\partial z } \\cdot x_i \\\\ & = - \\frac { 1 } { 1 - \\sigma ( z ) } \\frac { \\partial \\sigma ( z ) } { \\partial z } \\cdot x_i \\\\ & = - \\frac{1}{1 - \\sigma(z)} \\sigma(z) (1 - \\sigma(z)) \\cdot x_i \\\\ & = -\\sigma(z) \\cdot x_i \\\\ & = -f _ { w , b } \\left( x \\right) \\cdot x_i \\end{align} $$ \u900f\u904e\u4e0a\u9762\u8a08\u7b97\u51fa\u4f86\u7684\u7d50\u679c\uff0c\u6211\u5011\u53ef\u4ee5\u5c0d $*$ \u5f0f\u9032\u884c\u4ee3\u63db\uff1a $$ \\begin{align} \\frac { - \\ln L ( w , b ) } { \\partial w _ { i } } & = \\sum _ { n } - \\left[ \\hat { y } ^ { n } \\frac { \\ln f _ { w , b } \\left( x ^ { n } \\right) } { \\partial w _ { i } } + \\left( 1 - \\hat { y } ^ { n } \\right) \\frac { \\ln \\left( 1 - f _ { w , b } \\left( x ^ { n } \\right) \\right) } { \\partial w _ { i } } \\right] \\\\ & = \\sum _ { n } - \\left[ \\hat { y } ^ { n } \\left( 1 - f _ { w , b } \\left( x ^ { n } \\right) \\right) x _ { i } ^ { n } - \\left( 1 - \\hat { y } ^ { n } \\right) f _ { w , b } \\left( x ^ { n } \\right) x _ { i } ^ { n } \\right] \\\\ & = \\sum _ { n } - \\left[ \\hat { y } ^ { n } - \\hat { y } ^ { n } f _ { w , b } \\left( x ^ { n } \\right) - f _ { w , b } \\left( x ^ { n } \\right) + \\hat { y } ^ { n } f _ { W , b } \\left( x ^ { n } \\right) \\right] x _ { i } ^ { n } \\\\ & = \\sum _ { n } - \\left( \\hat { y } ^ { n } - f _ { w , b } \\left( x ^ { n } \\right) \\right) x _ { i } ^ { n } \\end{align} $$ \u53c3\u6578\u66f4\u65b0\u65b9\u5f0f\u5982\u4e0b\uff1a $$ w _ { i } \\leftarrow w _ { i } - \\eta \\sum _ { n } - \\left( \\hat { y } ^ { n } - f _ { w , b } \\left( x ^ { n } \\right) \\right) x _ { i } ^ { n } $$","title":"Step 3: Find the best function"},{"location":"ML/5/#logistic-vs-linear","text":"\u4e0b\u9762\u5c0d Logistic Regression \u548c Linear Regression \u505a\u6bd4\u8f03\uff1a Logistic Regression Linear Regression $f_{w, b}(x)$ $\\sigma (\\sum_i w_ix_i + b)$ $\\sum_i w_ix_i + b$ Output between $0$ and $1$ any value Training data $(x^n, \\hat y^n)$ $(x^n, \\hat y^n)$ $\\hat y^n$ $1$ for class 1, $0$ for class 2 a real number $L(f)$ $\\sum_n C(f(x^n), \\hat y^n) = \\sum_n - \\left[ \\hat { y } ^ { n } \\ln f \\left( x ^ { n } \\right) + \\left( 1 - \\hat { y } ^ { n } \\right) \\ln \\left( 1 - f \\left( x ^ { n } \\right) \\right) \\right]$ $\\frac 1 2 \\sum_n (f(x^n) - \\hat y^n)^2$ update method $w_i \\leftarrow w_i - \\eta \\sum_n - (\\hat y^n - f_{w, b}(x^n)) x_i^n$","title":"Logistic v.s. Linear"},{"location":"ML/5/#why-not-logistic-regression-with-square-error","text":"\u82e5\u6211\u5011\u7684 loss function \u6539\u5beb\u6210 square error \u7248\u672c\uff1a $$ L ( f ) = \\frac { 1 } { 2 } \\sum _ { n } \\left( f _ { w , b } \\left( x ^ { n } \\right) - \\hat { y } ^ { n } \\right) ^ { 2 } $$ \u5728 Step 3: Find the best function \u5c0d $w_i$ \u505a\u5fae\u5206\u6642\uff1a $$ \\begin{align} \\frac { \\partial \\left( f _ { w , b } ( x ) - \\hat { y } \\right) ^ { 2 } } { \\partial w _ { i } } & = 2 \\left( f _ { w , b } ( x ) - \\hat { y } \\right) \\frac { \\partial f _ { w , b } ( x ) } { \\partial z } \\frac { \\partial z } { \\partial w _ { i } } \\\\ & = 2 \\left( f _ { w , b } ( x ) - \\hat { y } \\right) f _ { w , b } ( x ) \\left( 1 - f _ { w , b } ( x ) \\right) x _ { i } \\end{align} $$ \u4e0d\u8ad6 $\\hat y^n = 1$ \u6216 $\\hat y^n = 0$\uff0c\u90fd\u53ef\u80fd\u5c0e\u81f4 $\\partial L / \\partial w _ { i } = 0$\uff0c\u7121\u6cd5\u6709\u6548\u66f4\u65b0\u53c3\u6578\u3002","title":"Why not Logistic Regression with Square Error?"},{"location":"ML/5/#generative-vs-discriminative","text":"Benefit of generative model With the assumption of probability distribution, less training data is needed With the assumption of probability distribution, more robust to the noise Priors and class-dependent probabilities can be estimated from different sources.","title":"Generative v.s. Discriminative"},{"location":"ML/5/#multi-class-classification","text":"\u6211\u5011\u7528 3 \u500b classes \u4f86\u505a\u4f8b\u5b50\uff1a","title":"Multi-class Classification"},{"location":"ML/5/#limitation-of-logistic-regression","text":"\u7d66\u5b9a\u4e0a\u8ff0\u7684 4 \u500b features\uff0c\u6211\u5011\u7121\u6cd5\u6709\u6548\u7684\u5206\u985e\uff0c\u56e0\u70ba\u6c92\u6709\u4efb\u4f55\u7dda\u6027\u7684\u5207\u6cd5\u662f\u53ef\u4ee5\u5b8c\u7f8e\u5c07\u7d05\u9ede\u548c\u85cd\u9ede\u5206\u958b\uff0c\u56e0\u6b64\u6211\u5011\u6709\u4ee5\u4e0b\u5169\u7a2e\u65b9\u6cd5\uff1a Feature Transformation: Not always easy to find a good transformation Cascading logistic regression models","title":"Limitation of Logistic Regression"},{"location":"ML/6/","text":"Deep Learning Three Steps Step 1: Function Set Lecture 5 \u63d0\u5230\u7684\uff0c\u6211\u5011\u53ef\u4ee5\u5c07\u591a\u500b logistic regression models \u9023\u63a5\u8d77\u4f86\uff0c\u800c\u6bcf\u4e00\u500b model \u5c31\u50cf\u662f\u4e00\u500b neuron\uff0c\u7d66\u5b9a\u4e86\u7db2\u8def\u67b6\u69cb\uff0c\u5c31\u5b9a\u7fa9\u4e86\u4e00\u500b function set\u3002 \u7d66\u5b9a\u4e86\u67b6\u69cb\uff0c\u518d\u7d66\u4e0a\u6bcf\u500b neuron \u7684\u53c3\u6578\uff08$w^l, b^l$\uff09\uff0c\u6574\u500b\u7db2\u8def\u5c31\u50cf\u662f\u4e00\u500b function\u3002 \u6bcf\u500b $W^l$ \u90fd\u662f\u4e00\u500b\u77e9\u9663\uff0c\u90fd\u662f\u6211\u5011\u53ef\u4ee5\u7528 GPU \u7684\u5e73\u884c\u904b\u7b97\u53bb\u52a0\u901f\u77e9\u9663\u904b\u7b97\uff1a $$ \\begin{align} y & = f(x) \\\\ & = \\sigma(W^L \\cdots \\sigma(W^2 \\sigma(W^1x + b^1) + b^2) \\cdots + b^L) \\end{align} $$ Output Layer Step 2: Goodness of Function Step 3: Find the best function \u6211\u5011\u4e00\u6a23\u53ef\u4ee5\u7528 Gradient Descent \u4f86\u6c42\u89e3\uff0c\u9019\u7528\u5230 Backpropagation \u7684\u539f\u7406\u3002 \u5176\u4e2d\uff0c $$ \\nabla L = \\left[ \\begin{array} { c } { \\frac { \\partial L } { \\partial w _ { 1 } } } \\\\ { \\frac { \\partial L } { \\partial w _ { 2 } } } \\\\ { \\vdots } \\\\ { \\frac { \\partial L } { \\partial b _ { 1 } } } \\\\ { \\vdots } \\end{array} \\right] $$","title":"Lec 6 - Deep Learning"},{"location":"ML/6/#deep-learning","text":"","title":"Deep Learning"},{"location":"ML/6/#three-steps","text":"","title":"Three Steps"},{"location":"ML/6/#step-1-function-set","text":"Lecture 5 \u63d0\u5230\u7684\uff0c\u6211\u5011\u53ef\u4ee5\u5c07\u591a\u500b logistic regression models \u9023\u63a5\u8d77\u4f86\uff0c\u800c\u6bcf\u4e00\u500b model \u5c31\u50cf\u662f\u4e00\u500b neuron\uff0c\u7d66\u5b9a\u4e86\u7db2\u8def\u67b6\u69cb\uff0c\u5c31\u5b9a\u7fa9\u4e86\u4e00\u500b function set\u3002 \u7d66\u5b9a\u4e86\u67b6\u69cb\uff0c\u518d\u7d66\u4e0a\u6bcf\u500b neuron \u7684\u53c3\u6578\uff08$w^l, b^l$\uff09\uff0c\u6574\u500b\u7db2\u8def\u5c31\u50cf\u662f\u4e00\u500b function\u3002 \u6bcf\u500b $W^l$ \u90fd\u662f\u4e00\u500b\u77e9\u9663\uff0c\u90fd\u662f\u6211\u5011\u53ef\u4ee5\u7528 GPU \u7684\u5e73\u884c\u904b\u7b97\u53bb\u52a0\u901f\u77e9\u9663\u904b\u7b97\uff1a $$ \\begin{align} y & = f(x) \\\\ & = \\sigma(W^L \\cdots \\sigma(W^2 \\sigma(W^1x + b^1) + b^2) \\cdots + b^L) \\end{align} $$","title":"Step 1: Function Set"},{"location":"ML/6/#output-layer","text":"","title":"Output Layer"},{"location":"ML/6/#step-2-goodness-of-function","text":"","title":"Step 2: Goodness of Function"},{"location":"ML/6/#step-3-find-the-best-function","text":"\u6211\u5011\u4e00\u6a23\u53ef\u4ee5\u7528 Gradient Descent \u4f86\u6c42\u89e3\uff0c\u9019\u7528\u5230 Backpropagation \u7684\u539f\u7406\u3002 \u5176\u4e2d\uff0c $$ \\nabla L = \\left[ \\begin{array} { c } { \\frac { \\partial L } { \\partial w _ { 1 } } } \\\\ { \\frac { \\partial L } { \\partial w _ { 2 } } } \\\\ { \\vdots } \\\\ { \\frac { \\partial L } { \\partial b _ { 1 } } } \\\\ { \\vdots } \\end{array} \\right] $$","title":"Step 3: Find the best function"},{"location":"ML/7/","text":"Backpropagation Backpropagation \u662f\u4e00\u7a2e\u80fd\u6709\u6548\u8a08\u7b97\u68af\u5ea6\u7684\u65b9\u6cd5\u3002 \u6211\u5011\u7684\u7db2\u8def\uff1a $$x^n \\to \\text{NN}(\\theta) \\to y^n \\leftrightarrow_{C^n} \\hat y^n$$ Gradient Descent Review \u5047\u8a2d\u6211\u5011\u6709\u4ee5\u4e0b\u53c3\u6578\uff1a$\\theta = \\{w_1, w_2, \\dots, b_1, b_2, \\dots\\}$ \u6211\u5011\u6703\u9010\u6b21\u66f4\u65b0\u53c3\u6578\u5982\u4e0b\uff1a$\\theta^0 \\to \\theta^1 \\to \\theta^2 \\to \\cdots$ \u4f46\u56e0\u70ba\u6709\u592a\u591a\u53c3\u6578\uff0c\u6240\u4ee5\u6211\u5011\u5fc5\u9700\u300c\u6709\u6548\u5730\u300d\u66f4\u65b0\u53c3\u6578\u3002 \u7d66\u5b9a loss function\uff1a \\begin{align} L ( \\theta ) & = \\sum _ { n = 1 } ^ { N } C ^ { n } ( \\theta ) \\\\ \\to \\frac { \\partial L ( \\theta ) } { \\partial w } & = \\sum _ { n = 1 } ^ { N } \\frac { \\partial C ^ { n } ( \\theta ) } { \\partial w } \\end{align} Backpropagation \u5047\u8a2d\u53ea\u8003\u616e\u5176\u4e2d\u4e00\u500b neuron\uff0c\u5247 $z = x_1w_1 + x_2w_2 + b$\uff0c\u6211\u5011\u6240\u8981\u6c42\u7684\u662f\uff1a $$\\frac { \\partial C } { \\partial w } = \\frac { \\partial z } { \\partial w } \\frac { \\partial C } { \\partial z }$$ \u900f\u904e forward pass \u548c backward pass \u53ef\u5206\u5225\u7b97\u51fa\uff1a Forward pass: Compute $\\partial z / \\partial w$ for all parameters Backward pass: Compute $\\partial C / \\partial z$ for all activation function inputs $z$ Forward pass $\\partial z / \\partial w_1 = x_1$ $\\partial z / \\partial w_2 = x_2$ \u5176\u5be6\u5c31\u662f input value \u524d\u9762\u6240\u63a5\u7684\u503c\u3002 \u4f60\u8981\u7b97\u51fa\u9019\u500b neural network \u88e1\u9762\u7684\u6bcf\u4e00\u500b weight \u5c0d\u5b83\u7684 activation function \u7684 input $z$ \u7684\u504f\u5fae\u5206\uff0c\u4f60\u5c31\u628a\u4f60\u7684 input \u4e1f\u9032\u53bb\u3002\u7136\u5f8c\uff0c\u8a08\u7b97\u6bcf\u4e00\u500b neuron \u7684 output \u5c31\u7d50\u675f\u4e86\u3002 Backward pass \u8981\u8a08\u7b97\u51fa $\\frac { \\partial C } { \\partial z }$ \u662f\u975e\u5e38\u8907\u96dc\u7684\uff0c\u4f46\u6211\u5011\u53ef\u4ee5\u900f\u904e chain rule \u505a\u4e9b\u62c6\u89e3\uff1a \\begin{align} \\frac { \\partial C } { \\partial z } & = \\frac { \\partial a } { \\partial z } \\frac { \\partial C } { \\partial a } \\\\ & = \\sigma ^ { \\prime } ( z ) \\left[ \\frac { \\partial z ^ { \\prime } } { \\partial a } \\frac { \\partial C } { \\partial z ^ { \\prime } } + \\frac { \\partial z ^ { \\prime \\prime } } { \\partial a } \\frac { \\partial C } { \\partial z ^ { \\prime \\prime } } \\right] \\\\ & = \\sigma ^ { \\prime } ( z ) \\left[ w _ { 3 } \\frac { \\partial C } { \\partial z ^ { \\prime } } + w _ { 4 } \\frac { \\partial C } { \\partial z ^ { \\prime \\prime } } \\right] \\end{align} \u60f3\u50cf\u6709\u53e6\u4e00\u500b network\uff0c\u65b9\u5411\u662f\u53cd\u904e\u4f86\u7684\uff0c\u4e0b\u9762\u9019\u500b\u7db2\u8def\u5373\u548c\u4e0a\u5f0f\u6240\u4ee3\u8868\u7684\u610f\u7fa9\u4e00\u6a23\uff1a Case 1. Output Layer \\begin{align} \\frac { \\partial C } { \\partial z ^ { \\prime } } & = \\frac { \\partial y _ { 1 } } { \\partial z ^ { \\prime } } \\frac { \\partial C } { \\partial y _ { 1 } } \\\\ \\frac { \\partial C } { \\partial z ^ { \\prime \\prime } } & = \\frac { \\partial y _ { 2 } } { \\partial z ^ { \\prime \\prime } } \\frac { \\partial C } { \\partial y _ { 2 } } \\end{align} Case 2. Not Output Layer Compute $\\partial C / \\partial z$ recursively until we reach the output layer. Summary","title":"Lec 7 - Backpropagation"},{"location":"ML/7/#backpropagation","text":"Backpropagation \u662f\u4e00\u7a2e\u80fd\u6709\u6548\u8a08\u7b97\u68af\u5ea6\u7684\u65b9\u6cd5\u3002 \u6211\u5011\u7684\u7db2\u8def\uff1a $$x^n \\to \\text{NN}(\\theta) \\to y^n \\leftrightarrow_{C^n} \\hat y^n$$","title":"Backpropagation"},{"location":"ML/7/#gradient-descent-review","text":"\u5047\u8a2d\u6211\u5011\u6709\u4ee5\u4e0b\u53c3\u6578\uff1a$\\theta = \\{w_1, w_2, \\dots, b_1, b_2, \\dots\\}$ \u6211\u5011\u6703\u9010\u6b21\u66f4\u65b0\u53c3\u6578\u5982\u4e0b\uff1a$\\theta^0 \\to \\theta^1 \\to \\theta^2 \\to \\cdots$ \u4f46\u56e0\u70ba\u6709\u592a\u591a\u53c3\u6578\uff0c\u6240\u4ee5\u6211\u5011\u5fc5\u9700\u300c\u6709\u6548\u5730\u300d\u66f4\u65b0\u53c3\u6578\u3002 \u7d66\u5b9a loss function\uff1a \\begin{align} L ( \\theta ) & = \\sum _ { n = 1 } ^ { N } C ^ { n } ( \\theta ) \\\\ \\to \\frac { \\partial L ( \\theta ) } { \\partial w } & = \\sum _ { n = 1 } ^ { N } \\frac { \\partial C ^ { n } ( \\theta ) } { \\partial w } \\end{align}","title":"Gradient Descent Review"},{"location":"ML/7/#backpropagation_1","text":"\u5047\u8a2d\u53ea\u8003\u616e\u5176\u4e2d\u4e00\u500b neuron\uff0c\u5247 $z = x_1w_1 + x_2w_2 + b$\uff0c\u6211\u5011\u6240\u8981\u6c42\u7684\u662f\uff1a $$\\frac { \\partial C } { \\partial w } = \\frac { \\partial z } { \\partial w } \\frac { \\partial C } { \\partial z }$$ \u900f\u904e forward pass \u548c backward pass \u53ef\u5206\u5225\u7b97\u51fa\uff1a Forward pass: Compute $\\partial z / \\partial w$ for all parameters Backward pass: Compute $\\partial C / \\partial z$ for all activation function inputs $z$","title":"Backpropagation"},{"location":"ML/7/#forward-pass","text":"$\\partial z / \\partial w_1 = x_1$ $\\partial z / \\partial w_2 = x_2$ \u5176\u5be6\u5c31\u662f input value \u524d\u9762\u6240\u63a5\u7684\u503c\u3002 \u4f60\u8981\u7b97\u51fa\u9019\u500b neural network \u88e1\u9762\u7684\u6bcf\u4e00\u500b weight \u5c0d\u5b83\u7684 activation function \u7684 input $z$ \u7684\u504f\u5fae\u5206\uff0c\u4f60\u5c31\u628a\u4f60\u7684 input \u4e1f\u9032\u53bb\u3002\u7136\u5f8c\uff0c\u8a08\u7b97\u6bcf\u4e00\u500b neuron \u7684 output \u5c31\u7d50\u675f\u4e86\u3002","title":"Forward pass"},{"location":"ML/7/#backward-pass","text":"\u8981\u8a08\u7b97\u51fa $\\frac { \\partial C } { \\partial z }$ \u662f\u975e\u5e38\u8907\u96dc\u7684\uff0c\u4f46\u6211\u5011\u53ef\u4ee5\u900f\u904e chain rule \u505a\u4e9b\u62c6\u89e3\uff1a \\begin{align} \\frac { \\partial C } { \\partial z } & = \\frac { \\partial a } { \\partial z } \\frac { \\partial C } { \\partial a } \\\\ & = \\sigma ^ { \\prime } ( z ) \\left[ \\frac { \\partial z ^ { \\prime } } { \\partial a } \\frac { \\partial C } { \\partial z ^ { \\prime } } + \\frac { \\partial z ^ { \\prime \\prime } } { \\partial a } \\frac { \\partial C } { \\partial z ^ { \\prime \\prime } } \\right] \\\\ & = \\sigma ^ { \\prime } ( z ) \\left[ w _ { 3 } \\frac { \\partial C } { \\partial z ^ { \\prime } } + w _ { 4 } \\frac { \\partial C } { \\partial z ^ { \\prime \\prime } } \\right] \\end{align} \u60f3\u50cf\u6709\u53e6\u4e00\u500b network\uff0c\u65b9\u5411\u662f\u53cd\u904e\u4f86\u7684\uff0c\u4e0b\u9762\u9019\u500b\u7db2\u8def\u5373\u548c\u4e0a\u5f0f\u6240\u4ee3\u8868\u7684\u610f\u7fa9\u4e00\u6a23\uff1a","title":"Backward pass"},{"location":"ML/7/#case-1-output-layer","text":"\\begin{align} \\frac { \\partial C } { \\partial z ^ { \\prime } } & = \\frac { \\partial y _ { 1 } } { \\partial z ^ { \\prime } } \\frac { \\partial C } { \\partial y _ { 1 } } \\\\ \\frac { \\partial C } { \\partial z ^ { \\prime \\prime } } & = \\frac { \\partial y _ { 2 } } { \\partial z ^ { \\prime \\prime } } \\frac { \\partial C } { \\partial y _ { 2 } } \\end{align}","title":"Case 1. Output Layer"},{"location":"ML/7/#case-2-not-output-layer","text":"Compute $\\partial C / \\partial z$ recursively until we reach the output layer.","title":"Case 2. Not Output Layer"},{"location":"ML/7/#summary","text":"","title":"Summary"},{"location":"ML/8/","text":"\"Hello world\" of deep learning Three Steps Step 1: define a set of function Step 2: goodness of function Step 3: pick the best function \u8001\u5e2b\u8ab2\u5802\u4e0a\u7684 task \u662f\u4f7f\u7528 Keras model = Sequential () # declare a model model . add ( Dense ( input_dim = 28 * 28 , output_dim = 500 )) # Dense: Fully connected model . add ( Activation ( 'sigmoid' )) model . add ( Dense ( units = 500 , activation = 'relu' )) model . add ( Dense ( units = 10 , activation = 'softmax' )) # output between 0 and 1 model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) model . fit ( s_train , y_train , batch_size = 100 , epochs = 20 ) # pick the best function Mini-batch Pseudo code: Randomly initialize network parameters while (!all mini-batches have been picked) pick the $i^\\text{th}$ batch $L' = C^1 + C^{31} + \\cdots$ update parameters once \u6bcf\u8dd1\u4e00\u6b21 while-loop\uff0c\u5373\u662f\u4e00\u500b epoch\u3002 \u5982\u679c batch_size = 1\uff1aSGP \u5982\u679c batch_size = #training data\uff1a\uff08full batch\uff09Gradient Descent Matrix Operation Stochastic Gradient Descent Mini-batch GPU \u7684\u52a0\u901f\uff0c\u5c31\u662f\u56e0\u70ba\u53d6 batch \u5f8c\uff0c\u80fd\u5920\u5e73\u884c\u904b\u7b97\uff0c\u6240\u4ee5\u82e5\u6c92\u6709 batch_size\uff0c\u662f\u7121\u6cd5\u7528 GPU \u52a0\u901f\u7684\u3002","title":"Lec 8 - \"Hello world\" of Deep Learning"},{"location":"ML/8/#hello-world-of-deep-learning","text":"","title":"\"Hello world\" of deep learning"},{"location":"ML/8/#three-steps","text":"Step 1: define a set of function Step 2: goodness of function Step 3: pick the best function \u8001\u5e2b\u8ab2\u5802\u4e0a\u7684 task \u662f\u4f7f\u7528 Keras model = Sequential () # declare a model model . add ( Dense ( input_dim = 28 * 28 , output_dim = 500 )) # Dense: Fully connected model . add ( Activation ( 'sigmoid' )) model . add ( Dense ( units = 500 , activation = 'relu' )) model . add ( Dense ( units = 10 , activation = 'softmax' )) # output between 0 and 1 model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) model . fit ( s_train , y_train , batch_size = 100 , epochs = 20 ) # pick the best function","title":"Three Steps"},{"location":"ML/8/#mini-batch","text":"Pseudo code: Randomly initialize network parameters while (!all mini-batches have been picked) pick the $i^\\text{th}$ batch $L' = C^1 + C^{31} + \\cdots$ update parameters once \u6bcf\u8dd1\u4e00\u6b21 while-loop\uff0c\u5373\u662f\u4e00\u500b epoch\u3002 \u5982\u679c batch_size = 1\uff1aSGP \u5982\u679c batch_size = #training data\uff1a\uff08full batch\uff09Gradient Descent","title":"Mini-batch"},{"location":"ML/8/#matrix-operation","text":"","title":"Matrix Operation"},{"location":"ML/8/#stochastic-gradient-descent","text":"","title":"Stochastic Gradient Descent"},{"location":"ML/8/#mini-batch_1","text":"GPU \u7684\u52a0\u901f\uff0c\u5c31\u662f\u56e0\u70ba\u53d6 batch \u5f8c\uff0c\u80fd\u5920\u5e73\u884c\u904b\u7b97\uff0c\u6240\u4ee5\u82e5\u6c92\u6709 batch_size\uff0c\u662f\u7121\u6cd5\u7528 GPU \u52a0\u901f\u7684\u3002","title":"Mini-batch"},{"location":"ML/9/","text":"Tips for Deep Learning Recipe of Deep Learning Vanishing Gradient Problem \u4ee5\u4e0a\u5716\u7684\u4f8b\u5b50\u4f86\u770b\uff1a\u7531\u65bc\u901a\u904e\u795e\u7d93\u5143\u7684 sigmoid function\uff0c\u6578\u503c\u5927\u7684 input \u6703\u88ab\u58d3\u7e2e\u5230 $0$ \u5230 $1$ \u4e4b\u9593\uff0c\u4ee5\u81f3\u65bc\u5f7c\u6b64\u660e\u660e\u5dee\u7570\u5f88\u5927\u7684 input\uff0c\u5728 output \u6642\u7684\u5dee\u7570\u537b\u6c92\u50cf\u672c\u4f86\u90a3\u9ebc\u660e\u986f\u3002 ReLU (Rectified Linear Unit) pros: fast to compute biological reason infinite sigmoid with different biases vanishing gradient problem variant Leack ReLU Parametric ReLU $\\alpha$ \u6703\u7531 gradient descent \u4e2d\u5b78\u7fd2\u3002 Maxout ReLU is a special cases of Maxout Learnable activation function Activation function in maxout network can be any piecewise linear convex function How many pieces depending on how many elements in a group Adaptive Learning Rate RMSProp \\begin{array} { c l } { w ^ { 1 } \\leftarrow w ^ { 0 } - \\frac { \\eta } { \\sigma ^ { 0 } } g ^ { 0 } } & { \\sigma ^ { 0 } = g ^ { 0 } } \\\\ { w ^ { 2 } \\leftarrow w ^ { 1 } - \\frac { \\eta } { \\sigma ^ { 1 } } g ^ { 1 } } & { \\sigma ^ { 1 } = \\sqrt { \\alpha \\left( \\sigma ^ { 0 } \\right) ^ { 2 } + ( 1 - \\alpha ) \\left( g ^ { 2 } \\right) ^ { 2 } } } \\\\ { w ^ { 3 } \\leftarrow w ^ { 2 } - \\frac { \\eta } { \\sigma ^ { 2 } } g ^ { 2 } } & { \\sigma ^ { 2 } = \\sqrt { \\alpha \\left( \\sigma ^ { 1 } \\right) ^ { 2 } + ( 1 - \\alpha ) \\left( g ^ { 2 } \\right) ^ { 2 } } } \\\\ { \\vdots } & { \\vdots } \\\\ { w ^ { t + 1 } \\leftarrow w ^ { t } - \\frac { \\eta } { \\sigma ^ { t } } g ^ { t } } & { \\sigma ^ { t } = \\sqrt { \\alpha \\left( \\sigma ^ { t - 1 } \\right) ^ { 2 } + ( 1 - \\alpha ) \\left( g ^ { t } \\right) ^ { 2 } } } \\end{array} Root Mean Square of the gradients with previous gradients being decayed. Momentum Adam Adam = RMSProp + Momentum Early Stopping Regularization Dropout","title":"Lec 9 - Tips for Deep Learning"},{"location":"ML/9/#tips-for-deep-learning","text":"","title":"Tips for Deep Learning"},{"location":"ML/9/#recipe-of-deep-learning","text":"","title":"Recipe of Deep Learning"},{"location":"ML/9/#vanishing-gradient-problem","text":"\u4ee5\u4e0a\u5716\u7684\u4f8b\u5b50\u4f86\u770b\uff1a\u7531\u65bc\u901a\u904e\u795e\u7d93\u5143\u7684 sigmoid function\uff0c\u6578\u503c\u5927\u7684 input \u6703\u88ab\u58d3\u7e2e\u5230 $0$ \u5230 $1$ \u4e4b\u9593\uff0c\u4ee5\u81f3\u65bc\u5f7c\u6b64\u660e\u660e\u5dee\u7570\u5f88\u5927\u7684 input\uff0c\u5728 output \u6642\u7684\u5dee\u7570\u537b\u6c92\u50cf\u672c\u4f86\u90a3\u9ebc\u660e\u986f\u3002","title":"Vanishing Gradient Problem"},{"location":"ML/9/#relu-rectified-linear-unit","text":"pros: fast to compute biological reason infinite sigmoid with different biases vanishing gradient problem","title":"ReLU (Rectified Linear Unit)"},{"location":"ML/9/#variant","text":"Leack ReLU Parametric ReLU $\\alpha$ \u6703\u7531 gradient descent \u4e2d\u5b78\u7fd2\u3002","title":"variant"},{"location":"ML/9/#maxout","text":"ReLU is a special cases of Maxout Learnable activation function Activation function in maxout network can be any piecewise linear convex function How many pieces depending on how many elements in a group","title":"Maxout"},{"location":"ML/9/#adaptive-learning-rate","text":"","title":"Adaptive Learning Rate"},{"location":"ML/9/#rmsprop","text":"\\begin{array} { c l } { w ^ { 1 } \\leftarrow w ^ { 0 } - \\frac { \\eta } { \\sigma ^ { 0 } } g ^ { 0 } } & { \\sigma ^ { 0 } = g ^ { 0 } } \\\\ { w ^ { 2 } \\leftarrow w ^ { 1 } - \\frac { \\eta } { \\sigma ^ { 1 } } g ^ { 1 } } & { \\sigma ^ { 1 } = \\sqrt { \\alpha \\left( \\sigma ^ { 0 } \\right) ^ { 2 } + ( 1 - \\alpha ) \\left( g ^ { 2 } \\right) ^ { 2 } } } \\\\ { w ^ { 3 } \\leftarrow w ^ { 2 } - \\frac { \\eta } { \\sigma ^ { 2 } } g ^ { 2 } } & { \\sigma ^ { 2 } = \\sqrt { \\alpha \\left( \\sigma ^ { 1 } \\right) ^ { 2 } + ( 1 - \\alpha ) \\left( g ^ { 2 } \\right) ^ { 2 } } } \\\\ { \\vdots } & { \\vdots } \\\\ { w ^ { t + 1 } \\leftarrow w ^ { t } - \\frac { \\eta } { \\sigma ^ { t } } g ^ { t } } & { \\sigma ^ { t } = \\sqrt { \\alpha \\left( \\sigma ^ { t - 1 } \\right) ^ { 2 } + ( 1 - \\alpha ) \\left( g ^ { t } \\right) ^ { 2 } } } \\end{array} Root Mean Square of the gradients with previous gradients being decayed.","title":"RMSProp"},{"location":"ML/9/#momentum","text":"","title":"Momentum"},{"location":"ML/9/#adam","text":"Adam = RMSProp + Momentum","title":"Adam"},{"location":"ML/9/#early-stopping","text":"","title":"Early Stopping"},{"location":"ML/9/#regularization","text":"","title":"Regularization"},{"location":"ML/9/#dropout","text":"","title":"Dropout"},{"location":"OS/","text":"Operating System | notes In this page, I'll work on my notes when I have class of Operating System, Spring 2018 by Professor Tei-Wei Kuo . Professor Kuo has also upload the course videos to YouTube . The materials are mainly from the Operating System Concepts, 9th Edition .","title":"Preface"},{"location":"OS/#operating-system-notes","text":"In this page, I'll work on my notes when I have class of Operating System, Spring 2018 by Professor Tei-Wei Kuo . Professor Kuo has also upload the course videos to YouTube . The materials are mainly from the Operating System Concepts, 9th Edition .","title":"Operating System | notes"},{"location":"OS/Chap01/","text":"Chapter 1 Introduction","title":"Chapter 1 Introduction"},{"location":"OS/Chap01/#chapter-1-introduction","text":"","title":"Chapter 1 Introduction"},{"location":"OS/Chap02/","text":"Chapter 2 Operating-System Structures Objectives: To describe the services an operating system provides to users, processes, and other systems. To discuss the various ways of structuring an operating system. To explain how operating systems are installed and customized and how they boot. 2.1 Operating-System Services User interface (UI) command-line interface (CLI) batch interface graphical user interface Program execution. OS load a program into memory $\\to$ run that program $\\to$ end execution normally abnormally (error) I/O operations. A running program may require I/O: file I/O device: recording to a CD or DVD ... File-system manipulation. read/write files create/delete them by name search list file (ls) Communications. shared memory message passing: packets of information in predefined formats are moved between processes by the operating system Error detection Resource allocation Accounting. users can be billed Protection and security 2.2 User and Operating-System Interface 2.2.1 Command Interpreters On systems with multiple command interpreters to choose from, the interpreters are known as shells . Two approaches: the command interpreter itself contains the code to execute the command. fast but the interpreter tends to be big! $\\to$ painful in revision! e.g. cd , ls , del the command interpreter merely uses the command to identify a file to be loaded into memory and executed $\\to$ search exec files parameter passing being slow inconsistent interpretation of parameters e.g. rm 2.2.2 Graphical User Interfaces desktop icons folder mouse gestures on the touchscreen 2.2.3 Choice of Interface shell scripts e.g. UNIX and Linux . 2.3 Systems Calls System calls provide an interface to the services made available by an operating system. e.g. writing a simple program to read data from one file and copy them to another file causes a lot of system calls! C/C++ Each read and write must return status information regarding various possible error conditions. application programming interface (API): it specifies a set of functions Windows API POSIX API UNIX Linux macOS Java API libc : UNIX and Linux for programs written in C Why prefer API rather than invoking actual system calls? protability (expected to run on any system) actual system calls can be more difficult to learn The relationship between an API , the system-call interface , and the OS \b The caller need know nothing about how the system call is implemented or what it does during execution. Rather, the caller need only obey the API and understand what the operating system will do as a result of the execution of that system call. Make explicit to implicit . Three general methods are used to pass parameters to the operating system. through registers (Linux and Solaris) block, table, memory, and the address of the placed or pushed onto the stack $\\to$ popped off the stack by the OS 2.4 Types of System Calls 2.4.1 Process Control A running program halts either normally: end() abnormally: abort() error $\\to$ dump (written to disk, may be examined by a debugger) More severe errors can be indicated by a higher-level error parameter. e.g. Standard C Library 2.4.2 File Management 2.4.3 Device Management 2.4.4 Information Maintenance Many systems provide system calls to dump() memory. This provision is useful for debugging. A program trace lists each system call as it is executed. Even microprocessors provide a CPU mode known as single step, in which a trap is executed by the CPU after every instruction. The trap is usually caught by a debugger. 2.4.5 Communication 2.4.6 Protection 2.5 System Programs 2.6 Operating-System Design and Implementation 2.6.1 Design Goals 2.6.2 Mechanisms and Policies 2.6.3 Implementation","title":"Chapter 2 Operating-System Structures"},{"location":"OS/Chap02/#chapter-2-operating-system-structures","text":"Objectives: To describe the services an operating system provides to users, processes, and other systems. To discuss the various ways of structuring an operating system. To explain how operating systems are installed and customized and how they boot.","title":"Chapter 2 Operating-System Structures"},{"location":"OS/Chap02/#21-operating-system-services","text":"User interface (UI) command-line interface (CLI) batch interface graphical user interface Program execution. OS load a program into memory $\\to$ run that program $\\to$ end execution normally abnormally (error) I/O operations. A running program may require I/O: file I/O device: recording to a CD or DVD ... File-system manipulation. read/write files create/delete them by name search list file (ls) Communications. shared memory message passing: packets of information in predefined formats are moved between processes by the operating system Error detection Resource allocation Accounting. users can be billed Protection and security","title":"2.1 Operating-System Services"},{"location":"OS/Chap02/#22-user-and-operating-system-interface","text":"","title":"2.2 User and Operating-System Interface"},{"location":"OS/Chap02/#221-command-interpreters","text":"On systems with multiple command interpreters to choose from, the interpreters are known as shells . Two approaches: the command interpreter itself contains the code to execute the command. fast but the interpreter tends to be big! $\\to$ painful in revision! e.g. cd , ls , del the command interpreter merely uses the command to identify a file to be loaded into memory and executed $\\to$ search exec files parameter passing being slow inconsistent interpretation of parameters e.g. rm","title":"2.2.1 Command Interpreters"},{"location":"OS/Chap02/#222-graphical-user-interfaces","text":"desktop icons folder mouse gestures on the touchscreen","title":"2.2.2 Graphical User Interfaces"},{"location":"OS/Chap02/#223-choice-of-interface","text":"shell scripts e.g. UNIX and Linux .","title":"2.2.3 Choice of Interface"},{"location":"OS/Chap02/#23-systems-calls","text":"System calls provide an interface to the services made available by an operating system. e.g. writing a simple program to read data from one file and copy them to another file causes a lot of system calls! C/C++ Each read and write must return status information regarding various possible error conditions. application programming interface (API): it specifies a set of functions Windows API POSIX API UNIX Linux macOS Java API libc : UNIX and Linux for programs written in C Why prefer API rather than invoking actual system calls? protability (expected to run on any system) actual system calls can be more difficult to learn The relationship between an API , the system-call interface , and the OS \b The caller need know nothing about how the system call is implemented or what it does during execution. Rather, the caller need only obey the API and understand what the operating system will do as a result of the execution of that system call. Make explicit to implicit . Three general methods are used to pass parameters to the operating system. through registers (Linux and Solaris) block, table, memory, and the address of the placed or pushed onto the stack $\\to$ popped off the stack by the OS","title":"2.3 Systems Calls"},{"location":"OS/Chap02/#24-types-of-system-calls","text":"","title":"2.4 Types of System Calls"},{"location":"OS/Chap02/#241-process-control","text":"A running program halts either normally: end() abnormally: abort() error $\\to$ dump (written to disk, may be examined by a debugger) More severe errors can be indicated by a higher-level error parameter. e.g. Standard C Library","title":"2.4.1 Process Control"},{"location":"OS/Chap02/#242-file-management","text":"","title":"2.4.2 File Management"},{"location":"OS/Chap02/#243-device-management","text":"","title":"2.4.3 Device Management"},{"location":"OS/Chap02/#244-information-maintenance","text":"Many systems provide system calls to dump() memory. This provision is useful for debugging. A program trace lists each system call as it is executed. Even microprocessors provide a CPU mode known as single step, in which a trap is executed by the CPU after every instruction. The trap is usually caught by a debugger.","title":"2.4.4 Information Maintenance"},{"location":"OS/Chap02/#245-communication","text":"","title":"2.4.5 Communication"},{"location":"OS/Chap02/#246-protection","text":"","title":"2.4.6 Protection"},{"location":"OS/Chap02/#25-system-programs","text":"","title":"2.5 System Programs"},{"location":"OS/Chap02/#26-operating-system-design-and-implementation","text":"","title":"2.6 Operating-System Design and Implementation"},{"location":"OS/Chap02/#261-design-goals","text":"","title":"2.6.1 Design Goals"},{"location":"OS/Chap02/#262-mechanisms-and-policies","text":"","title":"2.6.2 Mechanisms and Policies"},{"location":"OS/Chap02/#263-implementation","text":"","title":"2.6.3 Implementation"},{"location":"OS/Chap03/","text":"Chapter 3 Process Concept 3.1 Process Concept Process A program in execution, the basis of all computation. Batch system: jobs (= process) Time-shared system: user programs or tasks 3.1.1 The process Process consists text section: program code data section: global variables heap : memory current activity ( program counter + registers ) \bstack : temporary data : function parameters return addresses local variables Program Process passive entity active entity a file containing a list of instructions stored on disk (executable file) program counter: specifying the next instruction to execute + a set of associated resources When double-clicking an icon prog.exe a.out an executable file is loaded into memory: program $\\to$ process . Two different processes: the text section are equivalent, the data, heap and stack vary. Process can be an execution environment for other code. ( simulation ) e.g. java testProgram The command java runs the JVM as an ordinary process, then executes the Java program testProgram in the VM. 3.1.2 Process State New Running : execute instructions Waiting : wait some event (I/O, signal) Ready : wait to be assigned to a processor Terminated Process and Processor Only 1 process runs on any processor. (Many processes may be ready and waiting ) 3.1.3 Process Control Block Process state Program counter : address of the next instruction CPU registers : accumulators, index registers, stack pointers, general-purpose registers, and any condition-code information CPU-scheduling information Memory-management information Accounting information : the amount of CPU and real time used, time limits, account numbers, job or process numbers I/O status information : the list of I/O devices allocated to the process, a list of open files 3.2 Process Scheduling Multiprogramming : to have some process running at all times $\\to$ maximize CPU utilization Time sharing : switch the CPU among processes Process scheduler : selects an available process 3.2.1 Scheduling Queues As processes enter the system, they are put into a job queue . Job queue : consists of all processes in the system. Ready queue : keep ready and waiting processes. When a process exit, it is removed from all queues and has its PCB and resources deallocated. 3.2.2 Schedulers Processes are first spooled to a mass-storage device (e.g. disk). Then Long-term scheduler (job) selects processes from this pool loads them into memory for execution Short-term scheduler (CPU) selects from among the processes that are ready to execute allocates CPU to one of them Long-term scheduler Controls the degree of multiprogramming (# processes) Selects a good process mix of I/O-bound and CPU-bound Medium-term scheduler Swapping 3.2.3 Context Switch When a context switch occurs The kernel saves the context of the old process in its PCB and loads the saved context of the new process scheduled to run. 3.3 Operations on Processes 3.3.1 Process Creation Process Identifier (pid) An integer number, which provides a unique value for each process in the system, and it can be used as an index to access various attributes of a process within the kernel. init process A process has pid = 1, and serves as the root parent process for all user processes. When a process creates a child process, that child process may obtain the resources from OS a subset of parent process When a process creates a new process The parent continues to execute concurrently with its children. The parent waits until some or all of its children have terminated. There are also two address-space possibilities for the new process The child process is a duplicate of the parent process (it has the same program and data as the parent). The child process has a new program loaded into it. fork() The new process created by fork() consists of a copy of the address of parent process. Return code Child process: 0 Parent process: pid of the child After fork() syscall One of the two processes uses the exec() syscall to replace the process's memory space with a new program. Creating a separate process using the UNIX fork() system call. int main () { pid_t pid ; /* fork a child process */ pid = fork (); if ( pid < 0 ) { /* error occurred */ fprintf ( stderr , \"Fork Failed\" ); return 1 ; } else if ( pid == 0 ) { /* child process */ execlp ( \"/bin/ls\" , \"ls\" , NULL ); /* a version of the `exec()` */ } else { /* parent process */ /* parent will wait for the child to complete */ wait ( NULL ); printf ( \"Child Complete\" ); } return 0 ; } 3.3.2 Process Termination A process terminates when it finishes executing its final statement and asks the operating system to delete it by using the exit() system call. Terminating process A parent needs to know the identities of its children if it is to terminate them. A parent can terminate its children by The child use too much resources. (The parent have a mechanism to inspect the state of its children) The task assigned to the child is no longer required. The parent is exiting. Cascading Termination If a process terminates (either normally or abnormally), then all its children must also be terminated. exit() may be called either directly or indirectly ( return ): exit ( 1 ); /* directly exit with status 1 */ Process Table Entry (PTE) Contains the process's exit status. Zombie A process terminated, but whose parent hasn't called wait() . Once the parent calls wait() , the pid of the zombie process and its entry in the PTE are released. The init process periodically invokes wait() to collect and release the orphan's pid and PTE. 3.4 Interprocess Communication Processes have two classifications: Independent Cooperating Information sharing Computation speedup - multicore Modularity Convenience - parallel tasks Interprocess communication (IPC) Shared memory: slower (syscalls are required) Message passing: faster (syscalls are required only to establish shared memory regions) 3.4.1 Shared-Memory Systems Producer\u2013consumer problem A producer process produces information that is consumed by a consumer process. e.g. A compiler produce assembly code that is consumed by an assembler. The assembler, in turn, may produce object modules that are consumed by the loader. A server as a producer and a client as a consumer. We need a buffer which resides in a region of shared memory (producer & consumer), and can be filled by the producer and emptied by the consumer. Unbounded buffer Bounded buffer (more practical) Implement the shared buffer as a circular array. #define BUFFER_SIZE 10 typedef struct { ... } item ; item buffer [ BUFFER_SIZE ]; int in = 0 ; /* points to the next free position */ int out = 0 ; /* points to the first full position */ while ( true ) { /* produce an item in next_produced */ while ((( in + 1 ) % BUFFER_SIZE ) == out ) ; /* do nothing */ buffer [ in ] = next_produced ; in = ( in + 1 ) % BUFFER_SIZE ; } item next_consumed ; while ( true ) { while ( in == out ) ; /* do nothing */ next_consumed = buffer [ out ]; out = ( out + 1 ) % BUFFER_SIZE ; /* consume the item in next_consumed */ } 3.4.2 Message-Passing Systems Message passing provides a mechanism to allow processes to communicate and to synchronize their actions without sharing the same address space . Communication link If processes $P$ and $Q$ want to communicate, they must send messages to and receive messages from each other. Several implentation of send() / receive() operations: Direct of indirect communication Synchronous or asynchronous communication Automatic or explicit buffering 3.4.2.1 Naming Direct communication The messages are sent to and received from processes. Symmetry send(P, message) receive(Q, message) Asymmetry send(P, message) receive(id, message) Indirect communication The messages are sent to and received from mailboxes , or ports . send(A, message) \u2014 send a message to mailbox A receive(A, message) \u2014 receive a message from mailbox A The process that creates a new mailbox is that mailbox's owner by default. A mailbox can be owned by the OS. 3.4.2.2 Synchronization Message passing may be either Blocking (synchronous) Blocking send (blocked until the message is received) Blocking receive Nonblocking (asynchronous) Nonblocking send Nonblocking receive (valid message or a null) Rendezvous When both send() and receive() are blocking. 3.4.2.3 Buffering Messages reside in a temporary queue: Zero capacity (no buffering) Bounded capacity Unbounded capacity 3.5 Examples of IPC Systems 3.5.1 An Example: POSIX Shared Memory message next_consumed ; while ( true ) { receive ( next_consumed ); /* consume the item in next consumed */ } A process must first create a shared-memory: shm_fd = shm_open ( name , O_CREAT | O_RDRW , 0666 ); The ftruncate() function configure the size of the object in bytes: ftruncate ( shm_fd , 4096 ); 3.5.2 An Example: Mach Even system calls are made by messages. When a task is created, two special mailboxes kernel mailbox notify mailbox are also created. There are three syscalls needed: msg_send() If the mailbox is full: Wait indefinitely until there is room in the mailbox Wait at most $n$ milliseconds Do not wait at all but rather return immediately Temporarily cache a message (server tasks) msg_receive() msg_rpc() : sends a message and waits for exactly one return message from the sender. Remote The RPC (Remote Procedure Call) models a typical subroutine procedure call but can work between systems. port_allocate() Creates a new mailbox and allocates space for its queue of messages. Mach guarantees that multiple messages from the same sender are queued in first-in, first-out (FIFO) order but does not guarantee an absolute ordering One task can either own or receive from a mailbox Mailbox set A collection of mailboxes. port_status() e.g. # of messages in a mailbox. 3.5.3 An Example: Windows Application programs can be considered clients of a subsystem server. Advanced Local Procedure Call (ALPC) It is used for communication between two processes on the same machine . Windows uses two types of ports Connection ports Communication ports Callback Allows the client and server to accept requests when they would normally be expecting a reply. When an ALPC channel is created, 1 of 3 message-passing techniques is chosen: Small messages: using the port's message queue. Larger messages: passed through a section object (a region of shared memory) Very large messages: calling API to read/write directly into the address space. 3.6 Communication in Client\u2013Server Systems 3.6.1 Sockets Socket An endpoint for communication. (IP + port#) A pair of processes communicating over a network employs a pair of sockets\u2014one for each process. Socket behavior The server waits for incoming client requests by listening to a specified port. Once a request is received, the server accepts a connection from the client socket to complete the connection. Well-known ports: (all ports below 1024 are considered well known) 23: telnet 21: FTP 80: HTTP Java provides: Connection-oriented (TCP) sockets: Socket Connectionless (UDP) sockets: DatagramSocket MulticastSocket : a subclass of DatagramSocket . It allows data to be sent to multiple recipients. Loopback IP address 127.0.0.1. 3.6.2 Remote Procedure Calls The RPC was designed as a way to abstract the procedure-call mechanism for use between systems with network connections. Each message is addressed to an RPC daemon listening to a port on the remote system, and each contains an identifier specifying the function to execute and the parameters to pass to that function. The semantics of RPCs allows a client to invoke a procedure on a remote host as it would invoke a procedure locally. Stub The RPC system hides the details that allow communication to take place by providing a stub on the client side. Parameter marshalling Packaging the parameters into a form that can be transmitted over a network. Procedure of RPCs: The client invokes a RPC RPC system calls the appropriate stub (client side) passes the stub the parameters to the RPC Marshals parameter: packaging the parameters into a form that can be transmitted over a network The stub transmits a message to the server using message passing. A stub (server side) receives this message invokes the procedure on the server (optional) Return values using the same technique Issues for RPC: Data representation External Data Representation (XDR) Parameter marshalling Semantics of a call at most once exactly once (ACK) Binding of the client and server port Matchmaker (a rendezvous mechanism) 3.6.3 Pipes In implementing a pipe, four issues: Does the pipe allow bidirectional communication, or is communication unidirectional? If two-way communication is allowed, is it half duplex (data can travel only one way at a time) or full duplex (data can travel in both directions at the same time)? Must a relationship (such as parent\u2013child) exist between the communicating processes? Can the pipes communicate over a network, or must the communicating processes reside on the same machine? 3.6.3.1 Ordinary Pipes pipe ( int fd []) Ordinarya pipes on on Windows: anonymous pipes (similar to UNIX) 3.6.3.2 Named Pipes Ordinary Pipes Named Pipes unidirectional bidirectional parent-child required not required In UNIX, named pipes = FIFOs. A FIFO is created with the mkfifo() . Pipes in practice: # In this scenario, the ls command serves as the producer, and its output is consumed by the more command. $ ls | more","title":"Chapter 3 Processes"},{"location":"OS/Chap03/#chapter-3-process-concept","text":"","title":"Chapter 3 Process Concept"},{"location":"OS/Chap03/#31-process-concept","text":"Process A program in execution, the basis of all computation. Batch system: jobs (= process) Time-shared system: user programs or tasks","title":"3.1 Process Concept"},{"location":"OS/Chap03/#311-the-process","text":"Process consists text section: program code data section: global variables heap : memory current activity ( program counter + registers ) \bstack : temporary data : function parameters return addresses local variables Program Process passive entity active entity a file containing a list of instructions stored on disk (executable file) program counter: specifying the next instruction to execute + a set of associated resources When double-clicking an icon prog.exe a.out an executable file is loaded into memory: program $\\to$ process . Two different processes: the text section are equivalent, the data, heap and stack vary. Process can be an execution environment for other code. ( simulation ) e.g. java testProgram The command java runs the JVM as an ordinary process, then executes the Java program testProgram in the VM.","title":"3.1.1 The process"},{"location":"OS/Chap03/#312-process-state","text":"New Running : execute instructions Waiting : wait some event (I/O, signal) Ready : wait to be assigned to a processor Terminated Process and Processor Only 1 process runs on any processor. (Many processes may be ready and waiting )","title":"3.1.2 Process State"},{"location":"OS/Chap03/#313-process-control-block","text":"Process state Program counter : address of the next instruction CPU registers : accumulators, index registers, stack pointers, general-purpose registers, and any condition-code information CPU-scheduling information Memory-management information Accounting information : the amount of CPU and real time used, time limits, account numbers, job or process numbers I/O status information : the list of I/O devices allocated to the process, a list of open files","title":"3.1.3 Process Control Block"},{"location":"OS/Chap03/#32-process-scheduling","text":"Multiprogramming : to have some process running at all times $\\to$ maximize CPU utilization Time sharing : switch the CPU among processes Process scheduler : selects an available process","title":"3.2 Process Scheduling"},{"location":"OS/Chap03/#321-scheduling-queues","text":"As processes enter the system, they are put into a job queue . Job queue : consists of all processes in the system. Ready queue : keep ready and waiting processes. When a process exit, it is removed from all queues and has its PCB and resources deallocated.","title":"3.2.1 Scheduling Queues"},{"location":"OS/Chap03/#322-schedulers","text":"Processes are first spooled to a mass-storage device (e.g. disk). Then Long-term scheduler (job) selects processes from this pool loads them into memory for execution Short-term scheduler (CPU) selects from among the processes that are ready to execute allocates CPU to one of them Long-term scheduler Controls the degree of multiprogramming (# processes) Selects a good process mix of I/O-bound and CPU-bound Medium-term scheduler Swapping","title":"3.2.2 Schedulers"},{"location":"OS/Chap03/#323-context-switch","text":"When a context switch occurs The kernel saves the context of the old process in its PCB and loads the saved context of the new process scheduled to run.","title":"3.2.3 Context Switch"},{"location":"OS/Chap03/#33-operations-on-processes","text":"","title":"3.3 Operations on Processes"},{"location":"OS/Chap03/#331-process-creation","text":"Process Identifier (pid) An integer number, which provides a unique value for each process in the system, and it can be used as an index to access various attributes of a process within the kernel. init process A process has pid = 1, and serves as the root parent process for all user processes. When a process creates a child process, that child process may obtain the resources from OS a subset of parent process When a process creates a new process The parent continues to execute concurrently with its children. The parent waits until some or all of its children have terminated. There are also two address-space possibilities for the new process The child process is a duplicate of the parent process (it has the same program and data as the parent). The child process has a new program loaded into it. fork() The new process created by fork() consists of a copy of the address of parent process. Return code Child process: 0 Parent process: pid of the child After fork() syscall One of the two processes uses the exec() syscall to replace the process's memory space with a new program. Creating a separate process using the UNIX fork() system call. int main () { pid_t pid ; /* fork a child process */ pid = fork (); if ( pid < 0 ) { /* error occurred */ fprintf ( stderr , \"Fork Failed\" ); return 1 ; } else if ( pid == 0 ) { /* child process */ execlp ( \"/bin/ls\" , \"ls\" , NULL ); /* a version of the `exec()` */ } else { /* parent process */ /* parent will wait for the child to complete */ wait ( NULL ); printf ( \"Child Complete\" ); } return 0 ; }","title":"3.3.1 Process Creation"},{"location":"OS/Chap03/#332-process-termination","text":"A process terminates when it finishes executing its final statement and asks the operating system to delete it by using the exit() system call. Terminating process A parent needs to know the identities of its children if it is to terminate them. A parent can terminate its children by The child use too much resources. (The parent have a mechanism to inspect the state of its children) The task assigned to the child is no longer required. The parent is exiting. Cascading Termination If a process terminates (either normally or abnormally), then all its children must also be terminated. exit() may be called either directly or indirectly ( return ): exit ( 1 ); /* directly exit with status 1 */ Process Table Entry (PTE) Contains the process's exit status. Zombie A process terminated, but whose parent hasn't called wait() . Once the parent calls wait() , the pid of the zombie process and its entry in the PTE are released. The init process periodically invokes wait() to collect and release the orphan's pid and PTE.","title":"3.3.2 Process Termination"},{"location":"OS/Chap03/#34-interprocess-communication","text":"Processes have two classifications: Independent Cooperating Information sharing Computation speedup - multicore Modularity Convenience - parallel tasks Interprocess communication (IPC) Shared memory: slower (syscalls are required) Message passing: faster (syscalls are required only to establish shared memory regions)","title":"3.4 Interprocess Communication"},{"location":"OS/Chap03/#341-shared-memory-systems","text":"","title":"3.4.1 Shared-Memory Systems"},{"location":"OS/Chap03/#producerconsumer-problem","text":"A producer process produces information that is consumed by a consumer process. e.g. A compiler produce assembly code that is consumed by an assembler. The assembler, in turn, may produce object modules that are consumed by the loader. A server as a producer and a client as a consumer. We need a buffer which resides in a region of shared memory (producer & consumer), and can be filled by the producer and emptied by the consumer. Unbounded buffer Bounded buffer (more practical) Implement the shared buffer as a circular array. #define BUFFER_SIZE 10 typedef struct { ... } item ; item buffer [ BUFFER_SIZE ]; int in = 0 ; /* points to the next free position */ int out = 0 ; /* points to the first full position */ while ( true ) { /* produce an item in next_produced */ while ((( in + 1 ) % BUFFER_SIZE ) == out ) ; /* do nothing */ buffer [ in ] = next_produced ; in = ( in + 1 ) % BUFFER_SIZE ; } item next_consumed ; while ( true ) { while ( in == out ) ; /* do nothing */ next_consumed = buffer [ out ]; out = ( out + 1 ) % BUFFER_SIZE ; /* consume the item in next_consumed */ }","title":"Producer\u2013consumer problem"},{"location":"OS/Chap03/#342-message-passing-systems","text":"Message passing provides a mechanism to allow processes to communicate and to synchronize their actions without sharing the same address space . Communication link If processes $P$ and $Q$ want to communicate, they must send messages to and receive messages from each other. Several implentation of send() / receive() operations: Direct of indirect communication Synchronous or asynchronous communication Automatic or explicit buffering","title":"3.4.2 Message-Passing Systems"},{"location":"OS/Chap03/#3421-naming","text":"Direct communication The messages are sent to and received from processes. Symmetry send(P, message) receive(Q, message) Asymmetry send(P, message) receive(id, message) Indirect communication The messages are sent to and received from mailboxes , or ports . send(A, message) \u2014 send a message to mailbox A receive(A, message) \u2014 receive a message from mailbox A The process that creates a new mailbox is that mailbox's owner by default. A mailbox can be owned by the OS.","title":"3.4.2.1 Naming"},{"location":"OS/Chap03/#3422-synchronization","text":"Message passing may be either Blocking (synchronous) Blocking send (blocked until the message is received) Blocking receive Nonblocking (asynchronous) Nonblocking send Nonblocking receive (valid message or a null) Rendezvous When both send() and receive() are blocking.","title":"3.4.2.2 Synchronization"},{"location":"OS/Chap03/#3423-buffering","text":"Messages reside in a temporary queue: Zero capacity (no buffering) Bounded capacity Unbounded capacity","title":"3.4.2.3 Buffering"},{"location":"OS/Chap03/#35-examples-of-ipc-systems","text":"","title":"3.5 Examples of IPC Systems"},{"location":"OS/Chap03/#351-an-example-posix-shared-memory","text":"message next_consumed ; while ( true ) { receive ( next_consumed ); /* consume the item in next consumed */ } A process must first create a shared-memory: shm_fd = shm_open ( name , O_CREAT | O_RDRW , 0666 ); The ftruncate() function configure the size of the object in bytes: ftruncate ( shm_fd , 4096 );","title":"3.5.1 An Example: POSIX Shared Memory"},{"location":"OS/Chap03/#352-an-example-mach","text":"Even system calls are made by messages. When a task is created, two special mailboxes kernel mailbox notify mailbox are also created. There are three syscalls needed: msg_send() If the mailbox is full: Wait indefinitely until there is room in the mailbox Wait at most $n$ milliseconds Do not wait at all but rather return immediately Temporarily cache a message (server tasks) msg_receive() msg_rpc() : sends a message and waits for exactly one return message from the sender. Remote The RPC (Remote Procedure Call) models a typical subroutine procedure call but can work between systems. port_allocate() Creates a new mailbox and allocates space for its queue of messages. Mach guarantees that multiple messages from the same sender are queued in first-in, first-out (FIFO) order but does not guarantee an absolute ordering One task can either own or receive from a mailbox Mailbox set A collection of mailboxes. port_status() e.g. # of messages in a mailbox.","title":"3.5.2 An Example: Mach"},{"location":"OS/Chap03/#353-an-example-windows","text":"Application programs can be considered clients of a subsystem server. Advanced Local Procedure Call (ALPC) It is used for communication between two processes on the same machine . Windows uses two types of ports Connection ports Communication ports Callback Allows the client and server to accept requests when they would normally be expecting a reply. When an ALPC channel is created, 1 of 3 message-passing techniques is chosen: Small messages: using the port's message queue. Larger messages: passed through a section object (a region of shared memory) Very large messages: calling API to read/write directly into the address space.","title":"3.5.3 An Example: Windows"},{"location":"OS/Chap03/#36-communication-in-clientserver-systems","text":"","title":"3.6 Communication in Client\u2013Server Systems"},{"location":"OS/Chap03/#361-sockets","text":"Socket An endpoint for communication. (IP + port#) A pair of processes communicating over a network employs a pair of sockets\u2014one for each process. Socket behavior The server waits for incoming client requests by listening to a specified port. Once a request is received, the server accepts a connection from the client socket to complete the connection. Well-known ports: (all ports below 1024 are considered well known) 23: telnet 21: FTP 80: HTTP Java provides: Connection-oriented (TCP) sockets: Socket Connectionless (UDP) sockets: DatagramSocket MulticastSocket : a subclass of DatagramSocket . It allows data to be sent to multiple recipients. Loopback IP address 127.0.0.1.","title":"3.6.1 Sockets"},{"location":"OS/Chap03/#362-remote-procedure-calls","text":"The RPC was designed as a way to abstract the procedure-call mechanism for use between systems with network connections. Each message is addressed to an RPC daemon listening to a port on the remote system, and each contains an identifier specifying the function to execute and the parameters to pass to that function. The semantics of RPCs allows a client to invoke a procedure on a remote host as it would invoke a procedure locally. Stub The RPC system hides the details that allow communication to take place by providing a stub on the client side. Parameter marshalling Packaging the parameters into a form that can be transmitted over a network. Procedure of RPCs: The client invokes a RPC RPC system calls the appropriate stub (client side) passes the stub the parameters to the RPC Marshals parameter: packaging the parameters into a form that can be transmitted over a network The stub transmits a message to the server using message passing. A stub (server side) receives this message invokes the procedure on the server (optional) Return values using the same technique Issues for RPC: Data representation External Data Representation (XDR) Parameter marshalling Semantics of a call at most once exactly once (ACK) Binding of the client and server port Matchmaker (a rendezvous mechanism)","title":"3.6.2 Remote Procedure Calls"},{"location":"OS/Chap03/#363-pipes","text":"In implementing a pipe, four issues: Does the pipe allow bidirectional communication, or is communication unidirectional? If two-way communication is allowed, is it half duplex (data can travel only one way at a time) or full duplex (data can travel in both directions at the same time)? Must a relationship (such as parent\u2013child) exist between the communicating processes? Can the pipes communicate over a network, or must the communicating processes reside on the same machine?","title":"3.6.3 Pipes"},{"location":"OS/Chap03/#3631-ordinary-pipes","text":"pipe ( int fd []) Ordinarya pipes on on Windows: anonymous pipes (similar to UNIX)","title":"3.6.3.1 Ordinary Pipes"},{"location":"OS/Chap03/#3632-named-pipes","text":"Ordinary Pipes Named Pipes unidirectional bidirectional parent-child required not required In UNIX, named pipes = FIFOs. A FIFO is created with the mkfifo() . Pipes in practice: # In this scenario, the ls command serves as the producer, and its output is consumed by the more command. $ ls | more","title":"3.6.3.2 Named Pipes"},{"location":"OS/Chap04/","text":"Chapter 4 Threads 4.1 Overview Thread\u2013Lightweight process (LWP) A basit unit of CPU utilization. A thread shares code section data section OS resources (e.g. open files and signals) A thread have its own thread ID program counter register set stack 4.1.1 Motivation It is generally more efficient to use one process that contains multiple threads since process creation is time consuming and resource intensive. 4.1.2 Benefits The benefits of multithreaded: Responsiveness Resource sharing Economy Scalability/Utilization 4.2 Multicore Programming A more recent, similar trend in system design is to place multiple computing cores on a single chip. Multicore or Multiprocessor systems The cores appear across CPU chips or within CPU chips. Consider an application with 4 threads. With a single core With multiple cores Parallelism Concurrency Perform more than one task simultaneously. Allow all the tasks to make progress. Amdahl's Law If $S$ is the portion cannot be accelerated by $N$ cores (serially). $$speedup \\le \\frac{1}{S + \\frac{(1 - S)}{N}}$$ 4.2.1 Programming Challenges Identifying tasks: Dividing Activities Balance (Equal value) Data splitting Data dependency Testing and debugging 4.2.2 Types of Parallelism Data parallelism Distribute subsets of the same data across multiple computing cores. Each core performs the same operation. e.g. $$\\sum_{i = 0}^{N - 1} arr[i] = \\sum_{i = 0}^{N / 2 - 1} arr[i] (\\text{thread } A) + \\sum_{i = N / 2}^{N - 1} arr[i] (\\text{thread } B).$$ Task parallelism Distribute tasks (threads) across multiple computing cores. Each thread performs a unique operation. 4.3 Multithreading Models 4.3.1 Many-to-One Model pros: Efficiency cons: One blocking syscall blocks all No parallelism for multiple processors e.g. Green threads (Solaris) 4.3.2 One-to-One Model pros: One syscall blocks one thread cons: Overheads in creating a kernel thread e.g. Windows NT/2000/XP, Linux, OS/2, Solaris 9 4.3.3 Many-to-Many Model pros: A combination of parallelism and efficiency e.g. Solaris 2 & 9, IRIX, HP-UX, Tru64 UNIX 4.4 Thread Libraries User level Kernel level Three main thread libraries: POSIX Pthreads: User or kernel level Windows: Kernel level Java: Level depending on the thread library on the host system. Two general strategie for creating threads: Asynchronous threading: parent doesn't know children. Synchronous threading: parent must wait for all of its children. ( fork-join ) All of the following examples use synchronous threading. 4.4.1 Pthreads Pthreads The POSIX standard (IEEE 1003.1c) defining an API for thread creation and synchronization. This is a specification for thread behavior, not and implementation . 4.4.2 Windows Threads 4.4.3 Java Threads Creating a Thread object does not specifically create the new thread. $\\to$ start() does! It allocates memory and initializes a new thread in the JVM. It calls the run() method, making the thread eligible to be run by the JVM. (Note again that we never call the run() method directly. Rather, we call the start() method, and it calls the run() method on our behalf) 4.5 Implicit Threading 4.5.1 Thread Pools The issue of multithreaded server: Time to create the thread Concurrency Thread pool Create a number of threads at process startup and place them into a pool, where they sit and wait for work. The benefits of thread pools: Speed Limited # of threads, which is good for OS. Seperating the task of creating tasks allows us to use different strategies. Dynamic or static thread pools e.g. QueueUserWorkItem() , java.util.concurrent . 4.5.2 OpenMP OpenMP A set of compiler directivese and APIs to support parallel programming in shared memory environment. Threads with divided workload are created automatically based on # of cores or a set bound. Parallel regions Blocks of code that may run in parallel. When OpenMP encounters #pragma omp parallel it creates as many threads are there are processing cores in the system. e.g. #pragma omp parallel for for ( i = 0 ; i < N ; i ++ ) c [ i ] = a [ i ] + b [ i ]; 4.5.3 Grand Central Dispatch A ma sOS/iOS combination of extensions to the C. Like OpenMP, GCD manges most of the details of threading. ^ { printf ( \"I am a block.\" ); } GCD schedules blocks for run-time execution by placing them on a dispatch queue . Serial (FIFO) Concurrent (FIFO) low default high dispatch_queue_t queue = dispatch_get_global_queue ( DISPATCH_QUEUE_PRIORITY_DEFAULT , 0 ); dispatch_async ( queue , ^ { printf ( \"I am a block.\" ) }); 4.5.4 Other Approaches e.g. Intel's Threading Building Blocks (TBB), java.util.concurrent . 4.6 Threading Issues 4.6.1 The fork() and exec() System Calls fork() issue Duplicate all threads? Is the new process single-threaded? exec() issue If a thread invokese the exec() , the program specified in the parameter to exec() will replace the entire process\u2014including all threads. Thus if exec() is called immediately after forking, then duplicating all threads is unnecessary. 4.6.2 Signal Handling All signals follows: A signal is generated by the occurrence of a particular event. The signal is delivered to a process. Once delivered, the signal must be handled. Two types of signals: Synchronous signal: delivered to the same process that performed the operation causing the signal. illegal memory access division by 0 Asynchronous signal generated by an external process (e.g. ^C) having a timer expire A signal may be handled by: A default signal handler A user-defined signal handler Signal delivering: Deliver the signal to the thread to which the signal applies. (e.g. division by 0) Deliver the signal to every thread in the process. (e.g. ^C) Deliver the signal to certain threads in the process. Assign a specific thread to receive all signals for the process. Functions/Methods for delivering a signal: UNIX: kill ( pid_t pid , int signal ) POSIX: pthread_kill ( pthread_t tid , int signal ) Windows: Asynchronous Procedure Calls (APCs) 4.6.3 Thread Cancellation Target thread A thread that is to be canceled. Cancellation of a target thread may occur in two different scenarios: Asynchronous cancellation . One thread immediately terminates the target thread. Deferred cancellation . The target thread periodically checks whether it should terminate, allowing it an opportunity to terminate itself in an orderly fashion. Canceling a thread asynchronously may not free a necessary system-wide resource. pthread_tid ; /* create the thread */ pthread_create ( & tid , 0 , worker , NULL ); ... /* cancel the thread */ pthread_cancel ( tid ); Pthreads supports three cancellation modes: Mode State Type Off Disabled \u2013 Deferred Enabled Deferred Asynchronous Enabled Asynchronous Cancellation point Cancellation occurs only when a thread reaches a cancellation point. Cleanup handler If a cancellation request is found to be pending, this function allows any resources a thread may have acquired to be released before the thread is terminated. e.g. deferred cancellation: while ( 1 ) { /* do some work for awhile */ /* check if there is a cancellation request */ pthread_testcancel (); } 4.6.4 Thread-Local Storage Thread-Local Storage (TLS) Each thread might need its own copy of certain data. TLS is similar to static data. TLS Local variables visible across function invocations visible only during a single function invocation 4.6.5 Scheduler Activations Lightweight process (LWP) A virtual processor (kernel threads) on which the application can schedule a user thread to run. (many-to-many or two-level) Scheduler activation The kernel provides an application with a set of virtual processors (LWPs), and the application can schedule user threads onto an available virtual processor. Upcall The kernel must inform an application about certain events. 4.7 Operating-System Examples 4.7.1 Windows Threads The general components of a thread include: A thread ID uniquely identifying the thread A register set representing the status of the processor A user stack, employed when the thread is running in user mode A kernel stack, employed when the thread is running in kernel mode A private storage area used by various run-time libraries and dynamic link libraries (DLLs) Context (register set, stacks, and private sotrage area) of the thread: Kernel space ETHREAD\u2014executive thread block: a pointer to the process the address of the routine a pointer to the corresponding KTHREAD KTHREAD\u2014kernel thread block scheduling/synchronization information the kernel stack a pointer to TEB User space TEB\u2014thread environment block the thread ID a user-mode stack an array for TLS 4.7.2 Linux Threads clone() vs fork() term task \u2014rather than process or thread Several per-process data structures points to the same data structures for open files, signal handling, virtual memory, etc.","title":"Chapter 4 Threads"},{"location":"OS/Chap04/#chapter-4-threads","text":"","title":"Chapter 4 Threads"},{"location":"OS/Chap04/#41-overview","text":"Thread\u2013Lightweight process (LWP) A basit unit of CPU utilization. A thread shares code section data section OS resources (e.g. open files and signals) A thread have its own thread ID program counter register set stack","title":"4.1 Overview"},{"location":"OS/Chap04/#411-motivation","text":"It is generally more efficient to use one process that contains multiple threads since process creation is time consuming and resource intensive.","title":"4.1.1 Motivation"},{"location":"OS/Chap04/#412-benefits","text":"The benefits of multithreaded: Responsiveness Resource sharing Economy Scalability/Utilization","title":"4.1.2 Benefits"},{"location":"OS/Chap04/#42-multicore-programming","text":"A more recent, similar trend in system design is to place multiple computing cores on a single chip. Multicore or Multiprocessor systems The cores appear across CPU chips or within CPU chips. Consider an application with 4 threads. With a single core With multiple cores Parallelism Concurrency Perform more than one task simultaneously. Allow all the tasks to make progress. Amdahl's Law If $S$ is the portion cannot be accelerated by $N$ cores (serially). $$speedup \\le \\frac{1}{S + \\frac{(1 - S)}{N}}$$","title":"4.2 Multicore Programming"},{"location":"OS/Chap04/#421-programming-challenges","text":"Identifying tasks: Dividing Activities Balance (Equal value) Data splitting Data dependency Testing and debugging","title":"4.2.1 Programming Challenges"},{"location":"OS/Chap04/#422-types-of-parallelism","text":"Data parallelism Distribute subsets of the same data across multiple computing cores. Each core performs the same operation. e.g. $$\\sum_{i = 0}^{N - 1} arr[i] = \\sum_{i = 0}^{N / 2 - 1} arr[i] (\\text{thread } A) + \\sum_{i = N / 2}^{N - 1} arr[i] (\\text{thread } B).$$ Task parallelism Distribute tasks (threads) across multiple computing cores. Each thread performs a unique operation.","title":"4.2.2 Types of Parallelism"},{"location":"OS/Chap04/#43-multithreading-models","text":"","title":"4.3 Multithreading Models"},{"location":"OS/Chap04/#431-many-to-one-model","text":"pros: Efficiency cons: One blocking syscall blocks all No parallelism for multiple processors e.g. Green threads (Solaris)","title":"4.3.1 Many-to-One Model"},{"location":"OS/Chap04/#432-one-to-one-model","text":"pros: One syscall blocks one thread cons: Overheads in creating a kernel thread e.g. Windows NT/2000/XP, Linux, OS/2, Solaris 9","title":"4.3.2 One-to-One Model"},{"location":"OS/Chap04/#433-many-to-many-model","text":"pros: A combination of parallelism and efficiency e.g. Solaris 2 & 9, IRIX, HP-UX, Tru64 UNIX","title":"4.3.3 Many-to-Many Model"},{"location":"OS/Chap04/#44-thread-libraries","text":"User level Kernel level Three main thread libraries: POSIX Pthreads: User or kernel level Windows: Kernel level Java: Level depending on the thread library on the host system. Two general strategie for creating threads: Asynchronous threading: parent doesn't know children. Synchronous threading: parent must wait for all of its children. ( fork-join ) All of the following examples use synchronous threading.","title":"4.4 Thread Libraries"},{"location":"OS/Chap04/#441-pthreads","text":"Pthreads The POSIX standard (IEEE 1003.1c) defining an API for thread creation and synchronization. This is a specification for thread behavior, not and implementation .","title":"4.4.1 Pthreads"},{"location":"OS/Chap04/#442-windows-threads","text":"","title":"4.4.2 Windows Threads"},{"location":"OS/Chap04/#443-java-threads","text":"Creating a Thread object does not specifically create the new thread. $\\to$ start() does! It allocates memory and initializes a new thread in the JVM. It calls the run() method, making the thread eligible to be run by the JVM. (Note again that we never call the run() method directly. Rather, we call the start() method, and it calls the run() method on our behalf)","title":"4.4.3 Java Threads"},{"location":"OS/Chap04/#45-implicit-threading","text":"","title":"4.5 Implicit Threading"},{"location":"OS/Chap04/#451-thread-pools","text":"The issue of multithreaded server: Time to create the thread Concurrency Thread pool Create a number of threads at process startup and place them into a pool, where they sit and wait for work. The benefits of thread pools: Speed Limited # of threads, which is good for OS. Seperating the task of creating tasks allows us to use different strategies. Dynamic or static thread pools e.g. QueueUserWorkItem() , java.util.concurrent .","title":"4.5.1 Thread Pools"},{"location":"OS/Chap04/#452-openmp","text":"OpenMP A set of compiler directivese and APIs to support parallel programming in shared memory environment. Threads with divided workload are created automatically based on # of cores or a set bound. Parallel regions Blocks of code that may run in parallel. When OpenMP encounters #pragma omp parallel it creates as many threads are there are processing cores in the system. e.g. #pragma omp parallel for for ( i = 0 ; i < N ; i ++ ) c [ i ] = a [ i ] + b [ i ];","title":"4.5.2 OpenMP"},{"location":"OS/Chap04/#453-grand-central-dispatch","text":"A ma sOS/iOS combination of extensions to the C. Like OpenMP, GCD manges most of the details of threading. ^ { printf ( \"I am a block.\" ); } GCD schedules blocks for run-time execution by placing them on a dispatch queue . Serial (FIFO) Concurrent (FIFO) low default high dispatch_queue_t queue = dispatch_get_global_queue ( DISPATCH_QUEUE_PRIORITY_DEFAULT , 0 ); dispatch_async ( queue , ^ { printf ( \"I am a block.\" ) });","title":"4.5.3 Grand Central Dispatch"},{"location":"OS/Chap04/#454-other-approaches","text":"e.g. Intel's Threading Building Blocks (TBB), java.util.concurrent .","title":"4.5.4 Other Approaches"},{"location":"OS/Chap04/#46-threading-issues","text":"","title":"4.6 Threading Issues"},{"location":"OS/Chap04/#461-the-fork-and-exec-system-calls","text":"fork() issue Duplicate all threads? Is the new process single-threaded? exec() issue If a thread invokese the exec() , the program specified in the parameter to exec() will replace the entire process\u2014including all threads. Thus if exec() is called immediately after forking, then duplicating all threads is unnecessary.","title":"4.6.1 The fork() and exec() System Calls"},{"location":"OS/Chap04/#462-signal-handling","text":"All signals follows: A signal is generated by the occurrence of a particular event. The signal is delivered to a process. Once delivered, the signal must be handled. Two types of signals: Synchronous signal: delivered to the same process that performed the operation causing the signal. illegal memory access division by 0 Asynchronous signal generated by an external process (e.g. ^C) having a timer expire A signal may be handled by: A default signal handler A user-defined signal handler Signal delivering: Deliver the signal to the thread to which the signal applies. (e.g. division by 0) Deliver the signal to every thread in the process. (e.g. ^C) Deliver the signal to certain threads in the process. Assign a specific thread to receive all signals for the process. Functions/Methods for delivering a signal: UNIX: kill ( pid_t pid , int signal ) POSIX: pthread_kill ( pthread_t tid , int signal ) Windows: Asynchronous Procedure Calls (APCs)","title":"4.6.2 Signal Handling"},{"location":"OS/Chap04/#463-thread-cancellation","text":"Target thread A thread that is to be canceled. Cancellation of a target thread may occur in two different scenarios: Asynchronous cancellation . One thread immediately terminates the target thread. Deferred cancellation . The target thread periodically checks whether it should terminate, allowing it an opportunity to terminate itself in an orderly fashion. Canceling a thread asynchronously may not free a necessary system-wide resource. pthread_tid ; /* create the thread */ pthread_create ( & tid , 0 , worker , NULL ); ... /* cancel the thread */ pthread_cancel ( tid ); Pthreads supports three cancellation modes: Mode State Type Off Disabled \u2013 Deferred Enabled Deferred Asynchronous Enabled Asynchronous Cancellation point Cancellation occurs only when a thread reaches a cancellation point. Cleanup handler If a cancellation request is found to be pending, this function allows any resources a thread may have acquired to be released before the thread is terminated. e.g. deferred cancellation: while ( 1 ) { /* do some work for awhile */ /* check if there is a cancellation request */ pthread_testcancel (); }","title":"4.6.3 Thread Cancellation"},{"location":"OS/Chap04/#464-thread-local-storage","text":"Thread-Local Storage (TLS) Each thread might need its own copy of certain data. TLS is similar to static data. TLS Local variables visible across function invocations visible only during a single function invocation","title":"4.6.4 Thread-Local Storage"},{"location":"OS/Chap04/#465-scheduler-activations","text":"Lightweight process (LWP) A virtual processor (kernel threads) on which the application can schedule a user thread to run. (many-to-many or two-level) Scheduler activation The kernel provides an application with a set of virtual processors (LWPs), and the application can schedule user threads onto an available virtual processor. Upcall The kernel must inform an application about certain events.","title":"4.6.5 Scheduler Activations"},{"location":"OS/Chap04/#47-operating-system-examples","text":"","title":"4.7 Operating-System Examples"},{"location":"OS/Chap04/#471-windows-threads","text":"The general components of a thread include: A thread ID uniquely identifying the thread A register set representing the status of the processor A user stack, employed when the thread is running in user mode A kernel stack, employed when the thread is running in kernel mode A private storage area used by various run-time libraries and dynamic link libraries (DLLs) Context (register set, stacks, and private sotrage area) of the thread: Kernel space ETHREAD\u2014executive thread block: a pointer to the process the address of the routine a pointer to the corresponding KTHREAD KTHREAD\u2014kernel thread block scheduling/synchronization information the kernel stack a pointer to TEB User space TEB\u2014thread environment block the thread ID a user-mode stack an array for TLS","title":"4.7.1 Windows Threads"},{"location":"OS/Chap04/#472-linux-threads","text":"clone() vs fork() term task \u2014rather than process or thread Several per-process data structures points to the same data structures for open files, signal handling, virtual memory, etc.","title":"4.7.2 Linux Threads"},{"location":"OS/Chap05/","text":"Chapter 5 Process Synchronization 5.1 Background Recall producer\u2013consumer problem . We modify it as follows: while ( true ) { /* produce an item in next_produced */ while ( counter == BUFFER_SIZE ) ; // do nothing buffer [ in ] = next_produced ; in = ( in + 1 ) % BUFFER_SIZE ; counter ++ ; } while ( true ) { while ( counter == 0 ) ; // do nothing next_consumed = buffer [ out ]; out = ( out + 1 ) % BUFFER_SIZE ; counter -- ; /* consume the item in next_consumed */ } Suppose counter == 5 initially. After executing counter++ in producer or counter-- in consumer. The value of counter may be $4$, $5$, or $6$! \\begin{array}{lllll} T_0: producer & \\text{execute} & register_1 = \\text{counter} & \\{register_1 = 5\\} \\\\ T_1: producer & \\text{execute} & register_1 = register_1 + 1 & \\{register_1 = 6\\} \\\\ T_2: consumer & \\text{execute} & register_2 = \\text{counter} & \\{register_2 = 5\\} \\\\ T_3: consumer & \\text{execute} & register_2 = register_2 - 1 & \\{register_2 = 4\\} \\\\ T_4: producer & \\text{execute} & \\text{counter} = register_1 & \\{counter = 6\\} \\\\ T_5: consumer & \\text{execute} & \\text{counter} = register_2 & \\{counter = 4\\} \\end{array} Race condition Several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place. We need to ensure that only one process at a time can be manipulating the variable counter . 5.2 The Critical-Section Problem Critical section In which the process may be changing common variables, updating a table, writing a file, and so on. do { /* entry section */ /* critical section */ /* exit section */ /* remainder section */ } while ( true ); A solution to the critical-section problem must satisfy: Mutual exclusion Progress . Cannot be postponed indefinitely. Bounded waiting Two general approches are used to handle critical sections: Preemptive kernels Nonpreemptive kernels Why anyone favor a preemptive kernel over a nonpreemptive one? Responsive. More suitable for real-time programming. 5.3 Peterson's Solution Peterson's solution is restricted to two processes that alternate execution between their critical sections and remainder sections. The processes are named $P_i$ and $P_j$. Shared data items: int turn ; boolean flag [ 2 ]; do { flag [ i ] = true ; turn = j ; while ( flag [ j ] && turn == j ) ; /* critical section */ flag [ i ] = false ; /* remainder section */ } while ( true ); If both processes try to enter at the same time, turn will be set to both $i$ and $j$ at roughly the same time. Only one of these assignments will last. Proof Mutual exclusion is preserved. The progress requirement is satisfied. The bounded-waiting requirement is met. Suppose $P_i$ execute $turn = j$ first, then $P_j$ execute $turn = i$. In this assumption, $P_i$ will enter its critical section first and $P_j$ will be stucked in the while(flag[i] && turn == i (remember that here i should be thinked as j in the code!). After $P_i$ entering exit section, there are two possibilities: $P_i$ sets flag[i] = false , then $P_j$ enters its critical section. After $P_i$ setting flag[i] = false , it immediately sets flag[i] = true again, consequently, it'll set turn = j , thus $P_j$ still can enter its critical section. Bakery Algorithm Originally designed for distrubuted systems Processes which are ready to enter their critical section must take a number and wait till the number becomes the lowest. int number [ i ]; // Pi's number if it is nonzeros boolean choosing [ i ]; // Pi is taking a number do { choosing [ i ] = true ; // A process want to enter its critical section number [ i ] = max ( number [ 0 ], ..., number [ n - 1 ]) + 1 ; choosing [ i ] = false ; // A process has got its number for ( int j = 0 ; j < n ; j ++ ) { while ( choosing [ j ]) ; while ( number [ j ] != 0 && ( number [ j ], j ) < ( number [ i ], i )) ; // If two processes got the same number, then we should compare their indices } /* critical section */ number [ i ] = 0 ; /* remainder section */ } while ( true ); An observation: If $P_i$ is in its critical section, and $P_k (k \\ne i)$ , then $(number[i], i) < (number[k], k)$. Proof Mutual exclusion: Only the process holds the lowest number can enter the critical section. For each process, when that process doens't get its number, the original process will be stucked in the first while-loop. After that process getting its number, we still need to compare their $numbers$ and $indices$. Progress requirement: The processes won't be forever postponed. Bounded-waiting: Assume that a process holds the biggest number, it should wait other processes in the second while-loop. But after all other process entering their exit section and again entering their entry section, they'll get a bigger number, thus the process won't wait forever. 5.4 Synchronization Hardware Disaple interrupt $\\to$ No preemption: Infeasible in multiprocessor environment. Potential impacts on interrupt-driven system clocks. Atomic Modern computer allow us either to test and modify the content of a word or to swap the contents of two words atomically \u2014that is, as one uninterruptible. Although following algorithms satisfy the mutual-exclusion requirement, they don't satisfy the bounded-waiting requirement. boolean test_and_set ( boolean * target ) { boolean rv = * target ; * target = true ; return rv ; } do { while ( test_and_set ( & lock )) ; /* critical section */ lock = false ; /* remainder section */ } while ( true ); The first process executing while (test_and_set(&lock)) will set the address value of lock to true and get the return value rv = false , thus it won't be stucked in the while-loop and it can enter its critical section. Mutual exclusion: OK Progress requirement: OK Bounded-waiting: FAIL Assume there is only one CPU, after $P_i$ entering its critical section, $P_j$ will be stucked in the while-loop. After $P_i$ exiting its critical section, there are two possibilities: $P_i$ sets lock = false , the CPU context switch to $P_j$, thus $P_j$ can enters its critical section. After $P_i$ setting lock = false , the CPU still executes the code of $P_i$, thus $P_i$ enters its critical section again, so $P_j$ may wait forever. void swap ( boolean * a , boolean * b ) { boolean temp = * a ; * a = * b ; * b = temp ; } do { key = true ; while ( key == true ) swap ( & lock , & key ); /* critical section */ lock = false ; /* remainder section */ } while ( true ); Mutual exclusion: OK Progress requirement: OK Bounded-waiting: FAIL (the reason is like above) int compare_and_swap ( int * value , int expected , int new_value ) { int temp = * value ; if ( * value == expected ) * value = new_value ; return temp ; } do { while ( compare_and_swap ( & lock , 0 , 1 ) != 0 ) ; /* critical section */ lock = 0 ; /* remainder section */ } while ( true ); Following algorithms satisfies all the critical-section requirements. boolean waiting [ n ]; boolean lock ; do { waiting [ i ] = true ; key = true ; while ( waiting [ i ] && key ) key = test_and_set ( & lock ); waiting [ i ] = false ; /* critical section */ j = ( i + 1 ) % n ; // Assign its next process while (( j != i ) && ! waiting [ j ]) // Find a following process who is waiting j = ( j + 1 ) % n ; if ( j == i ) // If no process is waiting lock = false ; else waiting [ j ] = false ; // Thus line 4 will be false and Pj won't be stucked anymore /* remainder section */ } while ( true ); boolean test_and_set ( boolean * target ) { boolean rv = * target ; * target = true ; return rv ; } Assume lock is initialized to false . Mutual exclusion: If many processes set their waiting[i] = true , after the first process execute key = test_and_set(&lock) , key will be set to false and lock will be set to true . Therefore, other processes will be stucked in while (waiting[i] && key) since their key will be set to true after test_and_set(&lock) ( lock is now true ). Progress requirement: Only the process first run test_and_set can enter its critical section. Bounded-waiting: Wait at most $n - 1$ times. Mutex Locks A high-level software solution to provide protect critical sections with mutual exclusion. Atomic execution of acquire() and release() . Spinlock: pros: No context switch for multiprocessor systems. cons: Busy waiting. acquire () { while ( ! available ) ; // busy wait available = false ; } release () { available = true ; } do { // acquire lock /* critical section */ // release lock /* remainder section */ } while ( true ); Spinlock The process \"spins\" while waiting for the lock to become available. 5.6 Semaphores A high-level solution for more complex problems. A variable S only accessible by two atomic operations Spinlock wait ( S ) { /* P */ while ( S <= 0 ) ; // busy wait S -- ; } signal ( S ) { /* V */ S ++ ; } 5.6.1 Semaphore Usage Critical sections: do { wait ( mutex ); /* critical section */ signal ( mutex ); /* remainder section */ } while ( true ); Precedence enforcement: $P_1$: S1 ; signal ( synch ); $P_2$: wait ( synch ); S2 ; 5.6.2 Semaphore Implementation It's not good for single CPU. Even if it's implemented in a multi-CPU environment, the locks should be held for a short time. We can implement the Semaphores with block waiting: typedef struct { int value ; struct process * list ; } semaphore ; wait ( semaphore * S ) { S -> value -- ; if ( S -> value < 0 ) { add this process to S -> list ; block (); } } signal ( semaphore * S ) { S -> value ++ ; if ( S -> value <= 0 ) { remove a process P from S -> list ; wakeup ( P ); } } $|S.value|$ = # of waiting processes if $S.value < 0$. Bounded-waiting can be satisfied by FIFO queue but may be unsatisfied by priority queue. 5.6.3 Deadlocks and Starvation Deadlock A set of processes is in a deadlock state when every process in the set is waiting for an event that can be caused only by another process in the set. \\begin{array}{cc} P_0 & P_1 \\\\ wait(S); & wait(Q); \\\\ wait(Q); & wait(S); \\\\ \\vdots & \\vdots \\\\ signal(S); & signal(Q); \\\\ signal(Q); & signal(S); \\\\ \\end{array} Deadlock may happen (assume $S = 1$ and $Q = 1$: $P_0$ calls $wait(S)$ $P_1$ calls $wait(Q)$ $P_1$ calls $wait(S)$ $P_0$ calls $wait(Q)$ Starvation (Indefinite blocking) A situation in which processes wait indefinitely within the semaphore. e.g. priority queue, stack (LIFO). 5.6.4 Priority Inversion Priority Inversion A higher-priority task is blocked by a lower-priority task due to some resource access conflict. Binary Semaphore We can implement counting semaphores by binary semaphores. ($S_1 = 1$, $S_2 = 0$ and $S_3 = 1$) WAIT ( S ) { wait ( S3 ); // protect the whole program wait ( S1 ); // protect C C -- ; if ( C < 0 ) { signal ( S1 ); wait ( S2 ); } else signal ( S1 ); signal ( S3 ); } SIGNAL ( S ) { wait ( S1 ); C ++ ; if ( C <= 0 ) signal ( S2 ); // wake up signal ( S1 ); } Is wait(S3) necessary? Can we change the order of signal(S1) and wait(S2) ? There are lots of implementation details. 5.7 Classic Problems of Synchronization 5.7.1 The Bounded-Buffer Problem int n ; semaphore mutex = 1 ; semaphore empty = n ; semaphore full = 0 ; Producer: do { /* produce an item in next_produced */ wait ( empty ); wait ( mutex ); /* add next_produced to the buffer */ signal ( mutex ); signal ( full ); } while ( true ); Consumer: do { wait ( full ); wait ( mutex ); /* remove an item from buffer to next_consumed */ signal ( mutex ); signal ( empty ); /* consume the item in next_consumed */ } while ( true ); 5.7.2 The Readers\u2013Writers Problem The basic assumption: Readers: shared locks Writers: exclusive locks The first reader-writers problem No readers will be kept waiting unless a writer has already obtained permission to use the shared object $\\to$ potential hazard to writers! The second reader-writers problem One a writer is ready, it performs its write asap $\\to$ potential hazard to readers. semaphore rw_mutex = 1 ; semaphore mutex = 1 ; int read_count = 0 ; do { wait ( rw_mutex ); /* writing is performed */ signal ( rw_mutex ); } while ( true ); do { wait ( mutex ); // protect read_count read_count ++ ; if ( read_count == 1 ) wait ( rw_mutex ); signal ( mutex ); /* reading is performed */ wait ( mutex ); // protect read_count read_count -- ; if ( read_count == 0 ) signal ( rw_mutex ); signal ( mutex ); } while ( true ); 5.7.3 The Dining-Philosophers Problem Each philosopher must pick up one chopstick beside him/her at a time. When two chopsticks are picked up, the philosopher can eat. semaphore chopstick [ 5 ]; do { wait ( chopstick [ i ]); wait ( chopstick [( i + 1 ) % 5 ]); /* eat for awhile */ signal ( chopstick [ i ]); signal ( chopstick [( i + 1 ) % 5 ]); /* think for awhile */ } while ( true ); Critical Regions Region $v$ when $C$ (condition) do $S$ (statements) Variable $v$ \u2014 shared among processes and only accessible in the region. struct buffer { item pool [ n ]; int count , in , out ; }; Producer: region buffer when ( count < n ) { pool [ in ] = next_produced ; in = ( in + 1 ) % n ; count ++ ; } Consumer: region buffer when ( count > 0 ) { next_consumed = pool [ out ]; out = ( out + 1 ) % n ; count -- ; } 5.8 Monitors monitor monitor name { /* shared variable declarations */ function P1 ( . . . ) { . . . } function P2 ( . . . ) { . . . } . . . function Pn ( . . . ) { . . . } initialization_code ( . . . ) { . . . } } 5.8.1 Monitor Usage An abstract data type\u2014or ADT\u2014encapsulates data with a set of functions to operate on that data that are independent of any specific implementation of the ADT. The monitor construct ensures that only one process at a time is active within the monitor. A programmer who needs to write a tailor-made synchronization scheme can define one or more variables of type $condition$: condition x , y ; The only operations that can be invoked on a condition variable are wait() and signal() . The operation x . wait (); means that the process invoking this operation is suspended until another process invokes x . signal (); Condition variables (of a monitor) vs. signal operation (of binary semaphore) The x.signal() operation resumes exactly one suspended process. If no process is suspended, then the signal() operation has no effect; that is, the state of x is the same as if the operation had never been executed. Contrast this operation with the signal() operation associated with semaphores, which always affects the state of the semaphore. Suppose that, when the x.signal() operation is invoked by a process P , there exists a suspended process Q associated with condition x . Clearly, if the suspended process Q is allowed to resume its execution, the signaling process P must wait. Two possibilities exist: Signal and wait : P either waits until Q leaves the monitor or waits for another condition. Signal and continue : Q either waits until P leaves the monitor or waits for another condition. 5.8.2 Dining-Philosophers Solution Using Monitors enum { THINKING , HUNGRY , EATING } state [ 5 ]; condition self [ 5 ]; monitor DiningPhilosophers { enum { THINKING , HUNGRY , EATING } state [ 5 ]; condition self [ 5 ]; void pickup ( int i ) { state [ i ] = HUNGRY ; test ( i ); if ( state [ i ] != EATING ) self [ i ]. wait (); } void putdown ( int i ) { state [ i ] = THINKING ; test (( i + 4 ) % 5 ); // help your right-hand side to run test test (( i + 1 ) % 5 ); // help your left-hand side to run test } void test ( int i ) { if (( state [( i + 4 ) % 5 ] != EATING ) && // right-hand side ( state [ i ] == HUNGRY ) && ( state [( i + 1 ) % 5 ] != EATING )) { // left-hand side state [ i ] = EATING ; self [ i ]. signal (); } } initialization_code () { for ( int i = 0 ; i < 5 ; i ++ ) state [ i ] = THINKING ; } } $P_i$: DiningPhilosophers . pickup ( i ); /* eat */ DiningPhilosophers . putdown ( i ); No deadlock, but starvation could occur! 5.8.3 Implementing a Monitor Using Semaphores Semaphores mutex : to protect the monitor next : being initialized to zero, on which processes may suspend themselves next_count Each external function F is replaced by wait ( mutex ); /* body of F */ if ( next_count > 0 ) signal ( next ); else signal ( mutex ); For each condition x , we introduce a semaphore x_sem and an integer variable x_count , both initialized to $0$. x.wait() x_count ++ ; if ( next_count > 0 ) signal ( next ); else signal ( mutex ); wait ( x_sem ); x_count -- ; x.signal() if ( x_count > 0 ) { /* If there's somebody being waiting */ next_count ++ ; signal ( x_sem ); wait ( next ); next_count -- ; } 5.8.4 Resuming Processes within a Monitor conditional-wait : x . wait ( c ); where $c$ is a priority number . When x.signal() is executed, the process with the smallest priority number is resumed next. Consider the ResourceAllocator monitor, which controls the allocation of a single resource among competing processes. monitor ResourceAllocator { boolean busy ; condition x ; void acquire ( int time ) { if ( busy ) x . wait ( time ); busy = true ; } void release () { busy = false ; x . signal (); } initialization_code () { busy = false ; } } A process that needs to access the resource in question must observe the following sequence: R . acquire ( t ); /* access the resource */ R . release (); The monitor concept cannot guarantee that the preceding access sequence will be observed. In particular, the following problems can occur: A process might access a resource without first gaining access permission to the resource. A process might never release a resource once it has been granted access to the resource. A process might attempt to release a resource that it never requested. A process might request the same resource twice (without first releasing the resource). 5.9 Synchronization Examples 5.9.1 Synchronization in Windows General Mechanism Spin-locking for short code segments in a multiprocessor platform. Interrupt disabling when the kernel accesses global variables in a uniprocessor platform. Dispatcher Object State: signaled or non-signaled Mutex: select one process from its waiting queue to the ready queue. Critical-section object \u2014 user-mode mutex Events: like condition variables Timers: select all waiting processes 5.9.2 Synchronization in Linux Preemptive kernel after version 2.6. Atomic integer atomic_t counter ; ... atomic_set ( & counter , 5 ); atomic_add ( 10 , & counter ); Semaphores for long code segments. Mutex locks for the kernel code. Sping-locking for short code segments in a multiprocessor platform. Preeption disabling and enabling in a uniprocessor platform. preempt_disable() and preempt_enable() preempt_count for each task in the system 5.9.3 Synchronization in Solaris Semaphores and condition variables Adaptive mutex spin-locking if the lock-holding thread is running; otherwise, blocking is used Readers-writers locks expensive in implementations Turnstile A queue structure containing threads blocked on a lock Priotity inversion $\\to$ priority inheritance protocol for kernel threads 5.9.4 Pthreads Synchronization General Mechanism Mutex locks: mutual exclusion condition variables: monitor Read-write locks 5.10 Alternative Approaches 5.10.1 Transactional Memory Memory transaction A sequence of memory read-write operations that are atomic. Committed or being roleld back: void update () { atomic { /* modify shared data */ } } Advantages: No deadlock Identification of potentially concurrently executing statements in atomic blocks Implementations: Software transaction memory Code is inserted by the compiler . Hardware transactional memory Hardware cache hierarchies and cache coherency protocols are used. 5.10.2 OpenMP A set of compiler directives and an API The critical-section compiler directive behaves like a binary semaphore or mutex. void update () { #pragma omp critical { counter += value ; } } pros: easy to use cons: identification of protected code and potentials of deadlocks 5.10.3 Functional Programming Languages","title":"Chapter 5 Process Synchronization"},{"location":"OS/Chap05/#chapter-5-process-synchronization","text":"","title":"Chapter 5 Process Synchronization"},{"location":"OS/Chap05/#51-background","text":"Recall producer\u2013consumer problem . We modify it as follows: while ( true ) { /* produce an item in next_produced */ while ( counter == BUFFER_SIZE ) ; // do nothing buffer [ in ] = next_produced ; in = ( in + 1 ) % BUFFER_SIZE ; counter ++ ; } while ( true ) { while ( counter == 0 ) ; // do nothing next_consumed = buffer [ out ]; out = ( out + 1 ) % BUFFER_SIZE ; counter -- ; /* consume the item in next_consumed */ } Suppose counter == 5 initially. After executing counter++ in producer or counter-- in consumer. The value of counter may be $4$, $5$, or $6$! \\begin{array}{lllll} T_0: producer & \\text{execute} & register_1 = \\text{counter} & \\{register_1 = 5\\} \\\\ T_1: producer & \\text{execute} & register_1 = register_1 + 1 & \\{register_1 = 6\\} \\\\ T_2: consumer & \\text{execute} & register_2 = \\text{counter} & \\{register_2 = 5\\} \\\\ T_3: consumer & \\text{execute} & register_2 = register_2 - 1 & \\{register_2 = 4\\} \\\\ T_4: producer & \\text{execute} & \\text{counter} = register_1 & \\{counter = 6\\} \\\\ T_5: consumer & \\text{execute} & \\text{counter} = register_2 & \\{counter = 4\\} \\end{array} Race condition Several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place. We need to ensure that only one process at a time can be manipulating the variable counter .","title":"5.1 Background"},{"location":"OS/Chap05/#52-the-critical-section-problem","text":"Critical section In which the process may be changing common variables, updating a table, writing a file, and so on. do { /* entry section */ /* critical section */ /* exit section */ /* remainder section */ } while ( true ); A solution to the critical-section problem must satisfy: Mutual exclusion Progress . Cannot be postponed indefinitely. Bounded waiting Two general approches are used to handle critical sections: Preemptive kernels Nonpreemptive kernels Why anyone favor a preemptive kernel over a nonpreemptive one? Responsive. More suitable for real-time programming.","title":"5.2 The Critical-Section Problem"},{"location":"OS/Chap05/#53-petersons-solution","text":"Peterson's solution is restricted to two processes that alternate execution between their critical sections and remainder sections. The processes are named $P_i$ and $P_j$. Shared data items: int turn ; boolean flag [ 2 ]; do { flag [ i ] = true ; turn = j ; while ( flag [ j ] && turn == j ) ; /* critical section */ flag [ i ] = false ; /* remainder section */ } while ( true ); If both processes try to enter at the same time, turn will be set to both $i$ and $j$ at roughly the same time. Only one of these assignments will last. Proof Mutual exclusion is preserved. The progress requirement is satisfied. The bounded-waiting requirement is met. Suppose $P_i$ execute $turn = j$ first, then $P_j$ execute $turn = i$. In this assumption, $P_i$ will enter its critical section first and $P_j$ will be stucked in the while(flag[i] && turn == i (remember that here i should be thinked as j in the code!). After $P_i$ entering exit section, there are two possibilities: $P_i$ sets flag[i] = false , then $P_j$ enters its critical section. After $P_i$ setting flag[i] = false , it immediately sets flag[i] = true again, consequently, it'll set turn = j , thus $P_j$ still can enter its critical section.","title":"5.3 Peterson's Solution"},{"location":"OS/Chap05/#bakery-algorithm","text":"Originally designed for distrubuted systems Processes which are ready to enter their critical section must take a number and wait till the number becomes the lowest. int number [ i ]; // Pi's number if it is nonzeros boolean choosing [ i ]; // Pi is taking a number do { choosing [ i ] = true ; // A process want to enter its critical section number [ i ] = max ( number [ 0 ], ..., number [ n - 1 ]) + 1 ; choosing [ i ] = false ; // A process has got its number for ( int j = 0 ; j < n ; j ++ ) { while ( choosing [ j ]) ; while ( number [ j ] != 0 && ( number [ j ], j ) < ( number [ i ], i )) ; // If two processes got the same number, then we should compare their indices } /* critical section */ number [ i ] = 0 ; /* remainder section */ } while ( true ); An observation: If $P_i$ is in its critical section, and $P_k (k \\ne i)$ , then $(number[i], i) < (number[k], k)$. Proof Mutual exclusion: Only the process holds the lowest number can enter the critical section. For each process, when that process doens't get its number, the original process will be stucked in the first while-loop. After that process getting its number, we still need to compare their $numbers$ and $indices$. Progress requirement: The processes won't be forever postponed. Bounded-waiting: Assume that a process holds the biggest number, it should wait other processes in the second while-loop. But after all other process entering their exit section and again entering their entry section, they'll get a bigger number, thus the process won't wait forever.","title":"Bakery Algorithm"},{"location":"OS/Chap05/#54-synchronization-hardware","text":"Disaple interrupt $\\to$ No preemption: Infeasible in multiprocessor environment. Potential impacts on interrupt-driven system clocks. Atomic Modern computer allow us either to test and modify the content of a word or to swap the contents of two words atomically \u2014that is, as one uninterruptible. Although following algorithms satisfy the mutual-exclusion requirement, they don't satisfy the bounded-waiting requirement. boolean test_and_set ( boolean * target ) { boolean rv = * target ; * target = true ; return rv ; } do { while ( test_and_set ( & lock )) ; /* critical section */ lock = false ; /* remainder section */ } while ( true ); The first process executing while (test_and_set(&lock)) will set the address value of lock to true and get the return value rv = false , thus it won't be stucked in the while-loop and it can enter its critical section. Mutual exclusion: OK Progress requirement: OK Bounded-waiting: FAIL Assume there is only one CPU, after $P_i$ entering its critical section, $P_j$ will be stucked in the while-loop. After $P_i$ exiting its critical section, there are two possibilities: $P_i$ sets lock = false , the CPU context switch to $P_j$, thus $P_j$ can enters its critical section. After $P_i$ setting lock = false , the CPU still executes the code of $P_i$, thus $P_i$ enters its critical section again, so $P_j$ may wait forever. void swap ( boolean * a , boolean * b ) { boolean temp = * a ; * a = * b ; * b = temp ; } do { key = true ; while ( key == true ) swap ( & lock , & key ); /* critical section */ lock = false ; /* remainder section */ } while ( true ); Mutual exclusion: OK Progress requirement: OK Bounded-waiting: FAIL (the reason is like above) int compare_and_swap ( int * value , int expected , int new_value ) { int temp = * value ; if ( * value == expected ) * value = new_value ; return temp ; } do { while ( compare_and_swap ( & lock , 0 , 1 ) != 0 ) ; /* critical section */ lock = 0 ; /* remainder section */ } while ( true ); Following algorithms satisfies all the critical-section requirements. boolean waiting [ n ]; boolean lock ; do { waiting [ i ] = true ; key = true ; while ( waiting [ i ] && key ) key = test_and_set ( & lock ); waiting [ i ] = false ; /* critical section */ j = ( i + 1 ) % n ; // Assign its next process while (( j != i ) && ! waiting [ j ]) // Find a following process who is waiting j = ( j + 1 ) % n ; if ( j == i ) // If no process is waiting lock = false ; else waiting [ j ] = false ; // Thus line 4 will be false and Pj won't be stucked anymore /* remainder section */ } while ( true ); boolean test_and_set ( boolean * target ) { boolean rv = * target ; * target = true ; return rv ; } Assume lock is initialized to false . Mutual exclusion: If many processes set their waiting[i] = true , after the first process execute key = test_and_set(&lock) , key will be set to false and lock will be set to true . Therefore, other processes will be stucked in while (waiting[i] && key) since their key will be set to true after test_and_set(&lock) ( lock is now true ). Progress requirement: Only the process first run test_and_set can enter its critical section. Bounded-waiting: Wait at most $n - 1$ times.","title":"5.4 Synchronization Hardware"},{"location":"OS/Chap05/#mutex-locks","text":"A high-level software solution to provide protect critical sections with mutual exclusion. Atomic execution of acquire() and release() . Spinlock: pros: No context switch for multiprocessor systems. cons: Busy waiting. acquire () { while ( ! available ) ; // busy wait available = false ; } release () { available = true ; } do { // acquire lock /* critical section */ // release lock /* remainder section */ } while ( true ); Spinlock The process \"spins\" while waiting for the lock to become available.","title":"Mutex Locks"},{"location":"OS/Chap05/#56-semaphores","text":"A high-level solution for more complex problems. A variable S only accessible by two atomic operations Spinlock wait ( S ) { /* P */ while ( S <= 0 ) ; // busy wait S -- ; } signal ( S ) { /* V */ S ++ ; }","title":"5.6 Semaphores"},{"location":"OS/Chap05/#561-semaphore-usage","text":"Critical sections: do { wait ( mutex ); /* critical section */ signal ( mutex ); /* remainder section */ } while ( true ); Precedence enforcement: $P_1$: S1 ; signal ( synch ); $P_2$: wait ( synch ); S2 ;","title":"5.6.1 Semaphore Usage"},{"location":"OS/Chap05/#562-semaphore-implementation","text":"It's not good for single CPU. Even if it's implemented in a multi-CPU environment, the locks should be held for a short time. We can implement the Semaphores with block waiting: typedef struct { int value ; struct process * list ; } semaphore ; wait ( semaphore * S ) { S -> value -- ; if ( S -> value < 0 ) { add this process to S -> list ; block (); } } signal ( semaphore * S ) { S -> value ++ ; if ( S -> value <= 0 ) { remove a process P from S -> list ; wakeup ( P ); } } $|S.value|$ = # of waiting processes if $S.value < 0$. Bounded-waiting can be satisfied by FIFO queue but may be unsatisfied by priority queue.","title":"5.6.2 Semaphore Implementation"},{"location":"OS/Chap05/#563-deadlocks-and-starvation","text":"Deadlock A set of processes is in a deadlock state when every process in the set is waiting for an event that can be caused only by another process in the set. \\begin{array}{cc} P_0 & P_1 \\\\ wait(S); & wait(Q); \\\\ wait(Q); & wait(S); \\\\ \\vdots & \\vdots \\\\ signal(S); & signal(Q); \\\\ signal(Q); & signal(S); \\\\ \\end{array} Deadlock may happen (assume $S = 1$ and $Q = 1$: $P_0$ calls $wait(S)$ $P_1$ calls $wait(Q)$ $P_1$ calls $wait(S)$ $P_0$ calls $wait(Q)$ Starvation (Indefinite blocking) A situation in which processes wait indefinitely within the semaphore. e.g. priority queue, stack (LIFO).","title":"5.6.3 Deadlocks and Starvation"},{"location":"OS/Chap05/#564-priority-inversion","text":"Priority Inversion A higher-priority task is blocked by a lower-priority task due to some resource access conflict.","title":"5.6.4 Priority Inversion"},{"location":"OS/Chap05/#binary-semaphore","text":"We can implement counting semaphores by binary semaphores. ($S_1 = 1$, $S_2 = 0$ and $S_3 = 1$) WAIT ( S ) { wait ( S3 ); // protect the whole program wait ( S1 ); // protect C C -- ; if ( C < 0 ) { signal ( S1 ); wait ( S2 ); } else signal ( S1 ); signal ( S3 ); } SIGNAL ( S ) { wait ( S1 ); C ++ ; if ( C <= 0 ) signal ( S2 ); // wake up signal ( S1 ); } Is wait(S3) necessary? Can we change the order of signal(S1) and wait(S2) ? There are lots of implementation details.","title":"Binary Semaphore"},{"location":"OS/Chap05/#57-classic-problems-of-synchronization","text":"","title":"5.7 Classic Problems of Synchronization"},{"location":"OS/Chap05/#571-the-bounded-buffer-problem","text":"int n ; semaphore mutex = 1 ; semaphore empty = n ; semaphore full = 0 ; Producer: do { /* produce an item in next_produced */ wait ( empty ); wait ( mutex ); /* add next_produced to the buffer */ signal ( mutex ); signal ( full ); } while ( true ); Consumer: do { wait ( full ); wait ( mutex ); /* remove an item from buffer to next_consumed */ signal ( mutex ); signal ( empty ); /* consume the item in next_consumed */ } while ( true );","title":"5.7.1 The Bounded-Buffer Problem"},{"location":"OS/Chap05/#572-the-readerswriters-problem","text":"The basic assumption: Readers: shared locks Writers: exclusive locks The first reader-writers problem No readers will be kept waiting unless a writer has already obtained permission to use the shared object $\\to$ potential hazard to writers! The second reader-writers problem One a writer is ready, it performs its write asap $\\to$ potential hazard to readers. semaphore rw_mutex = 1 ; semaphore mutex = 1 ; int read_count = 0 ; do { wait ( rw_mutex ); /* writing is performed */ signal ( rw_mutex ); } while ( true ); do { wait ( mutex ); // protect read_count read_count ++ ; if ( read_count == 1 ) wait ( rw_mutex ); signal ( mutex ); /* reading is performed */ wait ( mutex ); // protect read_count read_count -- ; if ( read_count == 0 ) signal ( rw_mutex ); signal ( mutex ); } while ( true );","title":"5.7.2 The Readers\u2013Writers Problem"},{"location":"OS/Chap05/#573-the-dining-philosophers-problem","text":"Each philosopher must pick up one chopstick beside him/her at a time. When two chopsticks are picked up, the philosopher can eat. semaphore chopstick [ 5 ]; do { wait ( chopstick [ i ]); wait ( chopstick [( i + 1 ) % 5 ]); /* eat for awhile */ signal ( chopstick [ i ]); signal ( chopstick [( i + 1 ) % 5 ]); /* think for awhile */ } while ( true );","title":"5.7.3 The Dining-Philosophers Problem"},{"location":"OS/Chap05/#critical-regions","text":"Region $v$ when $C$ (condition) do $S$ (statements) Variable $v$ \u2014 shared among processes and only accessible in the region. struct buffer { item pool [ n ]; int count , in , out ; }; Producer: region buffer when ( count < n ) { pool [ in ] = next_produced ; in = ( in + 1 ) % n ; count ++ ; } Consumer: region buffer when ( count > 0 ) { next_consumed = pool [ out ]; out = ( out + 1 ) % n ; count -- ; }","title":"Critical Regions"},{"location":"OS/Chap05/#58-monitors","text":"monitor monitor name { /* shared variable declarations */ function P1 ( . . . ) { . . . } function P2 ( . . . ) { . . . } . . . function Pn ( . . . ) { . . . } initialization_code ( . . . ) { . . . } }","title":"5.8 Monitors"},{"location":"OS/Chap05/#581-monitor-usage","text":"An abstract data type\u2014or ADT\u2014encapsulates data with a set of functions to operate on that data that are independent of any specific implementation of the ADT. The monitor construct ensures that only one process at a time is active within the monitor. A programmer who needs to write a tailor-made synchronization scheme can define one or more variables of type $condition$: condition x , y ; The only operations that can be invoked on a condition variable are wait() and signal() . The operation x . wait (); means that the process invoking this operation is suspended until another process invokes x . signal (); Condition variables (of a monitor) vs. signal operation (of binary semaphore) The x.signal() operation resumes exactly one suspended process. If no process is suspended, then the signal() operation has no effect; that is, the state of x is the same as if the operation had never been executed. Contrast this operation with the signal() operation associated with semaphores, which always affects the state of the semaphore. Suppose that, when the x.signal() operation is invoked by a process P , there exists a suspended process Q associated with condition x . Clearly, if the suspended process Q is allowed to resume its execution, the signaling process P must wait. Two possibilities exist: Signal and wait : P either waits until Q leaves the monitor or waits for another condition. Signal and continue : Q either waits until P leaves the monitor or waits for another condition.","title":"5.8.1 Monitor Usage"},{"location":"OS/Chap05/#582-dining-philosophers-solution-using-monitors","text":"enum { THINKING , HUNGRY , EATING } state [ 5 ]; condition self [ 5 ]; monitor DiningPhilosophers { enum { THINKING , HUNGRY , EATING } state [ 5 ]; condition self [ 5 ]; void pickup ( int i ) { state [ i ] = HUNGRY ; test ( i ); if ( state [ i ] != EATING ) self [ i ]. wait (); } void putdown ( int i ) { state [ i ] = THINKING ; test (( i + 4 ) % 5 ); // help your right-hand side to run test test (( i + 1 ) % 5 ); // help your left-hand side to run test } void test ( int i ) { if (( state [( i + 4 ) % 5 ] != EATING ) && // right-hand side ( state [ i ] == HUNGRY ) && ( state [( i + 1 ) % 5 ] != EATING )) { // left-hand side state [ i ] = EATING ; self [ i ]. signal (); } } initialization_code () { for ( int i = 0 ; i < 5 ; i ++ ) state [ i ] = THINKING ; } } $P_i$: DiningPhilosophers . pickup ( i ); /* eat */ DiningPhilosophers . putdown ( i ); No deadlock, but starvation could occur!","title":"5.8.2 Dining-Philosophers Solution Using Monitors"},{"location":"OS/Chap05/#583-implementing-a-monitor-using-semaphores","text":"Semaphores mutex : to protect the monitor next : being initialized to zero, on which processes may suspend themselves next_count Each external function F is replaced by wait ( mutex ); /* body of F */ if ( next_count > 0 ) signal ( next ); else signal ( mutex ); For each condition x , we introduce a semaphore x_sem and an integer variable x_count , both initialized to $0$. x.wait() x_count ++ ; if ( next_count > 0 ) signal ( next ); else signal ( mutex ); wait ( x_sem ); x_count -- ; x.signal() if ( x_count > 0 ) { /* If there's somebody being waiting */ next_count ++ ; signal ( x_sem ); wait ( next ); next_count -- ; }","title":"5.8.3 Implementing a Monitor Using Semaphores"},{"location":"OS/Chap05/#584-resuming-processes-within-a-monitor","text":"conditional-wait : x . wait ( c ); where $c$ is a priority number . When x.signal() is executed, the process with the smallest priority number is resumed next. Consider the ResourceAllocator monitor, which controls the allocation of a single resource among competing processes. monitor ResourceAllocator { boolean busy ; condition x ; void acquire ( int time ) { if ( busy ) x . wait ( time ); busy = true ; } void release () { busy = false ; x . signal (); } initialization_code () { busy = false ; } } A process that needs to access the resource in question must observe the following sequence: R . acquire ( t ); /* access the resource */ R . release (); The monitor concept cannot guarantee that the preceding access sequence will be observed. In particular, the following problems can occur: A process might access a resource without first gaining access permission to the resource. A process might never release a resource once it has been granted access to the resource. A process might attempt to release a resource that it never requested. A process might request the same resource twice (without first releasing the resource).","title":"5.8.4 Resuming Processes within a Monitor"},{"location":"OS/Chap05/#59-synchronization-examples","text":"","title":"5.9 Synchronization Examples"},{"location":"OS/Chap05/#591-synchronization-in-windows","text":"General Mechanism Spin-locking for short code segments in a multiprocessor platform. Interrupt disabling when the kernel accesses global variables in a uniprocessor platform. Dispatcher Object State: signaled or non-signaled Mutex: select one process from its waiting queue to the ready queue. Critical-section object \u2014 user-mode mutex Events: like condition variables Timers: select all waiting processes","title":"5.9.1 Synchronization in Windows"},{"location":"OS/Chap05/#592-synchronization-in-linux","text":"Preemptive kernel after version 2.6. Atomic integer atomic_t counter ; ... atomic_set ( & counter , 5 ); atomic_add ( 10 , & counter ); Semaphores for long code segments. Mutex locks for the kernel code. Sping-locking for short code segments in a multiprocessor platform. Preeption disabling and enabling in a uniprocessor platform. preempt_disable() and preempt_enable() preempt_count for each task in the system","title":"5.9.2 Synchronization in Linux"},{"location":"OS/Chap05/#593-synchronization-in-solaris","text":"Semaphores and condition variables Adaptive mutex spin-locking if the lock-holding thread is running; otherwise, blocking is used Readers-writers locks expensive in implementations Turnstile A queue structure containing threads blocked on a lock Priotity inversion $\\to$ priority inheritance protocol for kernel threads","title":"5.9.3 Synchronization in Solaris"},{"location":"OS/Chap05/#594-pthreads-synchronization","text":"General Mechanism Mutex locks: mutual exclusion condition variables: monitor Read-write locks","title":"5.9.4 Pthreads Synchronization"},{"location":"OS/Chap05/#510-alternative-approaches","text":"","title":"5.10 Alternative Approaches"},{"location":"OS/Chap05/#5101-transactional-memory","text":"Memory transaction A sequence of memory read-write operations that are atomic. Committed or being roleld back: void update () { atomic { /* modify shared data */ } } Advantages: No deadlock Identification of potentially concurrently executing statements in atomic blocks Implementations: Software transaction memory Code is inserted by the compiler . Hardware transactional memory Hardware cache hierarchies and cache coherency protocols are used.","title":"5.10.1 Transactional Memory"},{"location":"OS/Chap05/#5102-openmp","text":"A set of compiler directives and an API The critical-section compiler directive behaves like a binary semaphore or mutex. void update () { #pragma omp critical { counter += value ; } } pros: easy to use cons: identification of protected code and potentials of deadlocks","title":"5.10.2 OpenMP"},{"location":"OS/Chap05/#5103-functional-programming-languages","text":"","title":"5.10.3 Functional Programming Languages"},{"location":"OS/Chap06/","text":"Chapter 6 CPU Scheduling 6.1 Basic Concepts The objective of multiprogramming is to have some process running at all times, to maximize CPU utilization. A process is executed until it must wait, typically for the completion of some I/O request. 6.1.1 CPU\u2013I/O Burst Cycle Process execution = cycle of CPU + I/O wait. 6.1.2 CPU Scheduler Short-term scheduler (CPU scheduler): select process in the ready queue. 6.1.3 Preemptive Scheduling CPU-scheduling decisions when a process: (nonpreemptive) Switches from the running state $\\to$ the waiting state (e.g. I/O request, wait() for child) (preemptive) Switches from the running state $\\to$ the ready state (e.g. interrupt) (preemptive) Switches from the waiting state $\\to$ the ready state (e.g. completion of I/O) (nonpreemptive) Terminates 6.1.4 Dispatcher Dispatcher The module that gives control of the CPU to the process selected by the short-term scheduler . It should be fast. Switching context Switching to user mode Jumping to the proper location in the user program to restart that program Dispatch latency The time it takes for the dispatcher to stop one process and start another running. Stop a process $\\leftrightarrow$ Start a process 6.2 Scheduling Criteria CPU utilization Throughput Turnaround time : $\\text{Completion Time} - \\text{Start Time}$. Waiting time : The sum of the periods spent waiting in the ready queue. Response time 6.3 Scheduling Algorithms 6.3-1 First-Come, First-Served Scheduling The process which requests the CPU first is allocated the CPU. Properties: Nonpreemptive FCFS. CPU might be hold for an extended period. Critical problem: Convoy effect! Example: Given processes: Process Busrt Time $P_1$ 24 $P_2$ 3 $P_3$ 3 Consider order: $P_1 \\to P_2 \\to P_3$: Gantt chart: Average waiting time = (0 + 24 + 27) / 3 = 17 ms. Consider order: $P_2 \\to P_3 \\to P_1$: Gantt chart: Average waiting time = (0 + 3 + 6) / 3 = 9 ms. Convoy effect All the other processes wait for the one big process to get off the CPU. 6.3.2 Shortest-Job-First Scheduling Properties: Nonpreemptive SJF Shortest-next-CPU-burst first Problem: Measure the future! Example 1: Given processes: Process Burst Time $P_1$ 6 $P_2$ 8 $P_3$ 7 $P_4$ 3 By SJF scheduling: Gantt chart: Average waiting time = (3 + 16 + 9 + 0) / 4 = 7 ms. SJF is used frequently in long-term (job) scheduling, but it cannot be implemented at the level of short-term CPU scheduling. Exponential average Let $t_n$ be time of $n$th CPU burst, and $\\tau_{n + 1}$ be the next CPU burst. \\begin{align} \\tau_{n + 1} & = \\alpha t_n + (1 - \\alpha)\\tau_n, \\quad 0 \\le \\alpha \\le 1. \\\\ & = \\alpha t_n + (1 - \\alpha)\\alpha t_{n - 1} + \\cdots + (1 - \\alpha)^j \\alpha t_{n - j} + \\cdots + (1 - \\alpha)^{n + 1}\\tau_0. \\end{align} Example 2: Given processes: Process Arrival Time Burst Time $P_1$ 0 8 $P_2$ 1 4 $P_3$ 2 9 $P_4$ 3 5 By preemptive SJF scheduling: Gantt chart: Average waiting time = [(10 - 1) + (1 - 1) + (17 - 2) + (5 - 3)] / 4 = 26 / 4 = 6.5 ms. 6.3.3 Priority Scheduling Properties: CPU is assigned to the process with the highest priority \u2014 A framework for various scheduling algorithms: FCFS: Equal-priority with tie-breaking SJF: Priority = 1 / next CPU burst length Example: Given processes: Process Burst Time Priority $P_1$ 10 3 $P_2$ 1 1 $P_3$ 2 4 $P_4$ 1 5 $P_5$ 5 2 By preemptive SJF scheduling: Gantt chart: Average waiting time = (6 + 0 + 16 + 18 + 1) / 5 = 8.2 ms. Problem with priority scheduling indefinite blocking: low-priority processes could starve to death! starvation Aging Aging gradually increase the priority of processes that wait in the system for a long time. 6.3.4 Round-Robin Scheduling RR is similar to FCFS except that preemption is added to switch between processes. Goal: fairness, time sharing. Example: Given processes with quantum = 4ms: Process Burst Time $P_1$ 24 $P_2$ 3 $P_3$ 3 Gantt chart: Average waiting time = [(10 - 4) + 4 + 7] / 3 = 5.66 ms. Although the time quantum should be large compared with the context switch time, it should not be too large. 6.3.5 Multilevel Queue Scheduling Intra-queue scheduling Independent choice of scheduling algorithms Inter-queue scheduling Fixed-priority preemptive scheduling e.g., foreground queues always have absolute priority over the background queues. Time slice between queues e.g., 80% CPU is given to foreground processes, and 20% CPU is given to background processes. Each queue has absolute priority over lower-priority queues. If an interactive editing process entered the ready queue while a batch process was running, the batch process would be preempted. 6.3.6 Multilevel Feedback Queue Scheduling Multilevel feedback queue allows a process to move between queues. A multilevel feedback queue is defined by: #queues The scheduling algorithm for each queue The method used to determine when to upgrade a process to a higher priority queue demote a process to a lower priority queue The method used to determine which queue a process will enter when that process needs service 6.4 Thread Scheduling 6.4.1 Contention Scope Process Contention Scope (PCS) On systems implementing the many-to-one and many-to-many models, the thread library schedules user-level threads to run on an available LWP, since competition for the CPU takes place among threads belonging to the same process. It is important to note that PCS will typically preempt the thread currently running in favor of a higher-priority thread System Contention Scope (SCS) Competition for the CPU with SCS scheduling takes place among all threads in the system. Systems using the one-to-one model. 6.4.2 Pthread Scheduling PCS: PTHREAD_SCOPE_PROCESS SCS: PTHREAD_SCOPE_SYSTEM 2 methods: pthread_attr_setscope(pthread attr_t *attr, int scope) pthread_attr_getscope(pthread attr_t *attr, int *scope) 6.5 Multiple-Processor Scheduling 6.5.1 Approaches to Multiple-Processor Scheduling Asymmetric multiprocessing : only the master server process accesses the system data structures, reducing the need for data sharing. Symmetric multiprocessing (SMP) : each processor is self-scheduling. 6.5.2 Processor Affinity Processor Affinity A process has an affinity for the processor on which it is currently running. Soft affinity Hard affinity (e.g. Linux: sched_setaffinity() ) Non-Uniform Memory Access (NUMA) A CPU has faster access to some parts of main memory than to other parts. 6.5.3 Load Balancing On systems with a common run queue, load balancing is often unnecessary, because once a processor becomes idle, it immediately extracts a runnable process from the common run queue. Push migration : pushing processes from overloaded to less-busy processors. Pull migration : pulling a waiting task from a busy processor. 6.5.4 Multicore Processors SMP systems that use multicore processors are faster and consume less power than systems in which each processor has its own physical chip. There are two ways to multithread a processing core: Coarse-grained : a thread executes on a processor until a long-latency event such as a memory stall occurs. Fine-grained (interleaved) : switches between threads at a much finer level of granularity 6.6 Real-Time CPU Scheduling Soft real-time systems Hard real-time systems 6.6.1 Minimizing Latency Event latency The amount of time that elapses from when an event occurs to when it is serviced. There are 2 types: Interrupt latency Dispatch latency Interrupt latency The period of time from the arrival of an interrupt at the CPU to the start of the interrupt service routine (ISR) that services the interrupt. Dispatch latency The amount of time required for the scheduling dispatcher to stop one process and start another. 6.6.2 Priority-Based Scheduling The processes are considered periodic . That is, they require the CPU at constant intervals (periods). ($t$: fixed processing time, $d$: deadline, and $p$: period.) $$0 \\le t \\le d \\le p.$$ $$rate = 1 / p.$$ Admission-control : Admits the process. Rejects the request. 6.6.3 Rate-Monotonic Scheduling Example 1: Given processes: (the deadline for each process requires that it complete its CPU burst by the start of its next period.) Process Period ProcTime $P_1$ $p_1 = 50$ $t_1 = 20$ $P_2$ $p_2 = 100$ $t_2 = 35$ Example 2: Given processes: Process Period ProcTime $P_1$ $p_1 = 50$ $t_1 = 25$ $P_2$ $p_2 = 80$ $t_2 = 35$ 6.6.4 Earliest-Deadline-First Scheduling 6.6.5 Proportional Share Scheduling Proportional share schedulers operate by allocating $T$ shares among all applications. An application can receive $N$ shares of time. 6.6.6 POSIX Real-Time Scheduling POSIX.1b \u2014 Extensions for real-time computing SCHED_FIFO SCHED_RR SCHED_OTHER API pthread_attr_getsched_policy(pthread_attr_t *attr, int *policy) pthread_attr_setsched_policy(pthread_attr_t *attr, int *policy) 6.7 Operating-System Examples 6.7.1 Example: Linux Scheduling 6.7.2 Example: Windows Scheduling 6.7.3 Example: Solaris Scheduling 6.8 Algorithm Evaluation 6.8.1 Deterministic Modeling Deterministic modeling It is one type of analytic evaluation. This method takes a particular predetermined workload and defines the performance of each algorithm for that workload. 6.8.2 Queueing Models Little's formula ($n$: # of processes in the queue, $\\lambda$: arrival rate, $W$: average waiting time in the queue.) $$n = \\lambda \\times W.$$ 6.8.3 Simulations Properties: Accurate but expensive Procedures: Program a model of the computer system Drive the simulation with various data sets 6.8.4 Implementation","title":"Chapter 6 CPU Scheduling"},{"location":"OS/Chap06/#chapter-6-cpu-scheduling","text":"","title":"Chapter 6 CPU Scheduling"},{"location":"OS/Chap06/#61-basic-concepts","text":"The objective of multiprogramming is to have some process running at all times, to maximize CPU utilization. A process is executed until it must wait, typically for the completion of some I/O request.","title":"6.1 Basic Concepts"},{"location":"OS/Chap06/#611-cpuio-burst-cycle","text":"Process execution = cycle of CPU + I/O wait.","title":"6.1.1 CPU\u2013I/O Burst Cycle"},{"location":"OS/Chap06/#612-cpu-scheduler","text":"Short-term scheduler (CPU scheduler): select process in the ready queue.","title":"6.1.2 CPU Scheduler"},{"location":"OS/Chap06/#613-preemptive-scheduling","text":"CPU-scheduling decisions when a process: (nonpreemptive) Switches from the running state $\\to$ the waiting state (e.g. I/O request, wait() for child) (preemptive) Switches from the running state $\\to$ the ready state (e.g. interrupt) (preemptive) Switches from the waiting state $\\to$ the ready state (e.g. completion of I/O) (nonpreemptive) Terminates","title":"6.1.3 Preemptive Scheduling"},{"location":"OS/Chap06/#614-dispatcher","text":"Dispatcher The module that gives control of the CPU to the process selected by the short-term scheduler . It should be fast. Switching context Switching to user mode Jumping to the proper location in the user program to restart that program Dispatch latency The time it takes for the dispatcher to stop one process and start another running. Stop a process $\\leftrightarrow$ Start a process","title":"6.1.4 Dispatcher"},{"location":"OS/Chap06/#62-scheduling-criteria","text":"CPU utilization Throughput Turnaround time : $\\text{Completion Time} - \\text{Start Time}$. Waiting time : The sum of the periods spent waiting in the ready queue. Response time","title":"6.2 Scheduling Criteria"},{"location":"OS/Chap06/#63-scheduling-algorithms","text":"","title":"6.3 Scheduling Algorithms"},{"location":"OS/Chap06/#63-1-first-come-first-served-scheduling","text":"The process which requests the CPU first is allocated the CPU. Properties: Nonpreemptive FCFS. CPU might be hold for an extended period. Critical problem: Convoy effect! Example: Given processes: Process Busrt Time $P_1$ 24 $P_2$ 3 $P_3$ 3 Consider order: $P_1 \\to P_2 \\to P_3$: Gantt chart: Average waiting time = (0 + 24 + 27) / 3 = 17 ms. Consider order: $P_2 \\to P_3 \\to P_1$: Gantt chart: Average waiting time = (0 + 3 + 6) / 3 = 9 ms. Convoy effect All the other processes wait for the one big process to get off the CPU.","title":"6.3-1 First-Come, First-Served Scheduling"},{"location":"OS/Chap06/#632-shortest-job-first-scheduling","text":"Properties: Nonpreemptive SJF Shortest-next-CPU-burst first Problem: Measure the future! Example 1: Given processes: Process Burst Time $P_1$ 6 $P_2$ 8 $P_3$ 7 $P_4$ 3 By SJF scheduling: Gantt chart: Average waiting time = (3 + 16 + 9 + 0) / 4 = 7 ms. SJF is used frequently in long-term (job) scheduling, but it cannot be implemented at the level of short-term CPU scheduling. Exponential average Let $t_n$ be time of $n$th CPU burst, and $\\tau_{n + 1}$ be the next CPU burst. \\begin{align} \\tau_{n + 1} & = \\alpha t_n + (1 - \\alpha)\\tau_n, \\quad 0 \\le \\alpha \\le 1. \\\\ & = \\alpha t_n + (1 - \\alpha)\\alpha t_{n - 1} + \\cdots + (1 - \\alpha)^j \\alpha t_{n - j} + \\cdots + (1 - \\alpha)^{n + 1}\\tau_0. \\end{align} Example 2: Given processes: Process Arrival Time Burst Time $P_1$ 0 8 $P_2$ 1 4 $P_3$ 2 9 $P_4$ 3 5 By preemptive SJF scheduling: Gantt chart: Average waiting time = [(10 - 1) + (1 - 1) + (17 - 2) + (5 - 3)] / 4 = 26 / 4 = 6.5 ms.","title":"6.3.2 Shortest-Job-First Scheduling"},{"location":"OS/Chap06/#633-priority-scheduling","text":"Properties: CPU is assigned to the process with the highest priority \u2014 A framework for various scheduling algorithms: FCFS: Equal-priority with tie-breaking SJF: Priority = 1 / next CPU burst length Example: Given processes: Process Burst Time Priority $P_1$ 10 3 $P_2$ 1 1 $P_3$ 2 4 $P_4$ 1 5 $P_5$ 5 2 By preemptive SJF scheduling: Gantt chart: Average waiting time = (6 + 0 + 16 + 18 + 1) / 5 = 8.2 ms. Problem with priority scheduling indefinite blocking: low-priority processes could starve to death! starvation Aging Aging gradually increase the priority of processes that wait in the system for a long time.","title":"6.3.3 Priority Scheduling"},{"location":"OS/Chap06/#634-round-robin-scheduling","text":"RR is similar to FCFS except that preemption is added to switch between processes. Goal: fairness, time sharing. Example: Given processes with quantum = 4ms: Process Burst Time $P_1$ 24 $P_2$ 3 $P_3$ 3 Gantt chart: Average waiting time = [(10 - 4) + 4 + 7] / 3 = 5.66 ms. Although the time quantum should be large compared with the context switch time, it should not be too large.","title":"6.3.4 Round-Robin Scheduling"},{"location":"OS/Chap06/#635-multilevel-queue-scheduling","text":"Intra-queue scheduling Independent choice of scheduling algorithms Inter-queue scheduling Fixed-priority preemptive scheduling e.g., foreground queues always have absolute priority over the background queues. Time slice between queues e.g., 80% CPU is given to foreground processes, and 20% CPU is given to background processes. Each queue has absolute priority over lower-priority queues. If an interactive editing process entered the ready queue while a batch process was running, the batch process would be preempted.","title":"6.3.5 Multilevel Queue Scheduling"},{"location":"OS/Chap06/#636-multilevel-feedback-queue-scheduling","text":"Multilevel feedback queue allows a process to move between queues. A multilevel feedback queue is defined by: #queues The scheduling algorithm for each queue The method used to determine when to upgrade a process to a higher priority queue demote a process to a lower priority queue The method used to determine which queue a process will enter when that process needs service","title":"6.3.6 Multilevel Feedback Queue Scheduling"},{"location":"OS/Chap06/#64-thread-scheduling","text":"","title":"6.4 Thread Scheduling"},{"location":"OS/Chap06/#641-contention-scope","text":"Process Contention Scope (PCS) On systems implementing the many-to-one and many-to-many models, the thread library schedules user-level threads to run on an available LWP, since competition for the CPU takes place among threads belonging to the same process. It is important to note that PCS will typically preempt the thread currently running in favor of a higher-priority thread System Contention Scope (SCS) Competition for the CPU with SCS scheduling takes place among all threads in the system. Systems using the one-to-one model.","title":"6.4.1 Contention Scope"},{"location":"OS/Chap06/#642-pthread-scheduling","text":"PCS: PTHREAD_SCOPE_PROCESS SCS: PTHREAD_SCOPE_SYSTEM 2 methods: pthread_attr_setscope(pthread attr_t *attr, int scope) pthread_attr_getscope(pthread attr_t *attr, int *scope)","title":"6.4.2 Pthread Scheduling"},{"location":"OS/Chap06/#65-multiple-processor-scheduling","text":"","title":"6.5 Multiple-Processor Scheduling"},{"location":"OS/Chap06/#651-approaches-to-multiple-processor-scheduling","text":"Asymmetric multiprocessing : only the master server process accesses the system data structures, reducing the need for data sharing. Symmetric multiprocessing (SMP) : each processor is self-scheduling.","title":"6.5.1 Approaches to Multiple-Processor Scheduling"},{"location":"OS/Chap06/#652-processor-affinity","text":"Processor Affinity A process has an affinity for the processor on which it is currently running. Soft affinity Hard affinity (e.g. Linux: sched_setaffinity() ) Non-Uniform Memory Access (NUMA) A CPU has faster access to some parts of main memory than to other parts.","title":"6.5.2 Processor Affinity"},{"location":"OS/Chap06/#653-load-balancing","text":"On systems with a common run queue, load balancing is often unnecessary, because once a processor becomes idle, it immediately extracts a runnable process from the common run queue. Push migration : pushing processes from overloaded to less-busy processors. Pull migration : pulling a waiting task from a busy processor.","title":"6.5.3 Load Balancing"},{"location":"OS/Chap06/#654-multicore-processors","text":"SMP systems that use multicore processors are faster and consume less power than systems in which each processor has its own physical chip. There are two ways to multithread a processing core: Coarse-grained : a thread executes on a processor until a long-latency event such as a memory stall occurs. Fine-grained (interleaved) : switches between threads at a much finer level of granularity","title":"6.5.4 Multicore Processors"},{"location":"OS/Chap06/#66-real-time-cpu-scheduling","text":"Soft real-time systems Hard real-time systems","title":"6.6 Real-Time CPU Scheduling"},{"location":"OS/Chap06/#661-minimizing-latency","text":"Event latency The amount of time that elapses from when an event occurs to when it is serviced. There are 2 types: Interrupt latency Dispatch latency Interrupt latency The period of time from the arrival of an interrupt at the CPU to the start of the interrupt service routine (ISR) that services the interrupt. Dispatch latency The amount of time required for the scheduling dispatcher to stop one process and start another.","title":"6.6.1 Minimizing Latency"},{"location":"OS/Chap06/#662-priority-based-scheduling","text":"The processes are considered periodic . That is, they require the CPU at constant intervals (periods). ($t$: fixed processing time, $d$: deadline, and $p$: period.) $$0 \\le t \\le d \\le p.$$ $$rate = 1 / p.$$ Admission-control : Admits the process. Rejects the request.","title":"6.6.2 Priority-Based Scheduling"},{"location":"OS/Chap06/#663-rate-monotonic-scheduling","text":"Example 1: Given processes: (the deadline for each process requires that it complete its CPU burst by the start of its next period.) Process Period ProcTime $P_1$ $p_1 = 50$ $t_1 = 20$ $P_2$ $p_2 = 100$ $t_2 = 35$ Example 2: Given processes: Process Period ProcTime $P_1$ $p_1 = 50$ $t_1 = 25$ $P_2$ $p_2 = 80$ $t_2 = 35$","title":"6.6.3 Rate-Monotonic Scheduling"},{"location":"OS/Chap06/#664-earliest-deadline-first-scheduling","text":"","title":"6.6.4 Earliest-Deadline-First Scheduling"},{"location":"OS/Chap06/#665-proportional-share-scheduling","text":"Proportional share schedulers operate by allocating $T$ shares among all applications. An application can receive $N$ shares of time.","title":"6.6.5 Proportional Share Scheduling"},{"location":"OS/Chap06/#666-posix-real-time-scheduling","text":"POSIX.1b \u2014 Extensions for real-time computing SCHED_FIFO SCHED_RR SCHED_OTHER API pthread_attr_getsched_policy(pthread_attr_t *attr, int *policy) pthread_attr_setsched_policy(pthread_attr_t *attr, int *policy)","title":"6.6.6 POSIX Real-Time Scheduling"},{"location":"OS/Chap06/#67-operating-system-examples","text":"","title":"6.7 Operating-System Examples"},{"location":"OS/Chap06/#671-example-linux-scheduling","text":"","title":"6.7.1 Example: Linux Scheduling"},{"location":"OS/Chap06/#672-example-windows-scheduling","text":"","title":"6.7.2 Example: Windows Scheduling"},{"location":"OS/Chap06/#673-example-solaris-scheduling","text":"","title":"6.7.3 Example: Solaris Scheduling"},{"location":"OS/Chap06/#68-algorithm-evaluation","text":"","title":"6.8 Algorithm Evaluation"},{"location":"OS/Chap06/#681-deterministic-modeling","text":"Deterministic modeling It is one type of analytic evaluation. This method takes a particular predetermined workload and defines the performance of each algorithm for that workload.","title":"6.8.1 Deterministic Modeling"},{"location":"OS/Chap06/#682-queueing-models","text":"Little's formula ($n$: # of processes in the queue, $\\lambda$: arrival rate, $W$: average waiting time in the queue.) $$n = \\lambda \\times W.$$","title":"6.8.2 Queueing Models"},{"location":"OS/Chap06/#683-simulations","text":"Properties: Accurate but expensive Procedures: Program a model of the computer system Drive the simulation with various data sets","title":"6.8.3 Simulations"},{"location":"OS/Chap06/#684-implementation","text":"","title":"6.8.4 Implementation"},{"location":"OS/Chap07/","text":"Chapter 7 Deadlocks 7.1 System Model 7.2 Deadlock Characterization 7.2.1 Necessary Conditions 7.2.2 Resource-Allocation Graph","title":"Chapter 7 Deadlocks"},{"location":"OS/Chap07/#chapter-7-deadlocks","text":"","title":"Chapter 7 Deadlocks"},{"location":"OS/Chap07/#71-system-model","text":"","title":"7.1 System Model"},{"location":"OS/Chap07/#72-deadlock-characterization","text":"","title":"7.2 Deadlock Characterization"},{"location":"OS/Chap07/#721-necessary-conditions","text":"","title":"7.2.1 Necessary Conditions"},{"location":"OS/Chap07/#722-resource-allocation-graph","text":"","title":"7.2.2 Resource-Allocation Graph"},{"location":"OS/Chap08/","text":"Chapter 8 Main Memory 8.1 Background The CPU fetches instructions from memory according to the value of the program counter. Memory Management Motivation: Keep several processes in memory to improve a system's performance 8.1.1 Basic Hardware Main memory and the registers built into the processor itself are the only general-purpose storage that the CPU can access directly. We need to make sure that each process has a separate memory space. We can provide this protection by using two registers, usually a base and a limit . 8.1.2 Address Binding The binding of instructions and data to memory addresses can be done at any step along the way: Compile time : absolute code. Load time : relocatable code. Execution time A major portion of this chapter is devoted to showing how these various bindings can be implemented effectively in a computer system and to discussing appropriate hardware support. 8.1.3 Logical Versus Physical Address Space Logical address An address generated by the CPU. Physical address An address seen by the memory unit and loaded into the memory-address register. Generating identical logical and physical addresses: Compile-time address-binding Load-time address-binding Generating different logical and physical addresses: Execution-time address-binding logical address $\\to$ virtual address Logical address space The set of all logical addresses generated by a program. Physical address space The set of all physical addresses corresponding to these logical addresses. !!! note Memory-management unit (MMU) Run-time map from virtual to physical addresses. The user program never sees the real physical addresses and deals with logical addresses. The memory-mapping hardware converts logical addresses into physical addresses 8.1.4 Dynamic Loading Dynamic loading A routine is not loaded until it is called. 8.1.5 Dynamic Linking and Shared Libraries Dynamically linked libraries System libraries that are linked to user programs when the programs are run. Static linking System libraries are treated like any other object module and are combined by the loader into the binary program image. Dynamic linking Linking is postponed until execution time. e.g. language subroutine libraries. With dynamic linking, a stub is included in the image for each library-routine reference. Stub A small piece of code that indicates how to locate the appropriate memory-resident library routine or how to load the library if the routine is not already present. Shared libraries Only programs that are compiled with the new library version are affected by any incompatible changes incorporated in it. Other programs linked before the new library was installed will continue using the older library. 8.2 Swapping A process must be in memory to be executed. A process, however, can be swapped temporarily out of memory to a backing store and then brought back into memory for continued execution. 8.2.1 Standard Swapping The context-switch time in such a swapping system is fairly high. Let's assume that the user process is 100 MB in size and the backing store is a standard hard disk with a transfer rate of 50 MB per second. The actual transfer of the 100-MB process to or from main memory takes $$\\frac{100\\text{MB}}{50\\text{MB}/s} = 2s.$$ 8.2.2 Swapping on Mobile Systems Mobile systems do not support swapping in general but might have paging (iOS and Android) 8.3 Contiguous Memory Allocation The memory is usually divided into two partitions: one for the resident operating system and one for the user processes. Contiguous memory allocation Each process is contained in a single section of memory that is contiguous to the section containing the next process. 8.3.1 Memory Protection The relocation register contains the value of the smallest physical address . The limit register contains the range of logical addresses . 8.3.2 Memory Allocation One of the simplest methods for allocating memory is to divide memory into several fixed-sized partitions . Thus, the degree of multiprogramming is bound by the number of partitions. In this multiple-partition method , when a partition is free, a process is selected from the input queue and is loaded into the free partition. In the variable-partition scheme, the operating system keeps a table indicating which parts of memory are available and which are occupied. Dynamic storage-allocation problem concerns how to satisfy a request of size $n$ from a li\bst of free holes. There are many solutions to this problem: First fit Best fit Worst fit Simulations have shown that both first fit and best fit are better than worst fit in terms of decreasing time and storage utilization. Neither first fit nor best fit is clearly better than the other in terms of storage utilization, but first fit is generally faster. 8.3.3 Fragmentation Both the first-fit and best-fit strategies for memory allocation suffer from external fragmentation . External fragmentation There is enough total memory space to satisfy a request but the available spaces are not contiguous 50-percent rule Even with some optimization, given $N$ allocated blocks, another $0.5 N$ blocks will be lost to fragmentation. That is, one-third of memory may be unusable! Internal fragmentation The memory allocated to a process may be slightly larger than the requested memory. The difference between these two numbers is internal fragmentation. Compaction The goal is to shuffle the memory contents so as to place all free memory together in one large block. Compaction is possible only if relocation is dynamic and is done at execution time. Another possible solution to the external-fragmentation problem is to permit the logical address space of the processes to be noncontiguous, thus allowing a process to be allocated physical memory wherever such memory is available: Segmentation Paging 8.4 Segmentation Segmentation A memory-management scheme that supports this programmer view of memory. 8.4.1 Basic Method segments are numbered and are referred to by a segment number, rather than by a segment name. Thus, a logical address consists of a two tuple : < segment-number, offset > 8.4.2 Segmentation Hardware A logical address consists of two parts: a segment number: $s$ an offset into that segment: $d$ Segment table consists of a segment base: contains the starting physical address where the segment resides in memory. a segment limit: specifies the length of the segment. 8.5 Paging Paging avoids external fragmentation and the need for compaction, whereas segmentation does not. breaking physical memory into fixed-sized blocks called frames breaking logical memory into blocks of the same size called pages Every address generated by the CPU is divided into two parts: a page number (p) a page offset (d) If the size of the logical address space is $2^m$, and a page size is $2^n$ bytes. The logical address is as follows: On my MacBook Pro (15-inch, 2017), I obtain the page size of 4096 bytes = 4 KB. In the worst case, a process would need $n$ pages plus $1$ byte. It would be allocated $n + 1$ frames, resulting in internal fragmentation of almost an entire frame. Frame table It has one entry for each physical page frame, indicating whether the latter is free or allocated and, if it is allocated, to which page of which process or processes. 8.5.2 Hardware Support The page table is implemented as a set of dedicated registers . pros: fast cons: entries could be small The page table is kept in main memory, and a page-table base register ( PTBR ) points to the page table. Changing page tables requires changing only this one register, substantially reducing context-switch time. pros: entries could be large cons: slow (the time required to access a user memory location) With this scheme, two memory accesses are needed to access a byte (one for the page-table entry, one for the byte) Translation look-aside buffer (TLB) The TLB is associative, high-speed memory. Each entry in the TLB consists of two parts: a key (or tag) a value The TLB contains only a few of the page-table entries (typically between 32 and 1024 entries). When a logical address is generated by the CPU, its page number is presented to the TLB. If the page number is found, its frame number is immediately available and is used to access memory. As just mentioned, these steps are executed as part of the instruction pipeline within the CPU, adding no performance penalty compared with a system that does not implement paging. TLB miss The page number is not in the TLB. TLB replacement policies If the TLB is already full of entries, an existing entry must be selected for replacement. Replacement policies range from least recently used (LRU) through round-robin to random. Wire down Some TLBs allow certain entries to be wired down , meaning that they cannot be removed from the TLB. Typically, TLB entries for key kernel code are wired down. Some TLBs store address-space identifiers ( ASIDs ) in each TLB entry. ASID An ASID uniquely identi\ufb01es each process and is used to provide address-space protection for that process. Hit ratio The percentage of times that the page number of interest is found in the TLB. 8.5.3 Protection We can create hardware to provide read-only, read-write, or execute-only protection. Valid\u2013invalid bit When this bit is set to valid, the associated page is in the process's logical address space and is thus a legal page. Page-table length register (PTLR) To indicate the size of the page table. 8.5.4 Shared Pages To be sharable, the code must be reentrant. The read-only nature of shared code should not be left to the correctness of the code; the operating system should enforce this property. 8.6 Structure of the Page Table 8.6.1 Hierarchical Paging Hierarchical paging is also known as forward-mapped page table . The VAX minicomputer from Digital Equipment Corporation (DEC): 8.6.2 Hashed Page Tables Clustered page tables Each entry in the hash table refers to several pages (such as 16) rather than a single page, which are particularly useful for address spaces. 8.6.3 Inverted Page Tables Inverted page table An inverted page table has one entry for each real page (or frame) of memory. Each entry consists of the virtual address of the page stored in that real memory location, with information about the process that owns the page. For the IBM RT , each virtual address in the system consists of a triple: < process-id, page-number, offset > Shared memory & Inverted page tables Shared memory cannot be used with inverted page tables; because there is only one virtual page entry for every physical page, one physical page cannot have two (or more) shared virtual addresses. 8.6.4 Oracle SPARC Solaris TLB walk If a match is found in the TSB, the CPU copies the TSB entry into the TLB, and the memory translation completes. If no match is found in the TSB, the kernel is interrupted to search the hash table. 8.7 Example: Intel 32 and 64-bit Architectures 8.7.1 IA-32 Architecture segmentation unit + paging unit = memory-management unit (MMU) 8.7.1.1 IA-32 Segmentation The logical address is a pair (selector, offset), where the selector is a 16-bit number: $s$: the segment number $g$: GDT or LDT $p$: protection. 8.7.1.2 IA-32 Paging Page address extension (PAE) It allows 32-bit processors to access a physical address space larger than 4 GB. 8.7.2 x86-64 8.8 Example: ARM Architecture 8.9 Summary Hardware support Performance Fragmentation Relocation Swapping Sharing Protection","title":"Chapter 8 Main Memory"},{"location":"OS/Chap08/#chapter-8-main-memory","text":"","title":"Chapter 8 Main Memory"},{"location":"OS/Chap08/#81-background","text":"The CPU fetches instructions from memory according to the value of the program counter. Memory Management Motivation: Keep several processes in memory to improve a system's performance","title":"8.1 Background"},{"location":"OS/Chap08/#811-basic-hardware","text":"Main memory and the registers built into the processor itself are the only general-purpose storage that the CPU can access directly. We need to make sure that each process has a separate memory space. We can provide this protection by using two registers, usually a base and a limit .","title":"8.1.1 Basic Hardware"},{"location":"OS/Chap08/#812-address-binding","text":"The binding of instructions and data to memory addresses can be done at any step along the way: Compile time : absolute code. Load time : relocatable code. Execution time A major portion of this chapter is devoted to showing how these various bindings can be implemented effectively in a computer system and to discussing appropriate hardware support.","title":"8.1.2 Address Binding"},{"location":"OS/Chap08/#813-logical-versus-physical-address-space","text":"Logical address An address generated by the CPU. Physical address An address seen by the memory unit and loaded into the memory-address register. Generating identical logical and physical addresses: Compile-time address-binding Load-time address-binding Generating different logical and physical addresses: Execution-time address-binding logical address $\\to$ virtual address Logical address space The set of all logical addresses generated by a program. Physical address space The set of all physical addresses corresponding to these logical addresses. !!! note Memory-management unit (MMU) Run-time map from virtual to physical addresses. The user program never sees the real physical addresses and deals with logical addresses. The memory-mapping hardware converts logical addresses into physical addresses","title":"8.1.3 Logical Versus Physical Address Space"},{"location":"OS/Chap08/#814-dynamic-loading","text":"Dynamic loading A routine is not loaded until it is called.","title":"8.1.4 Dynamic Loading"},{"location":"OS/Chap08/#815-dynamic-linking-and-shared-libraries","text":"Dynamically linked libraries System libraries that are linked to user programs when the programs are run. Static linking System libraries are treated like any other object module and are combined by the loader into the binary program image. Dynamic linking Linking is postponed until execution time. e.g. language subroutine libraries. With dynamic linking, a stub is included in the image for each library-routine reference. Stub A small piece of code that indicates how to locate the appropriate memory-resident library routine or how to load the library if the routine is not already present. Shared libraries Only programs that are compiled with the new library version are affected by any incompatible changes incorporated in it. Other programs linked before the new library was installed will continue using the older library.","title":"8.1.5 Dynamic Linking and Shared Libraries"},{"location":"OS/Chap08/#82-swapping","text":"A process must be in memory to be executed. A process, however, can be swapped temporarily out of memory to a backing store and then brought back into memory for continued execution.","title":"8.2 Swapping"},{"location":"OS/Chap08/#821-standard-swapping","text":"The context-switch time in such a swapping system is fairly high. Let's assume that the user process is 100 MB in size and the backing store is a standard hard disk with a transfer rate of 50 MB per second. The actual transfer of the 100-MB process to or from main memory takes $$\\frac{100\\text{MB}}{50\\text{MB}/s} = 2s.$$","title":"8.2.1 Standard Swapping"},{"location":"OS/Chap08/#822-swapping-on-mobile-systems","text":"Mobile systems do not support swapping in general but might have paging (iOS and Android)","title":"8.2.2 Swapping on Mobile Systems"},{"location":"OS/Chap08/#83-contiguous-memory-allocation","text":"The memory is usually divided into two partitions: one for the resident operating system and one for the user processes. Contiguous memory allocation Each process is contained in a single section of memory that is contiguous to the section containing the next process.","title":"8.3 Contiguous Memory Allocation"},{"location":"OS/Chap08/#831-memory-protection","text":"The relocation register contains the value of the smallest physical address . The limit register contains the range of logical addresses .","title":"8.3.1 Memory Protection"},{"location":"OS/Chap08/#832-memory-allocation","text":"One of the simplest methods for allocating memory is to divide memory into several fixed-sized partitions . Thus, the degree of multiprogramming is bound by the number of partitions. In this multiple-partition method , when a partition is free, a process is selected from the input queue and is loaded into the free partition. In the variable-partition scheme, the operating system keeps a table indicating which parts of memory are available and which are occupied. Dynamic storage-allocation problem concerns how to satisfy a request of size $n$ from a li\bst of free holes. There are many solutions to this problem: First fit Best fit Worst fit Simulations have shown that both first fit and best fit are better than worst fit in terms of decreasing time and storage utilization. Neither first fit nor best fit is clearly better than the other in terms of storage utilization, but first fit is generally faster.","title":"8.3.2 Memory Allocation"},{"location":"OS/Chap08/#833-fragmentation","text":"Both the first-fit and best-fit strategies for memory allocation suffer from external fragmentation . External fragmentation There is enough total memory space to satisfy a request but the available spaces are not contiguous 50-percent rule Even with some optimization, given $N$ allocated blocks, another $0.5 N$ blocks will be lost to fragmentation. That is, one-third of memory may be unusable! Internal fragmentation The memory allocated to a process may be slightly larger than the requested memory. The difference between these two numbers is internal fragmentation. Compaction The goal is to shuffle the memory contents so as to place all free memory together in one large block. Compaction is possible only if relocation is dynamic and is done at execution time. Another possible solution to the external-fragmentation problem is to permit the logical address space of the processes to be noncontiguous, thus allowing a process to be allocated physical memory wherever such memory is available: Segmentation Paging","title":"8.3.3 Fragmentation"},{"location":"OS/Chap08/#84-segmentation","text":"Segmentation A memory-management scheme that supports this programmer view of memory.","title":"8.4 Segmentation"},{"location":"OS/Chap08/#841-basic-method","text":"segments are numbered and are referred to by a segment number, rather than by a segment name. Thus, a logical address consists of a two tuple : < segment-number, offset >","title":"8.4.1 Basic Method"},{"location":"OS/Chap08/#842-segmentation-hardware","text":"A logical address consists of two parts: a segment number: $s$ an offset into that segment: $d$ Segment table consists of a segment base: contains the starting physical address where the segment resides in memory. a segment limit: specifies the length of the segment.","title":"8.4.2 Segmentation Hardware"},{"location":"OS/Chap08/#85-paging","text":"Paging avoids external fragmentation and the need for compaction, whereas segmentation does not. breaking physical memory into fixed-sized blocks called frames breaking logical memory into blocks of the same size called pages Every address generated by the CPU is divided into two parts: a page number (p) a page offset (d) If the size of the logical address space is $2^m$, and a page size is $2^n$ bytes. The logical address is as follows: On my MacBook Pro (15-inch, 2017), I obtain the page size of 4096 bytes = 4 KB. In the worst case, a process would need $n$ pages plus $1$ byte. It would be allocated $n + 1$ frames, resulting in internal fragmentation of almost an entire frame. Frame table It has one entry for each physical page frame, indicating whether the latter is free or allocated and, if it is allocated, to which page of which process or processes.","title":"8.5 Paging"},{"location":"OS/Chap08/#852-hardware-support","text":"The page table is implemented as a set of dedicated registers . pros: fast cons: entries could be small The page table is kept in main memory, and a page-table base register ( PTBR ) points to the page table. Changing page tables requires changing only this one register, substantially reducing context-switch time. pros: entries could be large cons: slow (the time required to access a user memory location) With this scheme, two memory accesses are needed to access a byte (one for the page-table entry, one for the byte) Translation look-aside buffer (TLB) The TLB is associative, high-speed memory. Each entry in the TLB consists of two parts: a key (or tag) a value The TLB contains only a few of the page-table entries (typically between 32 and 1024 entries). When a logical address is generated by the CPU, its page number is presented to the TLB. If the page number is found, its frame number is immediately available and is used to access memory. As just mentioned, these steps are executed as part of the instruction pipeline within the CPU, adding no performance penalty compared with a system that does not implement paging. TLB miss The page number is not in the TLB. TLB replacement policies If the TLB is already full of entries, an existing entry must be selected for replacement. Replacement policies range from least recently used (LRU) through round-robin to random. Wire down Some TLBs allow certain entries to be wired down , meaning that they cannot be removed from the TLB. Typically, TLB entries for key kernel code are wired down. Some TLBs store address-space identifiers ( ASIDs ) in each TLB entry. ASID An ASID uniquely identi\ufb01es each process and is used to provide address-space protection for that process. Hit ratio The percentage of times that the page number of interest is found in the TLB.","title":"8.5.2 Hardware Support"},{"location":"OS/Chap08/#853-protection","text":"We can create hardware to provide read-only, read-write, or execute-only protection. Valid\u2013invalid bit When this bit is set to valid, the associated page is in the process's logical address space and is thus a legal page. Page-table length register (PTLR) To indicate the size of the page table.","title":"8.5.3 Protection"},{"location":"OS/Chap08/#854-shared-pages","text":"To be sharable, the code must be reentrant. The read-only nature of shared code should not be left to the correctness of the code; the operating system should enforce this property.","title":"8.5.4 Shared Pages"},{"location":"OS/Chap08/#86-structure-of-the-page-table","text":"","title":"8.6 Structure of the Page Table"},{"location":"OS/Chap08/#861-hierarchical-paging","text":"Hierarchical paging is also known as forward-mapped page table . The VAX minicomputer from Digital Equipment Corporation (DEC):","title":"8.6.1 Hierarchical Paging"},{"location":"OS/Chap08/#862-hashed-page-tables","text":"Clustered page tables Each entry in the hash table refers to several pages (such as 16) rather than a single page, which are particularly useful for address spaces.","title":"8.6.2 Hashed Page Tables"},{"location":"OS/Chap08/#863-inverted-page-tables","text":"Inverted page table An inverted page table has one entry for each real page (or frame) of memory. Each entry consists of the virtual address of the page stored in that real memory location, with information about the process that owns the page. For the IBM RT , each virtual address in the system consists of a triple: < process-id, page-number, offset > Shared memory & Inverted page tables Shared memory cannot be used with inverted page tables; because there is only one virtual page entry for every physical page, one physical page cannot have two (or more) shared virtual addresses.","title":"8.6.3 Inverted Page Tables"},{"location":"OS/Chap08/#864-oracle-sparc-solaris","text":"TLB walk If a match is found in the TSB, the CPU copies the TSB entry into the TLB, and the memory translation completes. If no match is found in the TSB, the kernel is interrupted to search the hash table.","title":"8.6.4 Oracle SPARC Solaris"},{"location":"OS/Chap08/#87-example-intel-32-and-64-bit-architectures","text":"","title":"8.7 Example: Intel 32 and 64-bit Architectures"},{"location":"OS/Chap08/#871-ia-32-architecture","text":"segmentation unit + paging unit = memory-management unit (MMU)","title":"8.7.1 IA-32 Architecture"},{"location":"OS/Chap08/#8711-ia-32-segmentation","text":"The logical address is a pair (selector, offset), where the selector is a 16-bit number: $s$: the segment number $g$: GDT or LDT $p$: protection.","title":"8.7.1.1 IA-32 Segmentation"},{"location":"OS/Chap08/#8712-ia-32-paging","text":"Page address extension (PAE) It allows 32-bit processors to access a physical address space larger than 4 GB.","title":"8.7.1.2 IA-32 Paging"},{"location":"OS/Chap08/#872-x86-64","text":"","title":"8.7.2 x86-64"},{"location":"OS/Chap08/#88-example-arm-architecture","text":"","title":"8.8 Example: ARM Architecture"},{"location":"OS/Chap08/#89-summary","text":"Hardware support Performance Fragmentation Relocation Swapping Sharing Protection","title":"8.9 Summary"},{"location":"OS/Chap09/","text":"Chapter 9 Virtual Memory 9.1 Background For instance, the routines on U.S. government computers that balance the budget have not been used in many years. Virtual memory It separates the logical memory perceived by users from physical memory. Virtual address space Logical (virtual) view of how a process is stored in memory. Sparse address spaces Virtual address spaces that include holes. 9.2 Demand Paging Demand paging A strategy to load pages only as they are needed. Lazy swapper A lazy swapper never swaps a page into memory unless that page will be needed. swapper: entire processes pager : individual pages of a process 9.2.1 Basic Concepts If the valid bit is set to \"invalid\", there are 2 possibilities: the page is invalid the page is valid but is currently on the dist. Pure demand paging Never bring a page into memory until it is required. Secondary memory This memory holds those pages that are not present in main memory. 9.2.2 Performance of Demand Paging For most computer systems, the memory-access time, denoted $ma$, ranges from 10 to 200 nanoseconds. Let $p$ be the probability of a page fault ($0 \\le p \\le 1$). We would expect $p$ to be close to zero. $$\\text{effective access time} = (1 - p) \\times ma + p \\times \\text{page fault time}.$$ 9.3 Copy-on-Write Many operating systems provide a pool of free pages for such requests. Zero-fill-on-demand Zero-fill-on-demand pages have been zeroed-out before being allocated, thus erasing the previous contents. vfork() (virtual memory fork) It operates differently from fork() with copy-on-write. With vfork() , the parent process is suspended, and the child process uses the address space of the parent. 9.4 Page Replacement 9.4.1 Basic Page Replacement Find the location of the desired page on the disk. Find a free frame: a. If there is a free frame, use it. b. If there is no free frame, use a page-replacement algorithm to select a victim frame . c. Write the victim frame to the disk; change the page and frame tables accordingly. Read the desired page into the newly freed frame; change the page and frame tables. Continue the user process from where the page fault occurred. Modify bit (dirty bit) The modify bit for a page is set by the hardware whenever any byte in the page is written into. When we select a page for replacement. If the modify bit is set, we must write the page to the disk. If the modify bit is not set, we need not write the page to the disk. We must solve two major problems to implement demand paging: frame-allocation algorithm: decide how many frames to allocate to each process. page-replacement algorithm: when page replacement is required. Reference string The string of memory references. 9.4.2 FIFO Page Replacement Belady's anomaly For some page-replacement algorithms, the page-fault rate may increase as the number of allocated frames increases. 9.4.3 Optimal Page Replacement Optimal page-replacement algorithm Replace the page that will not be used for the longest period of time, which will never suffer from Belady's anomaly. 9.4.4 LRU Page Replacement The LRU policy is often used as a page-replacement algorithm and is considered to be good. The major problem is how to implement LRU replacement. Counter cons: overflow Stack 9.4.5 LRU-Approximation Page Replacement 9.4.5.1 Additional-Reference-Bits Algorithm The operating system shifts the reference bit for each page into the high-order bit of its 8-bit byte, shifting the other bits right by 1 bit and discarding the low-order bit. These 8-bit shift registers contain the history of page use for the last eight time periods. e.g. A page with a history register value of 11000100 has been used more recently than one with a value of 01110111. 9.4.5.2 Second-Chance Algorithm In the worst case, when all bits are set, the pointer cycles through the whole queue, giving each page a second chance. It clears all the reference bits before selecting the next page for replacement. ($\\to$ FIFO) 9.4.5.3 Enhanced Second-Chance Algorithm ordered pair = (reference bit, modify bit) (0, 0) neither recently used nor modified\u2014best page to replace (0, 1) not recently used but modified\u2014not quite as good, because the page will need to be written out before replacement (1, 0) recently used but clean\u2014probably will be used again soon (1, 1) recently used and modified\u2014probably will be used again soon, and the page will be need to be written out to disk before it can be replaced 9.4.6 Counting-Based Page Replacement least frequently used (LFU) most frequently used (MFU) 9.4.7 Page-Buffering Algorithms Systems keep a pool of free frames The desired page is read into a free frame from the pool before the victime is written out. When the victim is later written out, its frame is added to the free-frame pool. There are two variations: To maintain a list of modified pages. Whenever the paging device is idle, a modified page is written to the dist and its modify bit is reset. To keep a pool of free frames but to remember which page was in each frame. No I/O is needed in this case $\\to$ \"Swapped-in\" time is saved! 9.4.8 Applications and Page Replacement","title":"Chapter 9 Virtual Memory"},{"location":"OS/Chap09/#chapter-9-virtual-memory","text":"","title":"Chapter 9 Virtual Memory"},{"location":"OS/Chap09/#91-background","text":"For instance, the routines on U.S. government computers that balance the budget have not been used in many years. Virtual memory It separates the logical memory perceived by users from physical memory. Virtual address space Logical (virtual) view of how a process is stored in memory. Sparse address spaces Virtual address spaces that include holes.","title":"9.1 Background"},{"location":"OS/Chap09/#92-demand-paging","text":"Demand paging A strategy to load pages only as they are needed. Lazy swapper A lazy swapper never swaps a page into memory unless that page will be needed. swapper: entire processes pager : individual pages of a process","title":"9.2 Demand Paging"},{"location":"OS/Chap09/#921-basic-concepts","text":"If the valid bit is set to \"invalid\", there are 2 possibilities: the page is invalid the page is valid but is currently on the dist. Pure demand paging Never bring a page into memory until it is required. Secondary memory This memory holds those pages that are not present in main memory.","title":"9.2.1 Basic Concepts"},{"location":"OS/Chap09/#922-performance-of-demand-paging","text":"For most computer systems, the memory-access time, denoted $ma$, ranges from 10 to 200 nanoseconds. Let $p$ be the probability of a page fault ($0 \\le p \\le 1$). We would expect $p$ to be close to zero. $$\\text{effective access time} = (1 - p) \\times ma + p \\times \\text{page fault time}.$$","title":"9.2.2 Performance of Demand Paging"},{"location":"OS/Chap09/#93-copy-on-write","text":"Many operating systems provide a pool of free pages for such requests. Zero-fill-on-demand Zero-fill-on-demand pages have been zeroed-out before being allocated, thus erasing the previous contents. vfork() (virtual memory fork) It operates differently from fork() with copy-on-write. With vfork() , the parent process is suspended, and the child process uses the address space of the parent.","title":"9.3 Copy-on-Write"},{"location":"OS/Chap09/#94-page-replacement","text":"","title":"9.4 Page Replacement"},{"location":"OS/Chap09/#941-basic-page-replacement","text":"Find the location of the desired page on the disk. Find a free frame: a. If there is a free frame, use it. b. If there is no free frame, use a page-replacement algorithm to select a victim frame . c. Write the victim frame to the disk; change the page and frame tables accordingly. Read the desired page into the newly freed frame; change the page and frame tables. Continue the user process from where the page fault occurred. Modify bit (dirty bit) The modify bit for a page is set by the hardware whenever any byte in the page is written into. When we select a page for replacement. If the modify bit is set, we must write the page to the disk. If the modify bit is not set, we need not write the page to the disk. We must solve two major problems to implement demand paging: frame-allocation algorithm: decide how many frames to allocate to each process. page-replacement algorithm: when page replacement is required. Reference string The string of memory references.","title":"9.4.1 Basic Page Replacement"},{"location":"OS/Chap09/#942-fifo-page-replacement","text":"Belady's anomaly For some page-replacement algorithms, the page-fault rate may increase as the number of allocated frames increases.","title":"9.4.2 FIFO Page Replacement"},{"location":"OS/Chap09/#943-optimal-page-replacement","text":"Optimal page-replacement algorithm Replace the page that will not be used for the longest period of time, which will never suffer from Belady's anomaly.","title":"9.4.3 Optimal Page Replacement"},{"location":"OS/Chap09/#944-lru-page-replacement","text":"The LRU policy is often used as a page-replacement algorithm and is considered to be good. The major problem is how to implement LRU replacement. Counter cons: overflow Stack","title":"9.4.4 LRU Page Replacement"},{"location":"OS/Chap09/#945-lru-approximation-page-replacement","text":"","title":"9.4.5 LRU-Approximation Page Replacement"},{"location":"OS/Chap09/#9451-additional-reference-bits-algorithm","text":"The operating system shifts the reference bit for each page into the high-order bit of its 8-bit byte, shifting the other bits right by 1 bit and discarding the low-order bit. These 8-bit shift registers contain the history of page use for the last eight time periods. e.g. A page with a history register value of 11000100 has been used more recently than one with a value of 01110111.","title":"9.4.5.1 Additional-Reference-Bits Algorithm"},{"location":"OS/Chap09/#9452-second-chance-algorithm","text":"In the worst case, when all bits are set, the pointer cycles through the whole queue, giving each page a second chance. It clears all the reference bits before selecting the next page for replacement. ($\\to$ FIFO)","title":"9.4.5.2 Second-Chance Algorithm"},{"location":"OS/Chap09/#9453-enhanced-second-chance-algorithm","text":"ordered pair = (reference bit, modify bit) (0, 0) neither recently used nor modified\u2014best page to replace (0, 1) not recently used but modified\u2014not quite as good, because the page will need to be written out before replacement (1, 0) recently used but clean\u2014probably will be used again soon (1, 1) recently used and modified\u2014probably will be used again soon, and the page will be need to be written out to disk before it can be replaced","title":"9.4.5.3 Enhanced Second-Chance Algorithm"},{"location":"OS/Chap09/#946-counting-based-page-replacement","text":"least frequently used (LFU) most frequently used (MFU)","title":"9.4.6 Counting-Based Page Replacement"},{"location":"OS/Chap09/#947-page-buffering-algorithms","text":"Systems keep a pool of free frames The desired page is read into a free frame from the pool before the victime is written out. When the victim is later written out, its frame is added to the free-frame pool. There are two variations: To maintain a list of modified pages. Whenever the paging device is idle, a modified page is written to the dist and its modify bit is reset. To keep a pool of free frames but to remember which page was in each frame. No I/O is needed in this case $\\to$ \"Swapped-in\" time is saved!","title":"9.4.7 Page-Buffering Algorithms"},{"location":"OS/Chap09/#948-applications-and-page-replacement","text":"","title":"9.4.8 Applications and Page Replacement"},{"location":"OS/final/","text":"Previous Operating System Midterms at NTUCSIE Spring 2011 The exam is 150 minutes long. The total score is 112pts. Please read the questions carefully. Terminologies (21pts). Condition Variable (Hint: Monitor) Stable Storage 50-Percent Rule for the First-Fit Algorithm (Hint: Dynamic Partitioning) Virtual Memory Lazy Swapper Network Information Service (NIS or Yellow Pages from SUN) Scan (or Elevator) Algorithm (Hint: Disk Scheduling) Please answer the following question on process synchronization: (18pts) Synchronization hardware might make programming easier for process synchronization. Please explain why interrupt disabling works for uniprocessor environments to avoid the race condition. (6pts) There are three requirements for a solution to critical section problem. Please tell me which requirement might not be satisfied for solutions based on semaphores with priority-waiting queues. You must provide your arguments. (6pts) When the Two-Phase Locking protocol (2PL) is adopted for process synchronization, and there is only exclusive lock, can we have a deadlock? You must provide your arguments. (6pts) Given the following snapshot of the system: Please determine whether there exists a safe sequence. (5pts) With binding at the compiling time, is it appropriate to have Paging? With binding at the loading time, is it appropriate to have Paging? You must provide explanation. (6pts) Given a computer system with 64-bit virtual address and 8 bytes per page entry, let the physical address be of 48 bits, and the system is byte-addressable. Assume that every page is of 4KB. Please answer the following questions: (20pts). What is the maximum number of frames? (5pts) Suppose that we have multi-level paging. How many levels do we have in multi-level paging? (5pts) Suppose that TLB is adopted for paging, and multi-level page tables are all in the main memory. Let the memory access time and TLB access time be 100ns and 10ns, respectively. What the TLB hit ratio should be so that the effective memory access time would be no more than 120ns? (5pts) Continued with the previous question, What is the maximum number of entries for an inverted page table? (5pts) Please answer the following questions for demand paging: (20pts) What is the maximum number of pages needed for the following instruction: Add (R1)+, -(R2), +(R3) (4pts) The effective access time is defined as (1 - p) * ma + p * pft, where p, ma, and pft are the probability of a page fault, the memory access time for paging, and the page fault time, respectively. Please give me two approaches to reduce the pft. Please give me two approaches to reduce p. (16pts) Given the following reference string, which reference causes a page fault under the Second Chance (Clock) Algorithm. Please also show us which page is replaced when a page replacement occurs? Suppose that we have 3 available frames with 0, 1, and 3 as shown in the following graph, all reference bits are 0, and the selection pointer starts at Page 0. (10pts) $$1 \\, 0 \\, 3 \\, 1 \\, 4 \\, 3 \\, 2 \\, 1 \\, 3 \\, 0 \\, 2 \\, 1 \\, 5$$ Modern operating system often only recognize few file types. Please give two advantages why file types are usually supported by applications, instead of the operating system. (8pts) Give me one reason when we need scheduling in I/O subsystem, beside performance improvement. (4pts) Spring 2012 Terminologies. (24pts) Deadlock A set of process is in a deadlock state when every process in the set is waiting for an event that can be caused by only another process in the set. Segmentation Segmentation is a memory management scheme that supports the user view of memory. Working Set (Hint: Thrashing and Frame Allocation) The working set is an approximation of a program's locality. Optimal Algorithm for Page Replacement (OPT) Replace the page that will not be used for the longest period of time. Hard Link (Hint: Acyclic-Graph Directory) A hard link of a file name let its directory entry point to the i-node that describes the file's contents. Network-Attached Storage (NAS) Clients access NAS via a remote procedure call interface such as NFS (for UNIX) or CIFS (for Windows) where NAS has a file system running on it. Raw Disk Ans: A partition as a large sequential array of logical blocks. Please provide one advantage and one disadvantage of a deadlock avoidance algorithm (such as the Banker's Algorithm), compared to a deadlock prevention algorithm (such as the one without \"Hold-and-Wait\"). (6pts) Advantage: Better resource utilization and/or better system throughput/performance. Dsadvantage: Processes must claim maximum usages of each resource. Spring 2013 Spring 2014 Spring 2015","title":"Final"},{"location":"OS/final/#previous-operating-system-midterms-at-ntucsie","text":"","title":"Previous Operating System Midterms at NTUCSIE"},{"location":"OS/final/#spring-2011","text":"The exam is 150 minutes long. The total score is 112pts. Please read the questions carefully. Terminologies (21pts). Condition Variable (Hint: Monitor) Stable Storage 50-Percent Rule for the First-Fit Algorithm (Hint: Dynamic Partitioning) Virtual Memory Lazy Swapper Network Information Service (NIS or Yellow Pages from SUN) Scan (or Elevator) Algorithm (Hint: Disk Scheduling) Please answer the following question on process synchronization: (18pts) Synchronization hardware might make programming easier for process synchronization. Please explain why interrupt disabling works for uniprocessor environments to avoid the race condition. (6pts) There are three requirements for a solution to critical section problem. Please tell me which requirement might not be satisfied for solutions based on semaphores with priority-waiting queues. You must provide your arguments. (6pts) When the Two-Phase Locking protocol (2PL) is adopted for process synchronization, and there is only exclusive lock, can we have a deadlock? You must provide your arguments. (6pts) Given the following snapshot of the system: Please determine whether there exists a safe sequence. (5pts) With binding at the compiling time, is it appropriate to have Paging? With binding at the loading time, is it appropriate to have Paging? You must provide explanation. (6pts) Given a computer system with 64-bit virtual address and 8 bytes per page entry, let the physical address be of 48 bits, and the system is byte-addressable. Assume that every page is of 4KB. Please answer the following questions: (20pts). What is the maximum number of frames? (5pts) Suppose that we have multi-level paging. How many levels do we have in multi-level paging? (5pts) Suppose that TLB is adopted for paging, and multi-level page tables are all in the main memory. Let the memory access time and TLB access time be 100ns and 10ns, respectively. What the TLB hit ratio should be so that the effective memory access time would be no more than 120ns? (5pts) Continued with the previous question, What is the maximum number of entries for an inverted page table? (5pts) Please answer the following questions for demand paging: (20pts) What is the maximum number of pages needed for the following instruction: Add (R1)+, -(R2), +(R3) (4pts) The effective access time is defined as (1 - p) * ma + p * pft, where p, ma, and pft are the probability of a page fault, the memory access time for paging, and the page fault time, respectively. Please give me two approaches to reduce the pft. Please give me two approaches to reduce p. (16pts) Given the following reference string, which reference causes a page fault under the Second Chance (Clock) Algorithm. Please also show us which page is replaced when a page replacement occurs? Suppose that we have 3 available frames with 0, 1, and 3 as shown in the following graph, all reference bits are 0, and the selection pointer starts at Page 0. (10pts) $$1 \\, 0 \\, 3 \\, 1 \\, 4 \\, 3 \\, 2 \\, 1 \\, 3 \\, 0 \\, 2 \\, 1 \\, 5$$ Modern operating system often only recognize few file types. Please give two advantages why file types are usually supported by applications, instead of the operating system. (8pts) Give me one reason when we need scheduling in I/O subsystem, beside performance improvement. (4pts)","title":"Spring 2011"},{"location":"OS/final/#spring-2012","text":"Terminologies. (24pts) Deadlock A set of process is in a deadlock state when every process in the set is waiting for an event that can be caused by only another process in the set. Segmentation Segmentation is a memory management scheme that supports the user view of memory. Working Set (Hint: Thrashing and Frame Allocation) The working set is an approximation of a program's locality. Optimal Algorithm for Page Replacement (OPT) Replace the page that will not be used for the longest period of time. Hard Link (Hint: Acyclic-Graph Directory) A hard link of a file name let its directory entry point to the i-node that describes the file's contents. Network-Attached Storage (NAS) Clients access NAS via a remote procedure call interface such as NFS (for UNIX) or CIFS (for Windows) where NAS has a file system running on it. Raw Disk Ans: A partition as a large sequential array of logical blocks. Please provide one advantage and one disadvantage of a deadlock avoidance algorithm (such as the Banker's Algorithm), compared to a deadlock prevention algorithm (such as the one without \"Hold-and-Wait\"). (6pts) Advantage: Better resource utilization and/or better system throughput/performance. Dsadvantage: Processes must claim maximum usages of each resource.","title":"Spring 2012"},{"location":"OS/final/#spring-2013","text":"","title":"Spring 2013"},{"location":"OS/final/#spring-2014","text":"","title":"Spring 2014"},{"location":"OS/final/#spring-2015","text":"","title":"Spring 2015"},{"location":"OS/midterm/","text":"Previous Operating System Midterms at NTUCSIE Spring 2011 Terminologies. (24pts) Security Defense of a system from external and internal attacks, e.g., viruses, denial of services, etc. A Layered Approach in OS Designs The operating system is broken into a number of layers (levels). The bottom layer (layer $0$) is the hardware; the highest (layer $N$) is the user interface. Para-virtualization A variation on virtualization that presents a guest/operating system that is similar but not identical to the underlying hardware. Lightweight Process A virtual processor (kernel threads) on which the application can schedule a user thread to run. (many-to-many or two-level) NUMA Non-Uniform Memory Access: A CPU has faster access to some parts of main memory than to other parts. Deterministic Modeling Take a particular predetermined workload and defines the performance of each algorithm for that workload. Race Condition Several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place. Write Ahead Logging All modifications are written to a log before they are applied. Please answer following questions regarding the design of operation systems. (22pts) There are two conflicting goals in OS designs: Convenience and Efficiency. Please give me one example feature of OS that shows the conflict in persuing efficiency and convenience (Hint: Live preview of open windows for Windows7 taskbar). (5pts) GUI vs Command Line. What is the main goal of UNIX process init? Is init a user or kernel process? (8pts) init is responsible for bringing up a UNIX system after the kernel has been bootstrapped. User process with superuser privileges! Please explain how I/O protection is done. (5pts) Dual Mode Operations: a mode bit. Give me one advantage in OS implementations in some high-level language. (4pts) Being easy to understand & debug. Being written fast, more compact, and portable. Please answer the following questions for process managment. (16pts) Inside the Process Control Block, we might have a filed \"Program Counter\". What is the purpose of the field? (4pts) It can let us know which line to run, e.g. saved return address $\\to$ program counter. There are a lot of segments for a process image, such as code segment, data segment, heap, and user stack. When we call malloc() or free() , which segment is involved? (4pts) heap. When a parent process calls fork() to create a child process, how does the parent process know the process ID of the created child process? (4pts) When a parent process invoke fork(), a duplication of the parent process is created for the resulted child process, where the parent process returns from fork() with the process ID of the created child process. The child process returns 0. Which of the following IPC mechanisms does not require two communicating processes to have a parent-child relationship: Named Pipes and sockets. (4pts) Sockets. Please answer the following questions for CPU scheduling. (17pts) Compared to user-level threads, why the cost of context switching for kernel-level threads in higher? (4pts) Kernel thread: involves switching registers, stack pointer, and program counter, but is independent of address space switching User-level threads: similar context switching happens often. The delivery of a signal for theads is complicated. Give me an example signal that should be delivered to the threads to which the signal applies. (4pts) Division by zero. For preemptive scheduling, there are serveral occasions in triggering scheduling. Please give me three, beside the one in which a running process terminates by itself. (9pts) A new process with a higher priority than the current process arrives. An interrupt occurs. (fairness) Periodically based on clock interrupt. (e.g. RR) Consider Shortest-Job-First(SJF) and Round-Robin(RR) scheduling algorithms, and processes under considerations are only of one single CPU burst and are all ready at time $0$. Please answer the folowing questions. Explanation is needed to receive any credit: (18pts) Is SJF always better than RR, for any time quantum, in terms of the average turnaround time?(6pts) Yes. Proof : WLOG, given processes $P_1$, $P_2$, $\\dots$, $P_n$ with CPU burst time $t_1$, $t_2$, $\\dots$, $t_n$, where $t_1 < t_2 < \\cdots < t_n$. By SJF, their turnaround time should be: \\begin{array}{c|c} P_i & \\text{waiting time} \\\\ \\hline P_1 & t_1 \\\\ P_2 & t_1 + t_2 \\\\ P_3 & t_1 + t_2 + t_3 \\\\ \\vdots & \\vdots \\\\ P_n & t_1 + t_2 + \\cdots + t_n \\end{array} Total waiting time: $$nt_1 + (n - 1)t_2 + \\cdots + t_n.$$ Obviously, this sum is minimized if $t_i$'s that are multiplied more times are smaller ones, i.e., $$t_1 < t_2 < \\cdots < t_n.$$ Thus, in non-preemptive scheduling, SJF (actually Shortest-next-CPU-burst-first) is optimal for the purpose of minimizing turnaround time. When all processes are of the same size, please tell us what the best time quantum is for RR in terms of the average waiting time. (6pts) $80\\%$ of the CPU bursts should be shorter than the time quantum. Now suppose that processes might arrive at different times, and SJF and RR are preemptive scheduling algorithms. Is SJF always better than RR, for any time quantum, in terms of the average waiting time. (6pts) Similar to 5.(a). Please design a solution for airplanes to land in an airport. Suppose that there is only one runway in the airport. Please make sure that only one airplane can control the runway to land at a time, and there should be no starvation for your solution. (Hint: (1) the Bakery Algorithm; (2) Each process denotes an airplane.) (10pts) int number [ i ]; // Pi's number if it is nonzeros boolean choosing [ i ]; // Pi is taking a number do { choosing [ i ] = true ; // A process want to enter its critical section number [ i ] = max ( number [ 0 ], ..., number [ n - 1 ]) + 1 ; choosing [ i ] = false ; // A process has got its number for ( int j = 0 ; j < n ; j ++ ) { while ( choosing [ j ]) ; while ( number [ j ] != 0 && ( number [ j ], j ) < ( number [ i ], i )) ; // If two processes got the same number, then we should compare their indices } /* critical section */ number [ i ] = 0 ; /* remainder section */ } while ( true ); Fall 2011 The exam is 180 minutes long. The total score is 107pts. Please read the questions carefully. Terminologies. (24pts) DMA Release CPU from handling excessive interrupts! Execute the device driver to set up the registers of the DMA controller. DMA moves blocks of data between the memory and its own buffers. Transfer from its buffers to its devices. Interrupt the CPU when the job is done. Multiprogramming Increases CPU utilization by organizing jobs so that the CPU always has one to execute. Horizontal Cache Coherency and Consistency Among units of the same storage level. From \u77e5\u4e4e : Coherence \u4fdd\u8b49\u7684\u662f\u540c\u4e00\u5730\u5740\u6709\u4e0d\u540c copy \u7684\u6642\u5019\uff0c\u4fdd\u8b49\u770b\u5230\u7684\u662f\u5728 timing \u4e0a\u96e2\u81ea\u5df1\u6700\u8fd1\u7684\u3002 \u4f46\u662f\uff0c\u53ea\u4fdd\u8b49 Coherence \u662f\u4e0d\u5920\u7684\uff0c\u5728 multiprocessor \u4e0d\u540c\u5730\u5740\u7684\u591a\u500b copy \u8a2a\u554f\u7684\u6642\u5019\u6703\u51fa\u73fe\u554f\u984c\uff0c\u9019\u500b\u5c31\u662fconsistency Coherence is concerned with updates/invalidations to a single shared variable. Consistency is concerned with the behavior of memory references from multiple concurrent threads. A Module Approach in OS Designs (Hint: A Layered Approach) [ask] Moving all nonessential components from the kernel to the user or system programs! Indirect Communication in Message Passing ($\\leftrightarrow$ Direct) The messages are sent to and received from mailboxes, or ports. ($\\leftrightarrow$) The messages are sent to and received from processes. Socket An endpoint for communication. (IP + port#) Deferred Cancellation ($\\leftrightarrow$ Asynchronous Cancellation) The target thread periodically checks whether it should terminate, allowing it an opportunity to terminate itself in an orderly fashion. ($\\leftrightarrow$) One thread immediately terminates the target thread. Pull Migration (Hint: Multipocessor Scheduling) ($\\leftrightarrow$ Push Migration) Pulling a waiting task from a busy processor. ($\\leftrightarrow$) Pushing processes from overloaded to less-busy processors. Please answer the following questions regarding the designs of operating systems: (23pts) Please give me two resources, beside CPU, that are managed by OS. (6pts) CPU time, Memory Space, File Storage, I/O Devices, Shared Code, Data Structures, and more. When an interrupt arrives, a running task is interrupted, and its context could be saved in different ways, such as \"a fixed address for all interrupts\", \"a fixed space for each interrupt type\", and \"a stack\". What is the advantage in using a stack, compared with the approach in using a fixed space for each interrupt type? (5pts) If there are 2 interrupts, using fixed space is bad since the address of the first interrupted will be covered by later interrupted process. In a memory hierarchy, we have registers, cache, memory, and disk. Which of them is managed by operating systems? Which of them is managed by hardware? (12pts) Operating system: registers, cache, memory Hardware: disk OS services are such as those for \"program execution\", \"file-system manipulation\", \"accounting\", and \"resource allocation\". Which of them are for system efficiency, instead of user convenience? (8pts) [ask] Program execution and resource allocation. Famous Application Programming Interfaces (API) are such as Win 32 API and POSIX API. What are the two major benefits in providing API, compared to the providing of system calls only? protability (expected to run on any system) actual system calls can be more difficult to learn What does POSIX API offers to programmers, compared to the offering of ANSI C to programmers? (8pts) Any program written only in ANSI C and without any hardware dependent assumptions is virtully guaranteed to compile correctly on any platform with a conforming C implementation. POSIX is an acronym for \"Portable Operating System Interface\". POSIX is for software compatibility with variants of UNIX and other operating systems. In the ordinary virtualization design, the virtualization layer runs in the system mode. For VMware, the virtualization layer runs in the system or user mode? The user mode. For Java, the Java virtual machine run in the system or user mode? (6pts) The user mode. Please answer the following questions for process management and scheduling. (20pts) Give me two conditions for a running process to relinquish the CPU to go back to the ready queue in preemptive CPU scheduling. (6pts) [Spring 2011 5.(c)] A new process with a higher priority than the current process arrives. An interrupt occurs. (fairness) Periodically based on clock interrupt. (e.g. RR) Why a long-term scheduler has more time to choose a process for a system than a short-term scheduler does in process scheduling? (5pts) Because the interval between executions are longer. Please explain the main difference between a user-level thread and a kernel thread. (5pts) Is a Java thread is a user-level thread or a kernel thread? (4pts) Level Depending on the Thread Library on the Host System. The scheduling algorithm of Solaris 9 is based on the Multilevel Feedback Queue Scheduling algorithm. There are six priority classes. Please explain the Fair Sharing class? Please explain how interactive threads in the Time Sharing or Interactive class are favored in scheduling in Solaris 8 or 9? (6pts) Consider the scheduling of processes in which processes might arrive at different times and have different deadlines to complete their execution. Let the processes be scheduled by the preemptive Shortest-Job-First algorithm (PSJF) and a Priority Scheduling algorithm (PS) in which processes with urgent deadlines have higher priorities, and there is only one processor. Can you give a set of processes such that PS can meet the deadlines of the processes, but PSJF can not do it? (5pts) Given processes: Process Period ProcTime $P_1$ $p_1 = 50$ $t_1 = 25$ $P_2$ $p_2 = 80$ $t_2 = 35$ Consider the Round Robin scheduling algorithm (RR) with two different time quantums $L$ and $S$, where $L > S$. Let the scheduling criteria be the average waiting time, and $L$ be larger than the largest CPU burst of all processes. Does RR, in general, favor a small time quantum $S$ when all processes are ready at time $0$? Please give me your answer with argument. (7pts) No. For example, given processes $P_1$, $P_2$ and $P_3$ with CPU burst time $10$, $10$ and $10$ With quantum $> 10 (L)$, average waiting time = (0 + (10 - 0) + (20 - 0)) / 3 = 30 / 10 = 10. With quantum $= 5$, average waiting time = [0 + (15 - 5) + (5 - 0) + (20 - 10) + (10 - 0) + (25 - 15)] = 45 / 3 = 15. With quantum $= 1$, average waiting time = ... = (18 + 19 + 20) / 3 = 19. With quantum $\\to 0$, average waiting time $\\approx$ (20 + 20 + 20) / 3 = 20. It is obvious that smaller time slice will lead a longer average waiting time. Fall 2012 The exam is 180 minutes long. The total score is 110pts. Please read the questions carefully. Terminologies. (24pts) Hardware Interrupt Services requests of I/O devices. Virtual Machine Provides an interface that is identical to the underlying bare hardware. Context Switch It saves the state of the currently running process and loads the state of the newly scheduled process. A Full Duplex Pipe A pipe that supports two ways of message passing simultaneously. Multilevel Queue Scheduling Processes can be classified into different groups and permanently assigned to one queue, where there are Inter-queue and intra-queue scheduling policies. Memory Stall A phenomenon in which a processor waits for a significant amount of time waiting for the data to become available. Bounded Waiting (A Requirement of a Critical Section Solution) A waiting process only waits for a bounded number of processes to enter their critical sections. Adaptive Mutex A binary semaphore in which it is a spinlock if the lock-holding thread is running; otherwise, blocking is used. Please answer the following questions regarding the designs of operating systems: (20pts) What is the difference between multiprogramming and time sharing ? (6pts) Time sharing (or multitasking) is a logical extension of multiprogramming, where CPU services each of ready tasks in a way that every task receives CPU time in an interactive fashion. Which one of the following memory unit is managed by the operating systems: Registers, Cache, Main Memory, Disks (8pts) Main Memory and Disks. Operating systems services include user interfaces. UNIX shells, including the Bourne shell and C shell, provide command interpreters. Consider UNIX shells, please give me one command that is implemented as some code inside the command interpreter and two commands that are implemented by system programs? (6pts) Inside the command interpreter: C shell: umask, cd and limit Bourne shell: ulimit -H and -S By system programs: rm and ls Message passing is a way for interprocess communication. Consider the capacity of a link between two processes has zero capacity. Is the message passing (between the two processes) synchronous or asynchronous? You must provide explanation to receive any credits. (5pts) It is synchronous because we can only have blocking sends and blocking receives. Please give me one occasion when a mid-term scheduler should run. The remaining main memory is low or the CPU utilization is too high. Can a short-term scheduler schedule the executions of user-level threads? No. Can a short-term scheduler schedule the executions of a Java thread? (9pts) It depends on the thread library on the host system. Consider signal handling for threads. Is a Division-By-Zero signal synchronous or asynchronous? Synchronous. Should a Division-By-Zero signal be delivered to every thread of its belonging process? (6pts) [Spring 2011 4.(b)] No. It should be only sent to the thread that causes the signal. Please answer the following questions for process scheduling. Explanation is needed to receive any credit. (24pts) For the Round-Robin Scheduling, what would be the preferred time slice in general? (4pts) [Spring 2011 6.(b)] $80\\%$ of the CPU bursts should be shorter than the time quantum. For the Round-Robin Scheduling, shall we have a small time slice for a better average turn around time? (4pts) No, a small time slice will increase the average turnaround time. For the Priority Scheduling, how to avoid the starvation problem (in which a low priority process can never be scheduled)? (4pts) An aging solution by increasing the priority of a process that waits for a long time. Please explain how a guest operating-system scheduling algorithm that assumes a certain amount of progress in a given amount of time might be negatively impacted by virtualization. (6pts) It is because the virtualization software needs to schedule the use of the physical CPUs among the virtual CPUs. A given amount of the time slice might take much more than the time of the virtual CPU time. For the evaluation of a scheduling algorithm, please give me two difficulties in using the implementation method. (6pts) Cost in modifying the OS User reaction to OS changes Changing of the environment in which the algorithm is used Please explain the difference between the signal operation of a binary semaphore and that of a condition variable (of a monitor). (5pts) The signal operation of a condition variable resumes one suspended process. If there is none, no effect is imposed. Consider a barber shop in which there are two barbers. Each barber can service exactly one customer at a time. Please use binary semaphores to implement the request for the hair-cut service. (12pts) Use an integer $S$ with initial value = $2$ to indicate the number of available barbers. typedef struct { int value ; struct customer * waiting_list ; } semaphore ; wait ( semaphore * S ) { S -> value -- ; if ( S -> value < 0 ) { add this customer to S -> waiting_list ; block (); } } signal ( semaphore * S ) { S -> value ++ ; if ( S -> value <= 0 ) { remove a customer C from S -> waiting_list ; wakeup ( C ); } } Customer $C_i$: wait ( S ); /* critical section */ signal ( S ); Consider the time-stamp protocol of Chapter 6, in which each process $T_i$ is given a time stamp $T_S(T_i)$, and each read/write operation must check it up with the read and write timestamp of the accessed data object $Q$. Is it possible to have any deadlock? You must provide your explanation. (5pts) [ask] No deadlock because there is no hold-and-wait among processes. Fall 2013 The exam is 180 minutes long. The total score is 103pts. Please read the questions carefully. Terminologies. (24pts) Booting Initialize all aspects of the system and then load and run the OS. Interrupt Vector An array of interrupt-handlers' addresses that are indexed by device numbers (or interrupt numbers). Cache Coherency [Fall 2011 1.(c)] Cache coherency problems can arise when more than one processors refer to the same data. Coherency defines what value is returned on a read. Platform as a service (PaaS) in Cloud Computing Pass provides a computing platform and a solution stack as a service, such as a database server. One example is Microsoft Azure. A Modular Kernel (Hint: OS Structure) [Fall 2011 1.(d)] A moduler kernel consists of a set of components, where there are core/primary modules, and it is of modules without a layer structure. Data Parellelism in Multicore Programming Distribute data over cores to execute the same operation . Rate Monotonic Scheduling A fixed priority real-time process scheduling algorithm in which the priorities of processes are inversely proportional to their periods . Deterministic Modeling [Spring 2011 1.(f)] Take a particular predetermined workload and defines the performance of each algorithm for that workload. Please answer the following questions regarding the designs of operating systems: (20pts) There are two conflicting goals in the designs of operating systems. What is the other one, beside \"convenient for the users\" ? (3pts) Efficient operation of the computer system. Which one of the following actions/events is belonging to or might result in software interrupts: System calls, child termination, mouse clicking, invalid memory access. (8pts) System calls, child temination, and invalid memory access. Consider the implementation of virtual machines in which operating systems run on the top of the virtual machine software (or referred to as the hypervisor). Is an operating system running in the user mode or kernel mode? (3pts) [Fall 2011 5.(a), Fall 2013 2.(c)] The user mode. Please give me 2 advantages of virtual machines, beside system consolidation and easy in system development/deployment. (6pts) Complete isolation and multiple personalities. ANSI C refers to the family of successive standards published by ANSI for the C programming language. Please compare difference (or provide the purpose difference) between POSIX and ANSI C. (5pts) [Fall 2011 4.(b)] Any program written only in ANSI C and without any hardware dependent assumptions is virtully guaranteed to compile correctly on any platform with a conforming C implementation. POSIX is an acronym for \"Portable Operating System Interface\". POSIX is for software compatibility with variants of UNIX and other operating systems. Please answer the following questions for task scheduling. (14pts) Is the \"Swapper\" a short-term or mid-term scheduler? [ask] Short-term scheduler. In UNIX, the process control block PCB[] of a process consists of proc[] and .u, where the attributes in .u are those needed when the process is running, and the attributes in proc[] are those needed all the time. Please indicate which one should be in .u : file[], task priority, pid, signal disposition, and task state. You must provide explanation to receive any credits. (10pts) [ask] file[]: what files are \"being\" opened. signal disposition: how to deal with signals. Consider message passing and shared memory for inerprocess communication. Is \"Pipe\" considered one for message passing or shared memory ? Is \"Pipe\" direct or indirect communication ? Is it \"synchronous\" or \"asynchronous\"? for a reader or a writer of the communication in UNIX. You must provide explanation to receive any credits. (12pts) Message passing. Since it's communication between processes without sharing the same address space. Indirect communication. [ask] Synchronous for a reader. [ask] Asynchronous for a writer. [ask] Kernel-level threads are superior than uer-level threads do in many ways. What is the main disadvantage of kernel-level threads? It is context switching cost. With OpenMP in program development, shall we prefer kernel-level or user-level threads? You must provide explanation to receive any credits. (8pts) We prefer kernel-level threads because we look for parallelism to better utilize multiple cores of a system. Please answer the following questions for process scheduling. Explanation is needed to receive any credits. (15pts) Given 3 processes $P_1$, $P_2$, and $P_3$ with CPU burst time $5$, $6$, $7$, respectively. Suppose that the 3 processes arrive at time $0$, and $P_1$ and $P_3$ are the first and the last processes in the ready queue, respectively. What is the average waiting time in running the 3 processes under the Round-Robin Scheduling with the time slice equal to $3$. (5pts) (6 + (3 + 5) + (6 + 5)) / 3 = 25 / 3. Consider FCFS and Round-Robin Scheduling. If process are only of CPU burst and all arrive at time $0$, do FCFS and Round-Robin Scheduling with time slice = $1$ always have the same total waiting time in running all processes ? Prove your answer. (5pts) No, for example, given processes of CPU burst 7, 1, and 1. Suppose that the variance of the tunaround time is the cirterion in process scheduling. Shall we have a small time slice for a better variance turnaround time when all processes arrive at time $0$? (5pts) No. For example, given processes $P_1$, $P_2$ and $P_3$ with CPU burst time $10$, $10$ and $10$. With quantum $= 10$, average turnaround time = (10 + 20 + 30) / 3 = 60 / 3 = 20. With quantum $= 5$, average turnaround time = (20 + 25 + 30) / 3 = 75 / 3 = 25. With quantum $= 1$, average turnaround time = ... = (28 + 29 + 30) / 3 = 87 / 3 = 29. With quantum $\\to 0$, average turnaround time $\\approx$ 30 + 30 + 30 = 90 / 3 = 30. It is obvious that smaller time slice will lead a longer average turnaround time. Please explain why the Completely Fair Scheduling (CFS) of Linux V2.6 favors I/O tasks. (5pts) CFS usually dispatches the CPU to the task with the smallest vruntime (that denotes how long the task runs) such that I/O tasks usually have smaller vruntime. Fall 2014 The exam is 180 minutes long. The total score is 108pts. Please read the questions carefully. Terminologies. (24pts) Buffering (Hint: It is not caching.) It means to keep data in a faster medium temporarily before moving them to a slower layer. Virtual Machine [Fall 2012 1.(b)] Provides an interface that is identical to the underlying bare hardware. System Generation (SYSGEN) The process to configure or generate an operating system for a one specific computer. Context Switch [Fall 2012 1.(c)] Save the state of the old process and load the state of the newly scheduled process. Remote Procedure Call (Hint: Message Passing) Senders are blocked until the receivers have received messages and replied by reply messages. (A way to abstract the procedure-call mechanism for use between systems with network connection.) Implicit Threading Transfer the creation and management of the threading from the application developers to compilers and run-time libraries. Earliest Deadline First Scheduling A dynamic-priority real-time process scheduling algorithm in which the priorities of processes are higher if their deadlines are closer. Race Condition [Spring 2011 1.(g)] Several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place. Please answer the following questions regarding the designs of operating systems: (22pts) An operating system could be considered as a resource allocator. Please list 3 resources managed by an operating systems. (6pts) [Fall 2011 2.(a)] CPU time, Memory Space, File Storage, I/O Devices, Shared Code, Data Structures, and more. Please explain what happens to the operating system when an interrupt arrives (until the interrupt is serviced). (4pts) Saving of the address of the interrupted instruction, determine the interrupt type (by polling or interrupt vector), call the corresponding handlers. Which one of the following instructions is a privileged instruction: Reading of the Timer, setting of the base register of the memory space of a process, increment the value of a CPU register by one. No explanation is needed. (6pts) Setting of the base register of the memory space of a process. Please explain what happens when a command-line user interface of UNIX executes a command. (6pts) Search the exec file which corresponds to the command; fork a process to execute the file. Consider parameter passing to a system call. Give me the major advantage in having a register pointing to a memory block that stores all parameters, compared to having all parameters stored in registers? (5pts) The space needed to store all parameters is virtually unlimited. The memory image of a UNIX process might consist of a code segment, a data segment, a heap area, a user stack, a kernel stack, an environment variable area, and .u. Which one of the above is used when malloc() is invoked? [Spring 2011 3.(b)] heap. When the kernel stack is used? (8pts) [ask] Procedure invocation in the system mode. Please explain how fork() works when a parent process invoke fork() . [Spring 2011 3.(c)] When a parent process invoke fork(), a duplication of the parent process is created for the resulted child process, where the parent process returns from fork() with the process ID of the created child process. The child process returns 0. Please use the answer to the fork() invocation to explain the difference between fork() and vfork() . (10pts) When a parent process invoke vfork(), a child process is created by using the process image of the parent process, including its code and data segments. However, the parent process stop temporarily until the child process invokes an execve()-like system call to start a new program or terminates. The child process can modify any contents of the data segment of the parent process. In the multi-core age, threading is encouraged. Please answer the following questions: (8pts) For multiple cores, do we prefer kernel-level threads or user-level threads? Why? (3pts) [Fall 2013 6.(b)] We prefer kernel-level threads because we look for parallelism to better utilize multiple cores of a system. There are also challenges in programming. Consider merge sorting, in which an unsorted list is divided into $N$ sublists, each containing $1$ element, and then we repeatedly merge sublists to produce new sorted sublists until there is only $1$ sublist remaining. This will be the sorted list. Please use the concept of task parallelism to execute the merge sort over $N$ cores. (5pts) Each core is given a sublist of one element. We then group 2 adjacent cores and let one of the cores merge the sublists of the two cores. Please answer the following questions for process scheduling. Explanation is needed to receive any credit. (15pts) Given 5 processes $P_1$, $P_2$, $P_3$, $P_4$, and $P_5$ with CPU burst time $6$, $5$, $2$, $10$, $5$, respectively. Suppose that the $P_1$, $P_2$, $P_3$, $P_4$, and $P_5$ arrive at time $0$, $3$, $4$, $2$, and $5$, respectively. What is the average waiting time in running the 5 processes under the Preemptive Shortest-Job-First Scheduling. (5pts) (0 + (8 - 3) + (6 - 4) + (18 - 2) + (13 - 5)) / 5 = 31 / 5. Consider Shortest-Job-First Scheduling and Round-Robin Scheduling. Please prove that the total waiting time in running all processes under Shortest-Job-First Scheduling is always no larger than that of Round-Robin Scheduling with time slice equal to $1$ when all processes are ready at time $0$? (5pts) [Spring 2011 5.(a)] Yes. Proof : WLOG, given processes $P_1$, $P_2$, $\\dots$, $P_n$ with CPU burst time $t_1$, $t_2$, $\\dots$, $t_n$, where $t_1 < t_2 < \\cdots < t_n$. By SJF, their waiting time should be: \\begin{array}{c|c} P_i & \\text{waiting time} \\\\ \\hline P_1 & 0 \\\\ P_2 & t_1 \\\\ P_3 & t_1 + t_2 \\\\ \\vdots & \\vdots \\\\ P_n & t_1 + t_2 + \\cdots + t_{n - 1} \\end{array} Total waiting time: $$(n - 1)t_1 + (n - 2)t_2 + \\cdots + t_{n - 1}.$$ Obviously, this sum is minimized if $t_i$'s that are multiplied more times are smaller ones, i.e., $$t_1 < t_2 < \\cdots < t_{n - 1} < t_n.$$ Thus, in non-preemptive scheduling, SJF (actually Shortest-next-CPU-burst-first) is optimal for the purpose of minimizing average waiting time. Suppose that the variance of the waiting time is the criterion in process scheduling. Shall we have a small time slice for a better variance for Round-Robin Scheduling when all processes of the same CPU burst arrive at time $0$? (5pts) [Fall 2011 9., Fall 2013, 7.(c)] No. For example, given processes $P_1$, $P_2$ and $P_3$ with CPU burst time $10$, $10$ and $10$ With quantum $= 10$, average waiting time = (0 + (10 - 0) + (20 - 0)) / 3 = 30 / 10 = 10. With quantum $= 5$, average waiting time = [0 + (15 - 5) + (5 - 0) + (20 - 10) + (10 - 0) + (25 - 15)] = 45 / 3 = 15. With quantum $= 1$, average waiting time = ... = (18 + 19 + 20) / 3 = 19. With quantum $\\to 0$, average waiting time $\\approx$ (20 + 20 + 20) / 3 = 20. It is obvious that smaller time slice will lead a longer average waiting time. Please answer the following questions for process synchronization: (16pts) Please compare the difference between a binary semaphore and a condition variable. (3pts) [Fall 2012 7.] The signal operation of a condition variable resumes one suspended process. If there is none, no effect is imposed. Please use Monitor to implement Consumer and Producer with a bounded buffer. (10pts) monitor ProducerConsumer { int counter = 0 ; condition empty ; condition full ; void producer () { if ( counter == n ) wait ( empty ); /* produce a slot */ counter ++ ; signal ( full ); } void consumer () { if ( counter == 0 ) wait ( full ); /* consume a slot */ counter -- ; signal ( empty ); } } counter: number of filled slots empty: buffer has at least one empty slot full: buffer has at lesat one full slot Please prove that your above solution satisfy the Progress requirement of the Critical Section Problem. (3pts) Mutual exclusioin: monitor ensures that only one process can execute. Progress requirement: by signal(full) to wake wait(full) and signal(empty) to wake wait(empty), the processes won't wait forever. Bounded-waiting: they will both at most wait for 1 process (each other). Fall 2015 The Exam is 180 minutes long. The total score is 105pts. Please read the questions carefully. Terminologies. (24pts) Software Interrupts Caused by software execution, e.g. signals, invalid memory access, division by zero, system calls. Performance Tuning A procedure that seeks to improve performance by removing bottlenecks. Mid-Term Scheduler ($\\leftrightarrow$ Long-Term Scheduler $\\leftrightarrow$ Short-Term Scheduler) Swap processes in and out to control the degree of multiprogramming. ($\\leftrightarrow$) Long-Term Scheduler: Selects processes from this pool Loads them into memory for execution Controls the degree of multiprogramming (# processes). Selects a good process mix of I/O-bound and CPU-bound. ($\\leftrightarrow$) Short-Term Scheduler: Selects from among the processes that are ready to execute Allocates CPU to one of them FIFOs of UNIX Named pipes. Asynchronous Signal ($\\leftrightarrow$ Synchronous Signal) An asynchronous signal usually reports some asynchronous event outside the program, e.g., ^C or time expiration. ($\\leftrightarrow$) Delivered to the same process that performed the operation causing the signal. e.g., illegal memory access, division by 0. Push Migration (Hint: Load Balancing) [Fall 2011 1.(h)] ($\\leftrightarrow$ Pull Migration) Pushing processes from overloaded to less-busy processors. ($\\leftrightarrow$) Pulling a waiting task from a busy processor. Coarse-Grained Multithreading of Hardware Threads ($\\leftrightarrow$ Fine-Grained (interleved)) A thread executes on a processor until a long-latency event such as a memory stall occurs. ($\\leftrightarrow$) Switches between threads at a much finer level of granularity Analytic Evaluation [Spring 2011 1.(f), Fall 2013 1.(h)] Analytic evaluation uses the given algorithm and the system workload to produce a formula or number to evaluate the performance of the algorithm for that workload. Please answer the following questions regarding operating systems: (20pts) Please compare the difference between interrupt handling by a generic handler and interrupt vector in terms of the mechanism and the response performance. (6pts) A generic handler finds out the interupt type and invokes the corresponding procedure but the interrupt vector lets the corresponding procedure directly invoked though checking up of the interrupt vector; performance: interrupt vector is faster. Please compare the difference between the terms \"time sharing\" and \"multiprogramming\". (4pts) [Fall 2012 2.(a)] Time sharing (or multitasking) is a logical extension of multiprogramming, where CPU services each of ready tasks in a way that every task receives CPU time in an interactive fashion. One of the most challenging parts in the implementations of a virtual machine is to satisfy the assumption of a certain amount of progress in a given amount of time. Please explain the challenge. (6pts) [Fall 2012 6.(d)] It is because the virtualization software needs to schedule the use of the physical CPUs among the virtual CPUs. A given amount of the time slice might take much more than the time of the virtual CPU time. Parameter passing is an important issue in the implementation of command interpreters. Please explain how a command interpreter of UNIX passes parameters to the running process of a command issued on the command interpreter. (4pts) It can be done by using one of the exec() system calls. Consider process states: New, Ready, Running, Waiting, and Terminated. Please explain how a state makes a transition to another state, where there is only one processor. (12pts) Message passing is one major way for interprocess communication. What is the main difficulty in using symmetric addressing for direct communication? Process naming (or modularity). For indirect communication, small messages are sent from a sender to a receiver usually by message copying. How to reduce system overheads in-sending large messages to a receiver? (8pts) Remapping of addressing space. For multicore programming, there could be data parallelism or task parallelism. Please explain how to use data parallelism to find the largest integer of a given set of integers. (5pts) [Fall 2014 6.(b)] First split data over $N$ cores, and let each core find the largest integer of its given integers. Then let one core find the largest integers among the largest integers found on each core. Please answer the following questions for process scheduling. Explanation is needed to receive any credit. (15pts) Is the First-Come, First-Served Scheduling (FIFO) a non-preemptive or preemptive scheduling algorithm? Why? (4pts) It is non-preemptive because the running process will not volunteer to give up its CPU until it stops. Given 4 processes $P_1$, $P_2$, $P_3$, and $P_4$ with CPU burst time $5$, $2$, $4$, and $6$, respectively. Suppose that the $P_1$, $P_2$, $P_3$, and $P_4$ all arrive at time $0$. What is the average waiting time in running the 4 processes under the Round-Robin Scheduling with the time slice equal to $3$? (5pts) [Fall 2013 7.(a)] (8 + 3 + (5 + 5) + (8 + 3)) / 4 = 32 / 4 = 8. Longest-Job-First Scheduling always schedules the process of the longest CPU burst first. When all processes arrive at time $0$, does Longest-Job-First Scheduling have the largest average turnaround time? (6pts) [Fall 2013 7.(c)] Yes. Consider the intersection of the following two roads, where cars can go from each of the four directions. There is a stop sign for each direction so that every car must stop at the intersection and wait for any car that arrives earlier at the intersection to leave first. Please use semaphores to implement your solution with some pseudo code. (10pts) [Fall 2012 8.] Use an integer $S$ with initial value = $1$ to indicate the number of available car and a FIFO queue waiting list. typedef struct { int value ; struct car * waiting_list ; } semaphore ; wait ( semaphore * S ) { S -> value -- ; if ( S -> value < 0 ) { add this car to S -> waiting_list ; block (); } } signal ( semaphore * S ) { S -> value ++ ; if ( S -> value <= 0 ) { remove a car C from S -> waiting_list ; wakeup ( C ); } } Car $C_i$: wait ( S ); /* critical section */ signal ( S ); Please prove that your above solution satisfy the three requirements of the Critical Section Problem. (6pts) Mutual exclusioin: only a car can go across the intersection. Progress requirement: by block() and wakeup(), the processes won't wait forever. Bounded-waiting: at most wait for $n - 1$ cars. Could you revise your solution so that an ambulance can always go first? (5pts) Let the ambulance has the highest priority in the waiting queue. Spring 2018 The exam is 180 minutes long. The total score is 112pts. Please read the questions carefully. Terminologies. (24pts) init (Hint: Booting; a UNIX process) A user process that initializes system processes, e.g., various daemons, login processes, after the kernel has been bootstrapped. Dual-Mode Operations (Hint: Mode Bit) Privileged and user-mode instructions, where privileged instructions are machine instructions that may cause harm. Memory Protection (Hint: Hardware Protection) Prevent a user program from modifying the code or data structures of either the OS or other users! A Modular Kernel A set of core components with characteristics: layer-like \u2014 modules; microkernel-like \u2014 the primary module Asymmetric Addressing for Direct Communication In Direct Communication, sender must specify the receiver ID, but the receiver does not need to specify the sender ID. For example, Send(P, msg), Receive(id, msg) Remote Procedure Call (RPC) A way to abstract the procedure-call mechanism for use between systems with network connection. FIFOS of UNIX Named pipes of UNIX. It is a file in the file system and created by mkfifo(). It offers Half Dulex and Byte-Oriented Transmissions. Implicit Threading Transfer the creation and management of the threading from the application developers to compilers and run-time libraries. For the implementation of the UNIX process control block PCB[]\uff0c we have proc[] and .u, which contain different attributes for different purposes (e.g., those must be known regardless of whether the process is running). Which one is in the kemel or user address space? Are they both in the kemel or user address space? Which one of proc[] and .u signal disposition resides? Where thread-specific data/thread local storage will reside (in the kemel or user address space)? (8pts) kernel address space: proc[]; user-address space: .u .u: signal disposition. user address space. Please answer the following questions regarding operating systems: (21pts) When timing sharing appears as a feature for operating systems, please tell us two emerging features/subsystems, beside job synchronization. (6pts) one-line file systems, virtual memory, sophisticated CPU scheduling. Virtual machine software provides an interface that is identical to the underlying bare hardware. I/O over a virtual machine could be slow or fast, depending on the implementations. Please give me a case in which I/O is fast, compared to I/O directly over physical devices. (5pts) I/O can be fast over a hard disk when the corresponding disk is implemented over DRAM by emulation. There are operating system services, such as program executions, I/O operations, accounting, resource allocation, e\u6c40or detection. Which one is belonging to those for user convenience? Which one is belonging to those for system efficiency? (1Opts) user convenience: program executions, I/O operations, error detection; system efficiency: accounting, resource allocation. Multithreading is important for multi-core architectures. Please answer the following questions: (12pts) Task parallelism is one typical parallelism type. Please give one example for task parallelism. (5pts) Run merge sorting over cores by partitioning data. Please provide two challenges in multicore programming. (4pts) Identifying Tasks \u2014 Dividing Activities, Balance, Data Splitting, Data Dependency, Testing and Debugging. Why the context switching cost of kernel-level threads is higher than that of user-level threads. (3pts) Context switching cost is a little bit higher because the kernel must do the switching. Compare preemptive scheduling and non-preemptive scheduling. Please answer the following questions: (21 pts) In what circumstance, non-preemptive scheduling wiU be be\u5410er than preemptive scheduling? (4pts) Non-preemptive scheduling could be better than preemptive scheduling when all jobs are ready together at time 0. The selection of CPU scheduling algorithms should consider the scheduling criteria. If the high Thoughput is the criterion, which one is the best: Shortest job first, FIFO, and round-robin scheduling. You must provide answer to get credits. (6pts) shortest job first. Priority scheduling is a framework for scheduling. Please design a priority assignment formula that gives a lower priority to a job when it runs more time. (5pts) priority = 1 / consumed-CPU-time. For Round-Robin Scheduling, if the lower average Tumaround Time is the criterion, sha11 we prefer a lower time slice? Suppose that a11 jobs are ready at time O. You must provide answer to get credits. (6pts) a lower time slice is bad to the low average turnaround time. Processor Affinity is somehow against push migration. Please explain why. (6pts) It is because of the cost in cache repopulation. Please explain the progress assumption concerns for virtualization. (6pts) lt is assumed that a certain amount of progress should be observed for a system in a given amount oftime under virtualization. Consider solutions to the Critical Section Problem. Please answer the following questions (14pts) Consider a computing server to service jobs from three FIFO queues in a round robin way whenever there are jobs in any non-empty queues. Please use semaphores to implement your solution with some pseudo code. (6pts) We let the dispatcher of each job queue to wait a shared semaphore whenever it has a pending job. Let the semaphore have a FIFO queue. What is the difference between a binary, semaphore and a condition variable? (3pts) A conditino variable has nothing happening to a signal request if no job is waiting for the variable. The Monitor-based implementation of the Dining Philosopher problem in the textbook implements the idea of two-chop-stick-per-time solution. Will it have the starvation problem? You must provide explanation to receive credits. (5pts) Yes, the starvation problem happens to a philosopher whenever his two neighboring philosophers take turn in eating while he is waiting.","title":"Midterm"},{"location":"OS/midterm/#previous-operating-system-midterms-at-ntucsie","text":"","title":"Previous Operating System Midterms at NTUCSIE"},{"location":"OS/midterm/#spring-2011","text":"Terminologies. (24pts) Security Defense of a system from external and internal attacks, e.g., viruses, denial of services, etc. A Layered Approach in OS Designs The operating system is broken into a number of layers (levels). The bottom layer (layer $0$) is the hardware; the highest (layer $N$) is the user interface. Para-virtualization A variation on virtualization that presents a guest/operating system that is similar but not identical to the underlying hardware. Lightweight Process A virtual processor (kernel threads) on which the application can schedule a user thread to run. (many-to-many or two-level) NUMA Non-Uniform Memory Access: A CPU has faster access to some parts of main memory than to other parts. Deterministic Modeling Take a particular predetermined workload and defines the performance of each algorithm for that workload. Race Condition Several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place. Write Ahead Logging All modifications are written to a log before they are applied. Please answer following questions regarding the design of operation systems. (22pts) There are two conflicting goals in OS designs: Convenience and Efficiency. Please give me one example feature of OS that shows the conflict in persuing efficiency and convenience (Hint: Live preview of open windows for Windows7 taskbar). (5pts) GUI vs Command Line. What is the main goal of UNIX process init? Is init a user or kernel process? (8pts) init is responsible for bringing up a UNIX system after the kernel has been bootstrapped. User process with superuser privileges! Please explain how I/O protection is done. (5pts) Dual Mode Operations: a mode bit. Give me one advantage in OS implementations in some high-level language. (4pts) Being easy to understand & debug. Being written fast, more compact, and portable. Please answer the following questions for process managment. (16pts) Inside the Process Control Block, we might have a filed \"Program Counter\". What is the purpose of the field? (4pts) It can let us know which line to run, e.g. saved return address $\\to$ program counter. There are a lot of segments for a process image, such as code segment, data segment, heap, and user stack. When we call malloc() or free() , which segment is involved? (4pts) heap. When a parent process calls fork() to create a child process, how does the parent process know the process ID of the created child process? (4pts) When a parent process invoke fork(), a duplication of the parent process is created for the resulted child process, where the parent process returns from fork() with the process ID of the created child process. The child process returns 0. Which of the following IPC mechanisms does not require two communicating processes to have a parent-child relationship: Named Pipes and sockets. (4pts) Sockets. Please answer the following questions for CPU scheduling. (17pts) Compared to user-level threads, why the cost of context switching for kernel-level threads in higher? (4pts) Kernel thread: involves switching registers, stack pointer, and program counter, but is independent of address space switching User-level threads: similar context switching happens often. The delivery of a signal for theads is complicated. Give me an example signal that should be delivered to the threads to which the signal applies. (4pts) Division by zero. For preemptive scheduling, there are serveral occasions in triggering scheduling. Please give me three, beside the one in which a running process terminates by itself. (9pts) A new process with a higher priority than the current process arrives. An interrupt occurs. (fairness) Periodically based on clock interrupt. (e.g. RR) Consider Shortest-Job-First(SJF) and Round-Robin(RR) scheduling algorithms, and processes under considerations are only of one single CPU burst and are all ready at time $0$. Please answer the folowing questions. Explanation is needed to receive any credit: (18pts) Is SJF always better than RR, for any time quantum, in terms of the average turnaround time?(6pts) Yes. Proof : WLOG, given processes $P_1$, $P_2$, $\\dots$, $P_n$ with CPU burst time $t_1$, $t_2$, $\\dots$, $t_n$, where $t_1 < t_2 < \\cdots < t_n$. By SJF, their turnaround time should be: \\begin{array}{c|c} P_i & \\text{waiting time} \\\\ \\hline P_1 & t_1 \\\\ P_2 & t_1 + t_2 \\\\ P_3 & t_1 + t_2 + t_3 \\\\ \\vdots & \\vdots \\\\ P_n & t_1 + t_2 + \\cdots + t_n \\end{array} Total waiting time: $$nt_1 + (n - 1)t_2 + \\cdots + t_n.$$ Obviously, this sum is minimized if $t_i$'s that are multiplied more times are smaller ones, i.e., $$t_1 < t_2 < \\cdots < t_n.$$ Thus, in non-preemptive scheduling, SJF (actually Shortest-next-CPU-burst-first) is optimal for the purpose of minimizing turnaround time. When all processes are of the same size, please tell us what the best time quantum is for RR in terms of the average waiting time. (6pts) $80\\%$ of the CPU bursts should be shorter than the time quantum. Now suppose that processes might arrive at different times, and SJF and RR are preemptive scheduling algorithms. Is SJF always better than RR, for any time quantum, in terms of the average waiting time. (6pts) Similar to 5.(a). Please design a solution for airplanes to land in an airport. Suppose that there is only one runway in the airport. Please make sure that only one airplane can control the runway to land at a time, and there should be no starvation for your solution. (Hint: (1) the Bakery Algorithm; (2) Each process denotes an airplane.) (10pts) int number [ i ]; // Pi's number if it is nonzeros boolean choosing [ i ]; // Pi is taking a number do { choosing [ i ] = true ; // A process want to enter its critical section number [ i ] = max ( number [ 0 ], ..., number [ n - 1 ]) + 1 ; choosing [ i ] = false ; // A process has got its number for ( int j = 0 ; j < n ; j ++ ) { while ( choosing [ j ]) ; while ( number [ j ] != 0 && ( number [ j ], j ) < ( number [ i ], i )) ; // If two processes got the same number, then we should compare their indices } /* critical section */ number [ i ] = 0 ; /* remainder section */ } while ( true );","title":"Spring 2011"},{"location":"OS/midterm/#fall-2011","text":"The exam is 180 minutes long. The total score is 107pts. Please read the questions carefully. Terminologies. (24pts) DMA Release CPU from handling excessive interrupts! Execute the device driver to set up the registers of the DMA controller. DMA moves blocks of data between the memory and its own buffers. Transfer from its buffers to its devices. Interrupt the CPU when the job is done. Multiprogramming Increases CPU utilization by organizing jobs so that the CPU always has one to execute. Horizontal Cache Coherency and Consistency Among units of the same storage level. From \u77e5\u4e4e : Coherence \u4fdd\u8b49\u7684\u662f\u540c\u4e00\u5730\u5740\u6709\u4e0d\u540c copy \u7684\u6642\u5019\uff0c\u4fdd\u8b49\u770b\u5230\u7684\u662f\u5728 timing \u4e0a\u96e2\u81ea\u5df1\u6700\u8fd1\u7684\u3002 \u4f46\u662f\uff0c\u53ea\u4fdd\u8b49 Coherence \u662f\u4e0d\u5920\u7684\uff0c\u5728 multiprocessor \u4e0d\u540c\u5730\u5740\u7684\u591a\u500b copy \u8a2a\u554f\u7684\u6642\u5019\u6703\u51fa\u73fe\u554f\u984c\uff0c\u9019\u500b\u5c31\u662fconsistency Coherence is concerned with updates/invalidations to a single shared variable. Consistency is concerned with the behavior of memory references from multiple concurrent threads. A Module Approach in OS Designs (Hint: A Layered Approach) [ask] Moving all nonessential components from the kernel to the user or system programs! Indirect Communication in Message Passing ($\\leftrightarrow$ Direct) The messages are sent to and received from mailboxes, or ports. ($\\leftrightarrow$) The messages are sent to and received from processes. Socket An endpoint for communication. (IP + port#) Deferred Cancellation ($\\leftrightarrow$ Asynchronous Cancellation) The target thread periodically checks whether it should terminate, allowing it an opportunity to terminate itself in an orderly fashion. ($\\leftrightarrow$) One thread immediately terminates the target thread. Pull Migration (Hint: Multipocessor Scheduling) ($\\leftrightarrow$ Push Migration) Pulling a waiting task from a busy processor. ($\\leftrightarrow$) Pushing processes from overloaded to less-busy processors. Please answer the following questions regarding the designs of operating systems: (23pts) Please give me two resources, beside CPU, that are managed by OS. (6pts) CPU time, Memory Space, File Storage, I/O Devices, Shared Code, Data Structures, and more. When an interrupt arrives, a running task is interrupted, and its context could be saved in different ways, such as \"a fixed address for all interrupts\", \"a fixed space for each interrupt type\", and \"a stack\". What is the advantage in using a stack, compared with the approach in using a fixed space for each interrupt type? (5pts) If there are 2 interrupts, using fixed space is bad since the address of the first interrupted will be covered by later interrupted process. In a memory hierarchy, we have registers, cache, memory, and disk. Which of them is managed by operating systems? Which of them is managed by hardware? (12pts) Operating system: registers, cache, memory Hardware: disk OS services are such as those for \"program execution\", \"file-system manipulation\", \"accounting\", and \"resource allocation\". Which of them are for system efficiency, instead of user convenience? (8pts) [ask] Program execution and resource allocation. Famous Application Programming Interfaces (API) are such as Win 32 API and POSIX API. What are the two major benefits in providing API, compared to the providing of system calls only? protability (expected to run on any system) actual system calls can be more difficult to learn What does POSIX API offers to programmers, compared to the offering of ANSI C to programmers? (8pts) Any program written only in ANSI C and without any hardware dependent assumptions is virtully guaranteed to compile correctly on any platform with a conforming C implementation. POSIX is an acronym for \"Portable Operating System Interface\". POSIX is for software compatibility with variants of UNIX and other operating systems. In the ordinary virtualization design, the virtualization layer runs in the system mode. For VMware, the virtualization layer runs in the system or user mode? The user mode. For Java, the Java virtual machine run in the system or user mode? (6pts) The user mode. Please answer the following questions for process management and scheduling. (20pts) Give me two conditions for a running process to relinquish the CPU to go back to the ready queue in preemptive CPU scheduling. (6pts) [Spring 2011 5.(c)] A new process with a higher priority than the current process arrives. An interrupt occurs. (fairness) Periodically based on clock interrupt. (e.g. RR) Why a long-term scheduler has more time to choose a process for a system than a short-term scheduler does in process scheduling? (5pts) Because the interval between executions are longer. Please explain the main difference between a user-level thread and a kernel thread. (5pts) Is a Java thread is a user-level thread or a kernel thread? (4pts) Level Depending on the Thread Library on the Host System. The scheduling algorithm of Solaris 9 is based on the Multilevel Feedback Queue Scheduling algorithm. There are six priority classes. Please explain the Fair Sharing class? Please explain how interactive threads in the Time Sharing or Interactive class are favored in scheduling in Solaris 8 or 9? (6pts) Consider the scheduling of processes in which processes might arrive at different times and have different deadlines to complete their execution. Let the processes be scheduled by the preemptive Shortest-Job-First algorithm (PSJF) and a Priority Scheduling algorithm (PS) in which processes with urgent deadlines have higher priorities, and there is only one processor. Can you give a set of processes such that PS can meet the deadlines of the processes, but PSJF can not do it? (5pts) Given processes: Process Period ProcTime $P_1$ $p_1 = 50$ $t_1 = 25$ $P_2$ $p_2 = 80$ $t_2 = 35$ Consider the Round Robin scheduling algorithm (RR) with two different time quantums $L$ and $S$, where $L > S$. Let the scheduling criteria be the average waiting time, and $L$ be larger than the largest CPU burst of all processes. Does RR, in general, favor a small time quantum $S$ when all processes are ready at time $0$? Please give me your answer with argument. (7pts) No. For example, given processes $P_1$, $P_2$ and $P_3$ with CPU burst time $10$, $10$ and $10$ With quantum $> 10 (L)$, average waiting time = (0 + (10 - 0) + (20 - 0)) / 3 = 30 / 10 = 10. With quantum $= 5$, average waiting time = [0 + (15 - 5) + (5 - 0) + (20 - 10) + (10 - 0) + (25 - 15)] = 45 / 3 = 15. With quantum $= 1$, average waiting time = ... = (18 + 19 + 20) / 3 = 19. With quantum $\\to 0$, average waiting time $\\approx$ (20 + 20 + 20) / 3 = 20. It is obvious that smaller time slice will lead a longer average waiting time.","title":"Fall 2011"},{"location":"OS/midterm/#fall-2012","text":"The exam is 180 minutes long. The total score is 110pts. Please read the questions carefully. Terminologies. (24pts) Hardware Interrupt Services requests of I/O devices. Virtual Machine Provides an interface that is identical to the underlying bare hardware. Context Switch It saves the state of the currently running process and loads the state of the newly scheduled process. A Full Duplex Pipe A pipe that supports two ways of message passing simultaneously. Multilevel Queue Scheduling Processes can be classified into different groups and permanently assigned to one queue, where there are Inter-queue and intra-queue scheduling policies. Memory Stall A phenomenon in which a processor waits for a significant amount of time waiting for the data to become available. Bounded Waiting (A Requirement of a Critical Section Solution) A waiting process only waits for a bounded number of processes to enter their critical sections. Adaptive Mutex A binary semaphore in which it is a spinlock if the lock-holding thread is running; otherwise, blocking is used. Please answer the following questions regarding the designs of operating systems: (20pts) What is the difference between multiprogramming and time sharing ? (6pts) Time sharing (or multitasking) is a logical extension of multiprogramming, where CPU services each of ready tasks in a way that every task receives CPU time in an interactive fashion. Which one of the following memory unit is managed by the operating systems: Registers, Cache, Main Memory, Disks (8pts) Main Memory and Disks. Operating systems services include user interfaces. UNIX shells, including the Bourne shell and C shell, provide command interpreters. Consider UNIX shells, please give me one command that is implemented as some code inside the command interpreter and two commands that are implemented by system programs? (6pts) Inside the command interpreter: C shell: umask, cd and limit Bourne shell: ulimit -H and -S By system programs: rm and ls Message passing is a way for interprocess communication. Consider the capacity of a link between two processes has zero capacity. Is the message passing (between the two processes) synchronous or asynchronous? You must provide explanation to receive any credits. (5pts) It is synchronous because we can only have blocking sends and blocking receives. Please give me one occasion when a mid-term scheduler should run. The remaining main memory is low or the CPU utilization is too high. Can a short-term scheduler schedule the executions of user-level threads? No. Can a short-term scheduler schedule the executions of a Java thread? (9pts) It depends on the thread library on the host system. Consider signal handling for threads. Is a Division-By-Zero signal synchronous or asynchronous? Synchronous. Should a Division-By-Zero signal be delivered to every thread of its belonging process? (6pts) [Spring 2011 4.(b)] No. It should be only sent to the thread that causes the signal. Please answer the following questions for process scheduling. Explanation is needed to receive any credit. (24pts) For the Round-Robin Scheduling, what would be the preferred time slice in general? (4pts) [Spring 2011 6.(b)] $80\\%$ of the CPU bursts should be shorter than the time quantum. For the Round-Robin Scheduling, shall we have a small time slice for a better average turn around time? (4pts) No, a small time slice will increase the average turnaround time. For the Priority Scheduling, how to avoid the starvation problem (in which a low priority process can never be scheduled)? (4pts) An aging solution by increasing the priority of a process that waits for a long time. Please explain how a guest operating-system scheduling algorithm that assumes a certain amount of progress in a given amount of time might be negatively impacted by virtualization. (6pts) It is because the virtualization software needs to schedule the use of the physical CPUs among the virtual CPUs. A given amount of the time slice might take much more than the time of the virtual CPU time. For the evaluation of a scheduling algorithm, please give me two difficulties in using the implementation method. (6pts) Cost in modifying the OS User reaction to OS changes Changing of the environment in which the algorithm is used Please explain the difference between the signal operation of a binary semaphore and that of a condition variable (of a monitor). (5pts) The signal operation of a condition variable resumes one suspended process. If there is none, no effect is imposed. Consider a barber shop in which there are two barbers. Each barber can service exactly one customer at a time. Please use binary semaphores to implement the request for the hair-cut service. (12pts) Use an integer $S$ with initial value = $2$ to indicate the number of available barbers. typedef struct { int value ; struct customer * waiting_list ; } semaphore ; wait ( semaphore * S ) { S -> value -- ; if ( S -> value < 0 ) { add this customer to S -> waiting_list ; block (); } } signal ( semaphore * S ) { S -> value ++ ; if ( S -> value <= 0 ) { remove a customer C from S -> waiting_list ; wakeup ( C ); } } Customer $C_i$: wait ( S ); /* critical section */ signal ( S ); Consider the time-stamp protocol of Chapter 6, in which each process $T_i$ is given a time stamp $T_S(T_i)$, and each read/write operation must check it up with the read and write timestamp of the accessed data object $Q$. Is it possible to have any deadlock? You must provide your explanation. (5pts) [ask] No deadlock because there is no hold-and-wait among processes.","title":"Fall 2012"},{"location":"OS/midterm/#fall-2013","text":"The exam is 180 minutes long. The total score is 103pts. Please read the questions carefully. Terminologies. (24pts) Booting Initialize all aspects of the system and then load and run the OS. Interrupt Vector An array of interrupt-handlers' addresses that are indexed by device numbers (or interrupt numbers). Cache Coherency [Fall 2011 1.(c)] Cache coherency problems can arise when more than one processors refer to the same data. Coherency defines what value is returned on a read. Platform as a service (PaaS) in Cloud Computing Pass provides a computing platform and a solution stack as a service, such as a database server. One example is Microsoft Azure. A Modular Kernel (Hint: OS Structure) [Fall 2011 1.(d)] A moduler kernel consists of a set of components, where there are core/primary modules, and it is of modules without a layer structure. Data Parellelism in Multicore Programming Distribute data over cores to execute the same operation . Rate Monotonic Scheduling A fixed priority real-time process scheduling algorithm in which the priorities of processes are inversely proportional to their periods . Deterministic Modeling [Spring 2011 1.(f)] Take a particular predetermined workload and defines the performance of each algorithm for that workload. Please answer the following questions regarding the designs of operating systems: (20pts) There are two conflicting goals in the designs of operating systems. What is the other one, beside \"convenient for the users\" ? (3pts) Efficient operation of the computer system. Which one of the following actions/events is belonging to or might result in software interrupts: System calls, child termination, mouse clicking, invalid memory access. (8pts) System calls, child temination, and invalid memory access. Consider the implementation of virtual machines in which operating systems run on the top of the virtual machine software (or referred to as the hypervisor). Is an operating system running in the user mode or kernel mode? (3pts) [Fall 2011 5.(a), Fall 2013 2.(c)] The user mode. Please give me 2 advantages of virtual machines, beside system consolidation and easy in system development/deployment. (6pts) Complete isolation and multiple personalities. ANSI C refers to the family of successive standards published by ANSI for the C programming language. Please compare difference (or provide the purpose difference) between POSIX and ANSI C. (5pts) [Fall 2011 4.(b)] Any program written only in ANSI C and without any hardware dependent assumptions is virtully guaranteed to compile correctly on any platform with a conforming C implementation. POSIX is an acronym for \"Portable Operating System Interface\". POSIX is for software compatibility with variants of UNIX and other operating systems. Please answer the following questions for task scheduling. (14pts) Is the \"Swapper\" a short-term or mid-term scheduler? [ask] Short-term scheduler. In UNIX, the process control block PCB[] of a process consists of proc[] and .u, where the attributes in .u are those needed when the process is running, and the attributes in proc[] are those needed all the time. Please indicate which one should be in .u : file[], task priority, pid, signal disposition, and task state. You must provide explanation to receive any credits. (10pts) [ask] file[]: what files are \"being\" opened. signal disposition: how to deal with signals. Consider message passing and shared memory for inerprocess communication. Is \"Pipe\" considered one for message passing or shared memory ? Is \"Pipe\" direct or indirect communication ? Is it \"synchronous\" or \"asynchronous\"? for a reader or a writer of the communication in UNIX. You must provide explanation to receive any credits. (12pts) Message passing. Since it's communication between processes without sharing the same address space. Indirect communication. [ask] Synchronous for a reader. [ask] Asynchronous for a writer. [ask] Kernel-level threads are superior than uer-level threads do in many ways. What is the main disadvantage of kernel-level threads? It is context switching cost. With OpenMP in program development, shall we prefer kernel-level or user-level threads? You must provide explanation to receive any credits. (8pts) We prefer kernel-level threads because we look for parallelism to better utilize multiple cores of a system. Please answer the following questions for process scheduling. Explanation is needed to receive any credits. (15pts) Given 3 processes $P_1$, $P_2$, and $P_3$ with CPU burst time $5$, $6$, $7$, respectively. Suppose that the 3 processes arrive at time $0$, and $P_1$ and $P_3$ are the first and the last processes in the ready queue, respectively. What is the average waiting time in running the 3 processes under the Round-Robin Scheduling with the time slice equal to $3$. (5pts) (6 + (3 + 5) + (6 + 5)) / 3 = 25 / 3. Consider FCFS and Round-Robin Scheduling. If process are only of CPU burst and all arrive at time $0$, do FCFS and Round-Robin Scheduling with time slice = $1$ always have the same total waiting time in running all processes ? Prove your answer. (5pts) No, for example, given processes of CPU burst 7, 1, and 1. Suppose that the variance of the tunaround time is the cirterion in process scheduling. Shall we have a small time slice for a better variance turnaround time when all processes arrive at time $0$? (5pts) No. For example, given processes $P_1$, $P_2$ and $P_3$ with CPU burst time $10$, $10$ and $10$. With quantum $= 10$, average turnaround time = (10 + 20 + 30) / 3 = 60 / 3 = 20. With quantum $= 5$, average turnaround time = (20 + 25 + 30) / 3 = 75 / 3 = 25. With quantum $= 1$, average turnaround time = ... = (28 + 29 + 30) / 3 = 87 / 3 = 29. With quantum $\\to 0$, average turnaround time $\\approx$ 30 + 30 + 30 = 90 / 3 = 30. It is obvious that smaller time slice will lead a longer average turnaround time. Please explain why the Completely Fair Scheduling (CFS) of Linux V2.6 favors I/O tasks. (5pts) CFS usually dispatches the CPU to the task with the smallest vruntime (that denotes how long the task runs) such that I/O tasks usually have smaller vruntime.","title":"Fall 2013"},{"location":"OS/midterm/#fall-2014","text":"The exam is 180 minutes long. The total score is 108pts. Please read the questions carefully. Terminologies. (24pts) Buffering (Hint: It is not caching.) It means to keep data in a faster medium temporarily before moving them to a slower layer. Virtual Machine [Fall 2012 1.(b)] Provides an interface that is identical to the underlying bare hardware. System Generation (SYSGEN) The process to configure or generate an operating system for a one specific computer. Context Switch [Fall 2012 1.(c)] Save the state of the old process and load the state of the newly scheduled process. Remote Procedure Call (Hint: Message Passing) Senders are blocked until the receivers have received messages and replied by reply messages. (A way to abstract the procedure-call mechanism for use between systems with network connection.) Implicit Threading Transfer the creation and management of the threading from the application developers to compilers and run-time libraries. Earliest Deadline First Scheduling A dynamic-priority real-time process scheduling algorithm in which the priorities of processes are higher if their deadlines are closer. Race Condition [Spring 2011 1.(g)] Several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place. Please answer the following questions regarding the designs of operating systems: (22pts) An operating system could be considered as a resource allocator. Please list 3 resources managed by an operating systems. (6pts) [Fall 2011 2.(a)] CPU time, Memory Space, File Storage, I/O Devices, Shared Code, Data Structures, and more. Please explain what happens to the operating system when an interrupt arrives (until the interrupt is serviced). (4pts) Saving of the address of the interrupted instruction, determine the interrupt type (by polling or interrupt vector), call the corresponding handlers. Which one of the following instructions is a privileged instruction: Reading of the Timer, setting of the base register of the memory space of a process, increment the value of a CPU register by one. No explanation is needed. (6pts) Setting of the base register of the memory space of a process. Please explain what happens when a command-line user interface of UNIX executes a command. (6pts) Search the exec file which corresponds to the command; fork a process to execute the file. Consider parameter passing to a system call. Give me the major advantage in having a register pointing to a memory block that stores all parameters, compared to having all parameters stored in registers? (5pts) The space needed to store all parameters is virtually unlimited. The memory image of a UNIX process might consist of a code segment, a data segment, a heap area, a user stack, a kernel stack, an environment variable area, and .u. Which one of the above is used when malloc() is invoked? [Spring 2011 3.(b)] heap. When the kernel stack is used? (8pts) [ask] Procedure invocation in the system mode. Please explain how fork() works when a parent process invoke fork() . [Spring 2011 3.(c)] When a parent process invoke fork(), a duplication of the parent process is created for the resulted child process, where the parent process returns from fork() with the process ID of the created child process. The child process returns 0. Please use the answer to the fork() invocation to explain the difference between fork() and vfork() . (10pts) When a parent process invoke vfork(), a child process is created by using the process image of the parent process, including its code and data segments. However, the parent process stop temporarily until the child process invokes an execve()-like system call to start a new program or terminates. The child process can modify any contents of the data segment of the parent process. In the multi-core age, threading is encouraged. Please answer the following questions: (8pts) For multiple cores, do we prefer kernel-level threads or user-level threads? Why? (3pts) [Fall 2013 6.(b)] We prefer kernel-level threads because we look for parallelism to better utilize multiple cores of a system. There are also challenges in programming. Consider merge sorting, in which an unsorted list is divided into $N$ sublists, each containing $1$ element, and then we repeatedly merge sublists to produce new sorted sublists until there is only $1$ sublist remaining. This will be the sorted list. Please use the concept of task parallelism to execute the merge sort over $N$ cores. (5pts) Each core is given a sublist of one element. We then group 2 adjacent cores and let one of the cores merge the sublists of the two cores. Please answer the following questions for process scheduling. Explanation is needed to receive any credit. (15pts) Given 5 processes $P_1$, $P_2$, $P_3$, $P_4$, and $P_5$ with CPU burst time $6$, $5$, $2$, $10$, $5$, respectively. Suppose that the $P_1$, $P_2$, $P_3$, $P_4$, and $P_5$ arrive at time $0$, $3$, $4$, $2$, and $5$, respectively. What is the average waiting time in running the 5 processes under the Preemptive Shortest-Job-First Scheduling. (5pts) (0 + (8 - 3) + (6 - 4) + (18 - 2) + (13 - 5)) / 5 = 31 / 5. Consider Shortest-Job-First Scheduling and Round-Robin Scheduling. Please prove that the total waiting time in running all processes under Shortest-Job-First Scheduling is always no larger than that of Round-Robin Scheduling with time slice equal to $1$ when all processes are ready at time $0$? (5pts) [Spring 2011 5.(a)] Yes. Proof : WLOG, given processes $P_1$, $P_2$, $\\dots$, $P_n$ with CPU burst time $t_1$, $t_2$, $\\dots$, $t_n$, where $t_1 < t_2 < \\cdots < t_n$. By SJF, their waiting time should be: \\begin{array}{c|c} P_i & \\text{waiting time} \\\\ \\hline P_1 & 0 \\\\ P_2 & t_1 \\\\ P_3 & t_1 + t_2 \\\\ \\vdots & \\vdots \\\\ P_n & t_1 + t_2 + \\cdots + t_{n - 1} \\end{array} Total waiting time: $$(n - 1)t_1 + (n - 2)t_2 + \\cdots + t_{n - 1}.$$ Obviously, this sum is minimized if $t_i$'s that are multiplied more times are smaller ones, i.e., $$t_1 < t_2 < \\cdots < t_{n - 1} < t_n.$$ Thus, in non-preemptive scheduling, SJF (actually Shortest-next-CPU-burst-first) is optimal for the purpose of minimizing average waiting time. Suppose that the variance of the waiting time is the criterion in process scheduling. Shall we have a small time slice for a better variance for Round-Robin Scheduling when all processes of the same CPU burst arrive at time $0$? (5pts) [Fall 2011 9., Fall 2013, 7.(c)] No. For example, given processes $P_1$, $P_2$ and $P_3$ with CPU burst time $10$, $10$ and $10$ With quantum $= 10$, average waiting time = (0 + (10 - 0) + (20 - 0)) / 3 = 30 / 10 = 10. With quantum $= 5$, average waiting time = [0 + (15 - 5) + (5 - 0) + (20 - 10) + (10 - 0) + (25 - 15)] = 45 / 3 = 15. With quantum $= 1$, average waiting time = ... = (18 + 19 + 20) / 3 = 19. With quantum $\\to 0$, average waiting time $\\approx$ (20 + 20 + 20) / 3 = 20. It is obvious that smaller time slice will lead a longer average waiting time. Please answer the following questions for process synchronization: (16pts) Please compare the difference between a binary semaphore and a condition variable. (3pts) [Fall 2012 7.] The signal operation of a condition variable resumes one suspended process. If there is none, no effect is imposed. Please use Monitor to implement Consumer and Producer with a bounded buffer. (10pts) monitor ProducerConsumer { int counter = 0 ; condition empty ; condition full ; void producer () { if ( counter == n ) wait ( empty ); /* produce a slot */ counter ++ ; signal ( full ); } void consumer () { if ( counter == 0 ) wait ( full ); /* consume a slot */ counter -- ; signal ( empty ); } } counter: number of filled slots empty: buffer has at least one empty slot full: buffer has at lesat one full slot Please prove that your above solution satisfy the Progress requirement of the Critical Section Problem. (3pts) Mutual exclusioin: monitor ensures that only one process can execute. Progress requirement: by signal(full) to wake wait(full) and signal(empty) to wake wait(empty), the processes won't wait forever. Bounded-waiting: they will both at most wait for 1 process (each other).","title":"Fall 2014"},{"location":"OS/midterm/#fall-2015","text":"The Exam is 180 minutes long. The total score is 105pts. Please read the questions carefully. Terminologies. (24pts) Software Interrupts Caused by software execution, e.g. signals, invalid memory access, division by zero, system calls. Performance Tuning A procedure that seeks to improve performance by removing bottlenecks. Mid-Term Scheduler ($\\leftrightarrow$ Long-Term Scheduler $\\leftrightarrow$ Short-Term Scheduler) Swap processes in and out to control the degree of multiprogramming. ($\\leftrightarrow$) Long-Term Scheduler: Selects processes from this pool Loads them into memory for execution Controls the degree of multiprogramming (# processes). Selects a good process mix of I/O-bound and CPU-bound. ($\\leftrightarrow$) Short-Term Scheduler: Selects from among the processes that are ready to execute Allocates CPU to one of them FIFOs of UNIX Named pipes. Asynchronous Signal ($\\leftrightarrow$ Synchronous Signal) An asynchronous signal usually reports some asynchronous event outside the program, e.g., ^C or time expiration. ($\\leftrightarrow$) Delivered to the same process that performed the operation causing the signal. e.g., illegal memory access, division by 0. Push Migration (Hint: Load Balancing) [Fall 2011 1.(h)] ($\\leftrightarrow$ Pull Migration) Pushing processes from overloaded to less-busy processors. ($\\leftrightarrow$) Pulling a waiting task from a busy processor. Coarse-Grained Multithreading of Hardware Threads ($\\leftrightarrow$ Fine-Grained (interleved)) A thread executes on a processor until a long-latency event such as a memory stall occurs. ($\\leftrightarrow$) Switches between threads at a much finer level of granularity Analytic Evaluation [Spring 2011 1.(f), Fall 2013 1.(h)] Analytic evaluation uses the given algorithm and the system workload to produce a formula or number to evaluate the performance of the algorithm for that workload. Please answer the following questions regarding operating systems: (20pts) Please compare the difference between interrupt handling by a generic handler and interrupt vector in terms of the mechanism and the response performance. (6pts) A generic handler finds out the interupt type and invokes the corresponding procedure but the interrupt vector lets the corresponding procedure directly invoked though checking up of the interrupt vector; performance: interrupt vector is faster. Please compare the difference between the terms \"time sharing\" and \"multiprogramming\". (4pts) [Fall 2012 2.(a)] Time sharing (or multitasking) is a logical extension of multiprogramming, where CPU services each of ready tasks in a way that every task receives CPU time in an interactive fashion. One of the most challenging parts in the implementations of a virtual machine is to satisfy the assumption of a certain amount of progress in a given amount of time. Please explain the challenge. (6pts) [Fall 2012 6.(d)] It is because the virtualization software needs to schedule the use of the physical CPUs among the virtual CPUs. A given amount of the time slice might take much more than the time of the virtual CPU time. Parameter passing is an important issue in the implementation of command interpreters. Please explain how a command interpreter of UNIX passes parameters to the running process of a command issued on the command interpreter. (4pts) It can be done by using one of the exec() system calls. Consider process states: New, Ready, Running, Waiting, and Terminated. Please explain how a state makes a transition to another state, where there is only one processor. (12pts) Message passing is one major way for interprocess communication. What is the main difficulty in using symmetric addressing for direct communication? Process naming (or modularity). For indirect communication, small messages are sent from a sender to a receiver usually by message copying. How to reduce system overheads in-sending large messages to a receiver? (8pts) Remapping of addressing space. For multicore programming, there could be data parallelism or task parallelism. Please explain how to use data parallelism to find the largest integer of a given set of integers. (5pts) [Fall 2014 6.(b)] First split data over $N$ cores, and let each core find the largest integer of its given integers. Then let one core find the largest integers among the largest integers found on each core. Please answer the following questions for process scheduling. Explanation is needed to receive any credit. (15pts) Is the First-Come, First-Served Scheduling (FIFO) a non-preemptive or preemptive scheduling algorithm? Why? (4pts) It is non-preemptive because the running process will not volunteer to give up its CPU until it stops. Given 4 processes $P_1$, $P_2$, $P_3$, and $P_4$ with CPU burst time $5$, $2$, $4$, and $6$, respectively. Suppose that the $P_1$, $P_2$, $P_3$, and $P_4$ all arrive at time $0$. What is the average waiting time in running the 4 processes under the Round-Robin Scheduling with the time slice equal to $3$? (5pts) [Fall 2013 7.(a)] (8 + 3 + (5 + 5) + (8 + 3)) / 4 = 32 / 4 = 8. Longest-Job-First Scheduling always schedules the process of the longest CPU burst first. When all processes arrive at time $0$, does Longest-Job-First Scheduling have the largest average turnaround time? (6pts) [Fall 2013 7.(c)] Yes. Consider the intersection of the following two roads, where cars can go from each of the four directions. There is a stop sign for each direction so that every car must stop at the intersection and wait for any car that arrives earlier at the intersection to leave first. Please use semaphores to implement your solution with some pseudo code. (10pts) [Fall 2012 8.] Use an integer $S$ with initial value = $1$ to indicate the number of available car and a FIFO queue waiting list. typedef struct { int value ; struct car * waiting_list ; } semaphore ; wait ( semaphore * S ) { S -> value -- ; if ( S -> value < 0 ) { add this car to S -> waiting_list ; block (); } } signal ( semaphore * S ) { S -> value ++ ; if ( S -> value <= 0 ) { remove a car C from S -> waiting_list ; wakeup ( C ); } } Car $C_i$: wait ( S ); /* critical section */ signal ( S ); Please prove that your above solution satisfy the three requirements of the Critical Section Problem. (6pts) Mutual exclusioin: only a car can go across the intersection. Progress requirement: by block() and wakeup(), the processes won't wait forever. Bounded-waiting: at most wait for $n - 1$ cars. Could you revise your solution so that an ambulance can always go first? (5pts) Let the ambulance has the highest priority in the waiting queue.","title":"Fall 2015"},{"location":"OS/midterm/#spring-2018","text":"The exam is 180 minutes long. The total score is 112pts. Please read the questions carefully. Terminologies. (24pts) init (Hint: Booting; a UNIX process) A user process that initializes system processes, e.g., various daemons, login processes, after the kernel has been bootstrapped. Dual-Mode Operations (Hint: Mode Bit) Privileged and user-mode instructions, where privileged instructions are machine instructions that may cause harm. Memory Protection (Hint: Hardware Protection) Prevent a user program from modifying the code or data structures of either the OS or other users! A Modular Kernel A set of core components with characteristics: layer-like \u2014 modules; microkernel-like \u2014 the primary module Asymmetric Addressing for Direct Communication In Direct Communication, sender must specify the receiver ID, but the receiver does not need to specify the sender ID. For example, Send(P, msg), Receive(id, msg) Remote Procedure Call (RPC) A way to abstract the procedure-call mechanism for use between systems with network connection. FIFOS of UNIX Named pipes of UNIX. It is a file in the file system and created by mkfifo(). It offers Half Dulex and Byte-Oriented Transmissions. Implicit Threading Transfer the creation and management of the threading from the application developers to compilers and run-time libraries. For the implementation of the UNIX process control block PCB[]\uff0c we have proc[] and .u, which contain different attributes for different purposes (e.g., those must be known regardless of whether the process is running). Which one is in the kemel or user address space? Are they both in the kemel or user address space? Which one of proc[] and .u signal disposition resides? Where thread-specific data/thread local storage will reside (in the kemel or user address space)? (8pts) kernel address space: proc[]; user-address space: .u .u: signal disposition. user address space. Please answer the following questions regarding operating systems: (21pts) When timing sharing appears as a feature for operating systems, please tell us two emerging features/subsystems, beside job synchronization. (6pts) one-line file systems, virtual memory, sophisticated CPU scheduling. Virtual machine software provides an interface that is identical to the underlying bare hardware. I/O over a virtual machine could be slow or fast, depending on the implementations. Please give me a case in which I/O is fast, compared to I/O directly over physical devices. (5pts) I/O can be fast over a hard disk when the corresponding disk is implemented over DRAM by emulation. There are operating system services, such as program executions, I/O operations, accounting, resource allocation, e\u6c40or detection. Which one is belonging to those for user convenience? Which one is belonging to those for system efficiency? (1Opts) user convenience: program executions, I/O operations, error detection; system efficiency: accounting, resource allocation. Multithreading is important for multi-core architectures. Please answer the following questions: (12pts) Task parallelism is one typical parallelism type. Please give one example for task parallelism. (5pts) Run merge sorting over cores by partitioning data. Please provide two challenges in multicore programming. (4pts) Identifying Tasks \u2014 Dividing Activities, Balance, Data Splitting, Data Dependency, Testing and Debugging. Why the context switching cost of kernel-level threads is higher than that of user-level threads. (3pts) Context switching cost is a little bit higher because the kernel must do the switching. Compare preemptive scheduling and non-preemptive scheduling. Please answer the following questions: (21 pts) In what circumstance, non-preemptive scheduling wiU be be\u5410er than preemptive scheduling? (4pts) Non-preemptive scheduling could be better than preemptive scheduling when all jobs are ready together at time 0. The selection of CPU scheduling algorithms should consider the scheduling criteria. If the high Thoughput is the criterion, which one is the best: Shortest job first, FIFO, and round-robin scheduling. You must provide answer to get credits. (6pts) shortest job first. Priority scheduling is a framework for scheduling. Please design a priority assignment formula that gives a lower priority to a job when it runs more time. (5pts) priority = 1 / consumed-CPU-time. For Round-Robin Scheduling, if the lower average Tumaround Time is the criterion, sha11 we prefer a lower time slice? Suppose that a11 jobs are ready at time O. You must provide answer to get credits. (6pts) a lower time slice is bad to the low average turnaround time. Processor Affinity is somehow against push migration. Please explain why. (6pts) It is because of the cost in cache repopulation. Please explain the progress assumption concerns for virtualization. (6pts) lt is assumed that a certain amount of progress should be observed for a system in a given amount oftime under virtualization. Consider solutions to the Critical Section Problem. Please answer the following questions (14pts) Consider a computing server to service jobs from three FIFO queues in a round robin way whenever there are jobs in any non-empty queues. Please use semaphores to implement your solution with some pseudo code. (6pts) We let the dispatcher of each job queue to wait a shared semaphore whenever it has a pending job. Let the semaphore have a FIFO queue. What is the difference between a binary, semaphore and a condition variable? (3pts) A conditino variable has nothing happening to a signal request if no job is waiting for the variable. The Monitor-based implementation of the Dining Philosopher problem in the textbook implements the idea of two-chop-stick-per-time solution. Will it have the starvation problem? You must provide explanation to receive credits. (5pts) Yes, the starvation problem happens to a philosopher whenever his two neighboring philosophers take turn in eating while he is waiting.","title":"Spring 2018"}]}